{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Qualtrics Processing Pipeline",
   "id": "45e267f26374d754"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Load Qualtrics Excel File",
   "id": "c9ccc1b6a7459ed2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Universal Qualtrics Excel File Loader",
   "id": "33399e8a4f8bb59b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Universal Qualtrics Excel File Loader with GUI Selector\n",
    "# This code is designed to work with any Qualtrics export regardless of survey content\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import re\n",
    "import tkinter as tk             # <-- ADDED for GUI\n",
    "from tkinter import filedialog   # <-- ADDED for GUI\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "def load_qualtrics_export(file_path=None, sheet_name=0):\n",
    "    \"\"\"\n",
    "    Load a Qualtrics Excel export file with a GUI selector and robust error handling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str or Path, optional\n",
    "        Path to the Excel file. If None, a GUI file selector will open.\n",
    "    sheet_name : str or int, default 0\n",
    "        Sheet name or index to load from the Excel file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains 'raw_data' (DataFrame), 'file_info' (dict), and 'quality_check' (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 1: Loading Qualtrics Export ===\")\n",
    "\n",
    "    # If no file_path is provided, open a GUI file selector\n",
    "    if file_path is None:\n",
    "        # --- REPLACED CODE BLOCK ---\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the main tkinter window\n",
    "\n",
    "        print(\"Opening file selector...\")\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select the Qualtrics Excel Export\",\n",
    "            filetypes=[(\"Excel Files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "\n",
    "        if not file_path:  # Handle case where user closes the dialog\n",
    "            raise FileNotFoundError(\"No file selected. Please run the script again.\")\n",
    "        # --- END REPLACED CODE BLOCK ---\n",
    "\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    print(f\"Loading: {file_path.name}\")\n",
    "\n",
    "    # Load the Excel file with error handling\n",
    "    try:\n",
    "        # First, get sheet information\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        print(f\"Available sheets: {sheet_names}\")\n",
    "\n",
    "        # Determine which sheet to load\n",
    "        if isinstance(sheet_name, str) and sheet_name not in sheet_names:\n",
    "            print(f\"Warning: Sheet '{sheet_name}' not found. Using first sheet: '{sheet_names[0]}'\")\n",
    "            sheet_name = 0\n",
    "        elif isinstance(sheet_name, int) and sheet_name >= len(sheet_names):\n",
    "            print(f\"Warning: Sheet index {sheet_name} out of range. Using first sheet: '{sheet_names[0]}'\")\n",
    "            sheet_name = 0\n",
    "\n",
    "        actual_sheet = sheet_names[sheet_name] if isinstance(sheet_name, int) else sheet_name\n",
    "        print(f\"Loading sheet: '{actual_sheet}'\")\n",
    "\n",
    "        # Load the data\n",
    "        raw_df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load Excel file: {str(e)}\")\n",
    "\n",
    "    # File information\n",
    "    file_info = {\n",
    "        'filename': file_path.name,\n",
    "        'file_size_mb': file_path.stat().st_size / (1024 * 1024),\n",
    "        'sheet_loaded': actual_sheet,\n",
    "        'available_sheets': sheet_names,\n",
    "        'raw_shape': raw_df.shape\n",
    "    }\n",
    "\n",
    "    print(f\"File loaded successfully:\")\n",
    "    print(f\"  Size: {file_info['file_size_mb']:.1f} MB\")\n",
    "    print(f\"  Dimensions: {raw_df.shape[0]:,} rows × {raw_df.shape[1]:,} columns\")\n",
    "\n",
    "    # Quality checks to identify Qualtrics structure\n",
    "    quality_check = analyze_qualtrics_structure(raw_df)\n",
    "\n",
    "    return {\n",
    "        'raw_data': raw_df,\n",
    "        'file_info': file_info,\n",
    "        'quality_check': quality_check\n",
    "    }\n",
    "\n",
    "def analyze_qualtrics_structure(df):\n",
    "    \"\"\"\n",
    "    Analyze the loaded DataFrame to identify Qualtrics-specific patterns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw loaded data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality check results and structural analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Analyzing Qualtrics Structure ===\")\n",
    "\n",
    "    # Initialize quality check results\n",
    "    quality_check = {\n",
    "        'is_qualtrics_format': False,\n",
    "        'header_row_index': None,\n",
    "        'data_start_row': None,\n",
    "        'standard_qualtrics_columns': [],\n",
    "        'question_columns': [],\n",
    "        'total_columns': len(df.columns),\n",
    "        'potential_issues': []\n",
    "    }\n",
    "\n",
    "    # Standard Qualtrics metadata columns (regardless of survey content)\n",
    "    standard_columns = [\n",
    "        'StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "        'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "        'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "        'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "        'DistributionChannel', 'UserLanguage'\n",
    "    ]\n",
    "\n",
    "    # Check if this looks like a Qualtrics export\n",
    "    columns_list = df.columns.tolist()\n",
    "    standard_found = [col for col in standard_columns if col in columns_list]\n",
    "    quality_check['standard_qualtrics_columns'] = standard_found\n",
    "\n",
    "    # Qualtrics exports typically have these key identifiers\n",
    "    qualtrics_indicators = ['ResponseId', 'StartDate', 'EndDate', 'Status']\n",
    "    indicators_found = sum(1 for indicator in qualtrics_indicators if indicator in columns_list)\n",
    "\n",
    "    if indicators_found >= 3:\n",
    "        quality_check['is_qualtrics_format'] = True\n",
    "        print(\"✓ Confirmed Qualtrics export format\")\n",
    "    else:\n",
    "        quality_check['potential_issues'].append(\"Does not appear to be standard Qualtrics export format\")\n",
    "        print(\"⚠ Warning: File may not be a standard Qualtrics export\")\n",
    "\n",
    "    # Identify question columns (typically start with Q followed by number)\n",
    "    question_pattern = re.compile(r'^Q\\d+', re.IGNORECASE)\n",
    "    question_columns = [col for col in columns_list if question_pattern.match(str(col))]\n",
    "    quality_check['question_columns'] = question_columns\n",
    "\n",
    "    print(f\"Standard Qualtrics columns found: {len(standard_found)}\")\n",
    "    print(f\"Question columns identified: {len(question_columns)}\")\n",
    "\n",
    "    # Analyze row structure for header/data separation\n",
    "    if quality_check['is_qualtrics_format']:\n",
    "        header_analysis = analyze_header_structure(df)\n",
    "        quality_check.update(header_analysis)\n",
    "\n",
    "    # Check for common issues\n",
    "    if df.shape[0] < 2:\n",
    "        quality_check['potential_issues'].append(\"Very few rows - may not contain response data\")\n",
    "\n",
    "    if df.isnull().all().sum() > len(df.columns) * 0.5:\n",
    "        quality_check['potential_issues'].append(\"Many completely empty columns detected\")\n",
    "\n",
    "    # Report findings\n",
    "    if quality_check['potential_issues']:\n",
    "        print(\"\\n⚠ Potential Issues Detected:\")\n",
    "        for issue in quality_check['potential_issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"✓ No structural issues detected\")\n",
    "\n",
    "    return quality_check\n",
    "\n",
    "def analyze_header_structure(df):\n",
    "    \"\"\"\n",
    "    Analyze the DataFrame to identify where the header row and data rows are.\n",
    "    Qualtrics exports typically have question text in row 0 and data starting from row 1 or 2.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw Qualtrics data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Header analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    header_info = {\n",
    "        'header_row_index': 0,  # Qualtrics question text is typically in row 0\n",
    "        'data_start_row': 1,    # Response data typically starts at row 1\n",
    "        'response_type_column': None,\n",
    "        'preview_responses_detected': False\n",
    "    }\n",
    "\n",
    "    # Check if there's a Status column to identify preview responses\n",
    "    if 'Status' in df.columns:\n",
    "        header_info['response_type_column'] = 'Status'\n",
    "\n",
    "        # Look for preview responses in first few rows\n",
    "        status_values = df['Status'].head(10).dropna().unique()\n",
    "        if any('preview' in str(val).lower() for val in status_values):\n",
    "            header_info['preview_responses_detected'] = True\n",
    "            print(\"✓ Preview responses detected in Status column\")\n",
    "\n",
    "    # Verify our assumptions by checking if row 0 contains question text\n",
    "    if len(df) > 0:\n",
    "        row_0_sample = df.iloc[0].dropna().head(3).tolist()\n",
    "        avg_text_length = np.mean([len(str(val)) for val in row_0_sample]) if row_0_sample else 0\n",
    "\n",
    "        if avg_text_length > 50:  # Long text suggests question descriptions\n",
    "            print(\"✓ Row 0 appears to contain question text (header row)\")\n",
    "        else:\n",
    "            print(\"⚠ Row 0 may not contain typical Qualtrics question text\")\n",
    "            header_info.setdefault('potential_issues', []).append(\"Row 0 structure atypical for Qualtrics\")\n",
    "\n",
    "    return header_info\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the Qualtrics file\n",
    "        result = load_qualtrics_export()\n",
    "\n",
    "        raw_data = result['raw_data']\n",
    "        file_info = result['file_info']\n",
    "        quality_check = result['quality_check']\n",
    "\n",
    "        print(f\"\\n=== Loading Summary ===\")\n",
    "        print(f\"File: {file_info['filename']}\")\n",
    "        print(f\"Qualtrics format: {quality_check['is_qualtrics_format']}\")\n",
    "        print(f\"Total responses: {raw_data.shape[0]:,}\")\n",
    "        print(f\"Total columns: {raw_data.shape[1]:,}\")\n",
    "        print(f\"Question columns: {len(quality_check['question_columns'])}\")\n",
    "        print(f\"Standard metadata columns: {len(quality_check['standard_qualtrics_columns'])}\")\n",
    "\n",
    "        # Show a sample of the data structure\n",
    "        print(f\"\\n=== Data Structure Preview ===\")\n",
    "        print(\"First few column names:\")\n",
    "        for i, col in enumerate(raw_data.columns[:8]):\n",
    "            print(f\"  {i}: {col}\")\n",
    "        if len(raw_data.columns) > 8:\n",
    "            print(f\"  ... and {len(raw_data.columns) - 8} more columns\")\n",
    "\n",
    "        print(f\"\\nFirst row sample (likely question text):\")\n",
    "        sample_row = raw_data.iloc[0].head(5)\n",
    "        for col, val in sample_row.items():\n",
    "            val_preview = str(val)[:60] + \"...\" if len(str(val)) > 60 else str(val)\n",
    "            print(f\"  {col}: {val_preview}\")\n",
    "\n",
    "        print(\"\\n✓ Step 1 Complete: File loaded and analyzed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 1: {str(e)}\")\n",
    "        print(\"Please check your file path and ensure it's a valid Qualtrics Excel export.\")"
   ],
   "id": "19357fb645c8d502",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Step 2: Data Structure Separation",
   "id": "725fdba3d757435b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract and Analyze Data Structure",
   "id": "7ed09c45e138a5d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2a: Extract and Analyze Data Structure\n",
    "# Objective: Separate codebook information from response data and identify the true structure\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_data_structure(result_from_step1):\n",
    "    \"\"\"\n",
    "    Analyze and extract the data structure from a Qualtrics export.\n",
    "    Handles different Qualtrics export formats robustly.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result_from_step1 : dict\n",
    "        Result dictionary from Step 1 containing raw_data, file_info, quality_check\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains separated codebook, response_data, and structure_analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 2a: Extracting Data Structure ===\")\n",
    "\n",
    "    raw_df = result_from_step1['raw_data']\n",
    "    quality_check = result_from_step1['quality_check']\n",
    "\n",
    "    # Initialize structure analysis\n",
    "    structure_analysis = {\n",
    "        'codebook_source': None,\n",
    "        'response_data_start_row': None,\n",
    "        'header_type': None,\n",
    "        'total_rows_analyzed': len(raw_df),\n",
    "        'metadata_columns': [],\n",
    "        'question_columns': [],\n",
    "        'response_types_found': []\n",
    "    }\n",
    "\n",
    "    # Identify metadata vs question columns\n",
    "    metadata_patterns = [\n",
    "        'StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "        'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "        'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "        'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "        'DistributionChannel', 'UserLanguage'\n",
    "    ]\n",
    "\n",
    "    all_columns = raw_df.columns.tolist()\n",
    "    metadata_cols = [col for col in all_columns if col in metadata_patterns]\n",
    "    question_cols = [col for col in all_columns if col not in metadata_patterns]\n",
    "\n",
    "    structure_analysis['metadata_columns'] = metadata_cols\n",
    "    structure_analysis['question_columns'] = question_cols\n",
    "\n",
    "    print(f\"Identified {len(metadata_cols)} metadata columns\")\n",
    "    print(f\"Identified {len(question_cols)} question/data columns\")\n",
    "\n",
    "    # Determine header structure and codebook location\n",
    "    codebook_info = analyze_codebook_structure(raw_df, structure_analysis)\n",
    "    structure_analysis.update(codebook_info)\n",
    "\n",
    "    # Separate codebook from response data\n",
    "    if structure_analysis['codebook_source'] == 'row_0':\n",
    "        # Traditional Qualtrics format - row 0 has question text\n",
    "        codebook_df = raw_df.iloc[[0]].copy()\n",
    "        response_data = raw_df.iloc[1:].copy()\n",
    "        print(\"Extracted codebook from row 0 (question text)\")\n",
    "\n",
    "    elif structure_analysis['codebook_source'] == 'column_names':\n",
    "        # Alternative format - column names are the questions\n",
    "        codebook_data = {'column_id': all_columns, 'question_text': all_columns}\n",
    "        codebook_df = pd.DataFrame(codebook_data)\n",
    "        response_data = raw_df.copy()\n",
    "        print(\"Using column names as codebook (no separate question text row)\")\n",
    "\n",
    "    elif structure_analysis['codebook_source'] == 'mixed':\n",
    "        # Hybrid format - some info in row 0, but not full questions\n",
    "        codebook_df = raw_df.iloc[[0]].copy()\n",
    "        response_data = raw_df.iloc[1:].copy()\n",
    "        print(\"Extracted partial codebook from row 0 (mixed format)\")\n",
    "\n",
    "    else:\n",
    "        # Fallback - treat as simple structure\n",
    "        codebook_data = {'column_id': all_columns, 'question_text': all_columns}\n",
    "        codebook_df = pd.DataFrame(codebook_data)\n",
    "        response_data = raw_df.copy()\n",
    "        print(\"Using fallback codebook structure\")\n",
    "\n",
    "    # Reset response data index\n",
    "    response_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Analyze response types and quality\n",
    "    response_analysis = analyze_response_types(response_data, structure_analysis)\n",
    "    structure_analysis.update(response_analysis)\n",
    "\n",
    "    return {\n",
    "        'codebook': codebook_df,\n",
    "        'response_data': response_data,\n",
    "        'structure_analysis': structure_analysis,\n",
    "        'metadata_columns': metadata_cols,\n",
    "        'question_columns': question_cols\n",
    "    }\n",
    "\n",
    "def analyze_codebook_structure(df, structure_info):\n",
    "    \"\"\"\n",
    "    Determine how the codebook/question text is stored in this export.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw data from Qualtrics\n",
    "    structure_info : dict\n",
    "        Current structure analysis\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Codebook structure information\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Analyzing Codebook Structure ---\")\n",
    "\n",
    "    codebook_info = {\n",
    "        'codebook_source': 'unknown',\n",
    "        'header_type': 'unknown',\n",
    "        'codebook_quality': 'unknown'\n",
    "    }\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"Warning: No data rows to analyze\")\n",
    "        return codebook_info\n",
    "\n",
    "    # Examine row 0 to see if it contains question text\n",
    "    row_0 = df.iloc[0]\n",
    "\n",
    "    # Calculate metrics to determine if row 0 has question text\n",
    "    non_null_values = row_0.dropna()\n",
    "    if len(non_null_values) == 0:\n",
    "        print(\"Row 0 is completely empty\")\n",
    "        codebook_info['codebook_source'] = 'column_names'\n",
    "        codebook_info['header_type'] = 'empty_row_0'\n",
    "        return codebook_info\n",
    "\n",
    "    # Analyze text characteristics of row 0\n",
    "    avg_length = np.mean([len(str(val)) for val in non_null_values])\n",
    "    max_length = max([len(str(val)) for val in non_null_values])\n",
    "\n",
    "    # Look for question-like patterns\n",
    "    question_patterns = 0\n",
    "    for val in non_null_values.head(10):  # Check first 10 non-null values\n",
    "        str_val = str(val).lower()\n",
    "        if any(pattern in str_val for pattern in ['which of', 'please', 'identify', 'provide', '?']):\n",
    "            question_patterns += 1\n",
    "\n",
    "    print(f\"Row 0 analysis:\")\n",
    "    print(f\"  Average text length: {avg_length:.1f} characters\")\n",
    "    print(f\"  Maximum text length: {max_length} characters\")\n",
    "    print(f\"  Question-like patterns found: {question_patterns}\")\n",
    "\n",
    "    # Decision logic for codebook source\n",
    "    if avg_length > 30 and question_patterns >= 2:\n",
    "        codebook_info['codebook_source'] = 'row_0'\n",
    "        codebook_info['header_type'] = 'full_questions'\n",
    "        codebook_info['codebook_quality'] = 'high'\n",
    "        print(\"✓ Row 0 contains full question text\")\n",
    "\n",
    "    elif avg_length > 15 and max_length > 20:\n",
    "        codebook_info['codebook_source'] = 'mixed'\n",
    "        codebook_info['header_type'] = 'descriptive_headers'\n",
    "        codebook_info['codebook_quality'] = 'medium'\n",
    "        print(\"✓ Row 0 contains descriptive headers (not full questions)\")\n",
    "\n",
    "    elif avg_length < 10:\n",
    "        codebook_info['codebook_source'] = 'column_names'\n",
    "        codebook_info['header_type'] = 'short_labels'\n",
    "        codebook_info['codebook_quality'] = 'low'\n",
    "        print(\"✓ Row 0 contains short labels - using column names as codebook\")\n",
    "\n",
    "    else:\n",
    "        codebook_info['codebook_source'] = 'mixed'\n",
    "        codebook_info['header_type'] = 'mixed_format'\n",
    "        codebook_info['codebook_quality'] = 'medium'\n",
    "        print(\"? Mixed format detected - treating as partial codebook\")\n",
    "\n",
    "    return codebook_info\n",
    "\n",
    "def analyze_response_types(df, structure_info):\n",
    "    \"\"\"\n",
    "    Analyze the types of responses in the data (real vs test responses).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Response data (after removing codebook row if applicable)\n",
    "    structure_info : dict\n",
    "        Current structure analysis\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Response type analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Analyzing Response Types ---\")\n",
    "\n",
    "    response_info = {\n",
    "        'total_responses': len(df),\n",
    "        'response_types_found': [],\n",
    "        'test_responses': 0,\n",
    "        'genuine_responses': 0,\n",
    "        'response_type_column': None\n",
    "    }\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"No response data to analyze\")\n",
    "        return response_info\n",
    "\n",
    "    # Look for Status column to identify response types\n",
    "    status_columns = [col for col in df.columns if 'status' in col.lower()]\n",
    "\n",
    "    if status_columns:\n",
    "        status_col = status_columns[0]  # Use first status column found\n",
    "        response_info['response_type_column'] = status_col\n",
    "\n",
    "        # Analyze response types\n",
    "        status_counts = df[status_col].value_counts(dropna=False)\n",
    "        response_info['response_types_found'] = status_counts.to_dict()\n",
    "\n",
    "        print(f\"Response types found in '{status_col}':\")\n",
    "        for resp_type, count in status_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {resp_type}: {count} responses ({percentage:.1f}%)\")\n",
    "\n",
    "            # Categorize as test vs genuine\n",
    "            if pd.isna(resp_type):\n",
    "                continue\n",
    "            elif any(keyword in str(resp_type).lower() for keyword in ['preview', 'test', 'spam']):\n",
    "                response_info['test_responses'] += count\n",
    "            else:\n",
    "                response_info['genuine_responses'] += count\n",
    "\n",
    "    else:\n",
    "        print(\"No Status column found - assuming all responses are genuine\")\n",
    "        response_info['genuine_responses'] = len(df)\n",
    "\n",
    "    # Additional data quality checks\n",
    "    completion_info = analyze_completion_patterns(df)\n",
    "    response_info.update(completion_info)\n",
    "\n",
    "    return response_info\n",
    "\n",
    "def analyze_completion_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze completion patterns and data quality indicators.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Response data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Completion analysis\n",
    "    \"\"\"\n",
    "\n",
    "    completion_info = {\n",
    "        'has_progress_data': False,\n",
    "        'has_duration_data': False,\n",
    "        'completion_rate_available': False,\n",
    "        'data_quality_indicators': []\n",
    "    }\n",
    "\n",
    "    # Check for progress tracking\n",
    "    progress_columns = [col for col in df.columns if 'progress' in col.lower()]\n",
    "    if progress_columns:\n",
    "        completion_info['has_progress_data'] = True\n",
    "        progress_col = progress_columns[0]\n",
    "\n",
    "        # Analyze completion rates\n",
    "        if df[progress_col].notna().sum() > 0:\n",
    "            completion_info['completion_rate_available'] = True\n",
    "            complete_responses = (df[progress_col] == 100).sum() if (df[progress_col] == 100).any() else 0\n",
    "            completion_rate = (complete_responses / len(df)) * 100\n",
    "            completion_info['completion_rate'] = completion_rate\n",
    "            print(f\"Survey completion rate: {completion_rate:.1f}%\")\n",
    "\n",
    "    # Check for duration data\n",
    "    duration_columns = [col for col in df.columns if 'duration' in col.lower()]\n",
    "    if duration_columns:\n",
    "        completion_info['has_duration_data'] = True\n",
    "        duration_col = duration_columns[0]\n",
    "\n",
    "        if df[duration_col].notna().sum() > 0:\n",
    "            median_duration = df[duration_col].median()\n",
    "            completion_info['median_duration_seconds'] = median_duration\n",
    "            print(f\"Median completion time: {median_duration/60:.1f} minutes\")\n",
    "\n",
    "    # Data quality flags\n",
    "    if completion_info.get('completion_rate', 100) < 70:\n",
    "        completion_info['data_quality_indicators'].append('Low completion rate')\n",
    "\n",
    "    return completion_info\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # This assumes 'result' exists from Step 1\n",
    "    try:\n",
    "        structure_result = extract_data_structure(result)\n",
    "\n",
    "        codebook = structure_result['codebook']\n",
    "        response_data = structure_result['response_data']\n",
    "        analysis = structure_result['structure_analysis']\n",
    "\n",
    "        print(f\"\\n=== Step 2a Summary ===\")\n",
    "        print(f\"Codebook source: {analysis['codebook_source']}\")\n",
    "        print(f\"Codebook shape: {codebook.shape}\")\n",
    "        print(f\"Response data shape: {response_data.shape}\")\n",
    "        print(f\"Total genuine responses: {analysis.get('genuine_responses', 'Unknown')}\")\n",
    "        print(f\"Total test responses: {analysis.get('test_responses', 0)}\")\n",
    "\n",
    "        if analysis.get('completion_rate_available'):\n",
    "            print(f\"Completion rate: {analysis.get('completion_rate', 0):.1f}%\")\n",
    "\n",
    "        print(\"\\n✓ Step 2a Complete: Data structure extracted and analyzed\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"❌ Please run Step 1 first to create the 'result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 2a: {str(e)}\")"
   ],
   "id": "8f6b9e637b9a7a6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean and Filter Response Data",
   "id": "9b70d95c953a08b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2b: Clean and Filter Response Data\n",
    "# Objective: Remove test responses, apply initial cleaning, create quality flags\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_and_filter_responses(structure_result):\n",
    "    \"\"\"\n",
    "    Clean the response data by removing test responses and applying universal cleaning rules.\n",
    "    Create data quality flags for transparent analysis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    structure_result : dict\n",
    "        Result from Step 2a containing codebook, response_data, and structure_analysis\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains cleaned_data, quality_flags, cleaning_log, and summary_stats\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 2b: Cleaning and Filtering Response Data ===\")\n",
    "\n",
    "    response_data = structure_result['response_data'].copy()\n",
    "    analysis = structure_result['structure_analysis']\n",
    "\n",
    "    cleaning_log = []\n",
    "    initial_row_count = len(response_data)\n",
    "\n",
    "    print(f\"Starting with {initial_row_count:,} total responses\")\n",
    "\n",
    "    # Step 1: Remove test/preview responses\n",
    "    genuine_data = filter_test_responses(response_data, analysis, cleaning_log)\n",
    "\n",
    "    # Step 2: Create data quality flags (before removing any data)\n",
    "    flagged_data = create_quality_flags(genuine_data, analysis, cleaning_log)\n",
    "\n",
    "    # Step 3: Apply universal cleaning rules\n",
    "    cleaned_data = apply_universal_cleaning(flagged_data, cleaning_log)\n",
    "\n",
    "    # Step 4: Standardize column names\n",
    "    final_data = standardize_column_names(cleaned_data, structure_result['metadata_columns'],\n",
    "                                        structure_result['question_columns'], cleaning_log)\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = generate_cleaning_summary(initial_row_count, final_data, cleaning_log)\n",
    "\n",
    "    return {\n",
    "        'cleaned_data': final_data,\n",
    "        'cleaning_log': cleaning_log,\n",
    "        'summary_stats': summary_stats,\n",
    "        'original_columns': response_data.columns.tolist(),\n",
    "        'final_columns': final_data.columns.tolist()\n",
    "    }\n",
    "\n",
    "def filter_test_responses(df, analysis, cleaning_log):\n",
    "    \"\"\"\n",
    "    Remove test responses (Survey Preview, Spam, etc.) while preserving genuine responses.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Response data\n",
    "    analysis : dict\n",
    "        Structure analysis from Step 2a\n",
    "    cleaning_log : list\n",
    "        Log of cleaning operations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Data with test responses removed\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Filtering Test Responses ---\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Define test response patterns (case-insensitive)\n",
    "    test_patterns = [\n",
    "        'survey preview', 'preview', 'test', 'spam', 'survey test'\n",
    "    ]\n",
    "\n",
    "    if analysis.get('response_type_column'):\n",
    "        status_col = analysis['response_type_column']\n",
    "\n",
    "        # Identify test responses\n",
    "        test_mask = pd.Series([False] * len(df), index=df.index)\n",
    "\n",
    "        for pattern in test_patterns:\n",
    "            pattern_mask = df[status_col].astype(str).str.lower().str.contains(pattern, na=False)\n",
    "            test_mask = test_mask | pattern_mask\n",
    "\n",
    "        test_count = test_mask.sum()\n",
    "        genuine_data = df[~test_mask].copy().reset_index(drop=True)\n",
    "\n",
    "        cleaning_log.append(f\"Removed {test_count} test responses based on Status column patterns\")\n",
    "        print(f\"Removed {test_count} test responses ({test_count/initial_count*100:.1f}%)\")\n",
    "        print(f\"Retained {len(genuine_data)} genuine responses ({len(genuine_data)/initial_count*100:.1f}%)\")\n",
    "\n",
    "    else:\n",
    "        # No status column - assume all are genuine\n",
    "        genuine_data = df.copy()\n",
    "        cleaning_log.append(\"No status column found - retained all responses as genuine\")\n",
    "        print(\"No status column found - assuming all responses are genuine\")\n",
    "\n",
    "    return genuine_data\n",
    "\n",
    "def create_quality_flags(df, analysis, cleaning_log):\n",
    "    \"\"\"\n",
    "    Create data quality flags without removing data - for transparent analysis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Genuine response data\n",
    "    analysis : dict\n",
    "        Structure analysis from Step 2a\n",
    "    cleaning_log : list\n",
    "        Log of cleaning operations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Data with quality flag columns added\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Creating Data Quality Flags ---\")\n",
    "\n",
    "    flagged_data = df.copy()\n",
    "    flags_created = 0\n",
    "\n",
    "    # Flag 1: Survey completion status\n",
    "    if analysis.get('has_progress_data'):\n",
    "        progress_cols = [col for col in df.columns if 'progress' in col.lower()]\n",
    "        if progress_cols:\n",
    "            progress_col = progress_cols[0]\n",
    "            flagged_data['flag_incomplete'] = df[progress_col] < 100\n",
    "            incomplete_count = flagged_data['flag_incomplete'].sum()\n",
    "            print(f\"Created flag_incomplete: {incomplete_count} responses ({incomplete_count/len(df)*100:.1f}%)\")\n",
    "            flags_created += 1\n",
    "\n",
    "    # Flag 2: Duration outliers\n",
    "    if analysis.get('has_duration_data'):\n",
    "        duration_cols = [col for col in df.columns if 'duration' in col.lower()]\n",
    "        if duration_cols:\n",
    "            duration_col = duration_cols[0]\n",
    "\n",
    "            # Convert to numeric if needed\n",
    "            duration_numeric = pd.to_numeric(df[duration_col], errors='coerce')\n",
    "\n",
    "            # Flag very short responses (< 60 seconds)\n",
    "            flagged_data['flag_duration_too_short'] = duration_numeric < 60\n",
    "            short_count = flagged_data['flag_duration_too_short'].sum()\n",
    "\n",
    "            # Flag very long responses (> 2 hours = 7200 seconds)\n",
    "            flagged_data['flag_duration_too_long'] = duration_numeric > 7200\n",
    "            long_count = flagged_data['flag_duration_too_long'].sum()\n",
    "\n",
    "            print(f\"Created flag_duration_too_short: {short_count} responses ({short_count/len(df)*100:.1f}%)\")\n",
    "            print(f\"Created flag_duration_too_long: {long_count} responses ({long_count/len(df)*100:.1f}%)\")\n",
    "            flags_created += 2\n",
    "\n",
    "    # Flag 3: Response pattern flags (straight-lining, etc.)\n",
    "    pattern_flags = create_response_pattern_flags(df, cleaning_log)\n",
    "    for flag_name, flag_data in pattern_flags.items():\n",
    "        flagged_data[flag_name] = flag_data\n",
    "        flag_count = flag_data.sum()\n",
    "        print(f\"Created {flag_name}: {flag_count} responses ({flag_count/len(df)*100:.1f}%)\")\n",
    "        flags_created += 1\n",
    "\n",
    "    # Summary of flagging\n",
    "    total_flagged = flagged_data[[col for col in flagged_data.columns if col.startswith('flag_')]].any(axis=1).sum()\n",
    "    print(f\"\\nTotal flags created: {flags_created}\")\n",
    "    print(f\"Responses with any flag: {total_flagged} ({total_flagged/len(df)*100:.1f}%)\")\n",
    "    print(f\"Clean responses (no flags): {len(df)-total_flagged} ({(len(df)-total_flagged)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    cleaning_log.append(f\"Created {flags_created} quality flag types affecting {total_flagged} responses\")\n",
    "\n",
    "    return flagged_data\n",
    "\n",
    "def create_response_pattern_flags(df, cleaning_log):\n",
    "    \"\"\"\n",
    "    Create flags for suspicious response patterns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Response data\n",
    "    cleaning_log : list\n",
    "        Cleaning log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of flag name -> boolean Series\n",
    "    \"\"\"\n",
    "\n",
    "    pattern_flags = {}\n",
    "\n",
    "    # Find potential rating/scale columns (likely to show straight-lining)\n",
    "    question_cols = [col for col in df.columns if not any(meta in col for meta in\n",
    "                    ['Date', 'Status', 'IP', 'Progress', 'Duration', 'Finished', 'Recorded', 'Response', 'Recipient', 'External', 'Location', 'Distribution', 'Language'])]\n",
    "\n",
    "    # Look for columns that might be scales (numeric responses)\n",
    "    numeric_question_cols = []\n",
    "    for col in question_cols:\n",
    "        # Try to convert to numeric and see if it's reasonable scale data\n",
    "        numeric_vals = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        if len(numeric_vals) > 0:\n",
    "            unique_vals = numeric_vals.unique()\n",
    "            if len(unique_vals) <= 10 and numeric_vals.min() >= 0 and numeric_vals.max() <= 10:\n",
    "                numeric_question_cols.append(col)\n",
    "\n",
    "    # Flag potential straight-lining if we have scale columns\n",
    "    if len(numeric_question_cols) >= 3:\n",
    "        straightline_flags = []\n",
    "        for idx, row in df.iterrows():\n",
    "            scale_responses = []\n",
    "            for col in numeric_question_cols[:10]:  # Check up to 10 scale columns\n",
    "                val = pd.to_numeric(row[col], errors='coerce')\n",
    "                if not pd.isna(val):\n",
    "                    scale_responses.append(val)\n",
    "\n",
    "            # Flag if 80% or more of scale responses are identical (and we have at least 3 responses)\n",
    "            if len(scale_responses) >= 3:\n",
    "                most_common_val = max(set(scale_responses), key=scale_responses.count)\n",
    "                same_response_pct = scale_responses.count(most_common_val) / len(scale_responses)\n",
    "                straightline_flags.append(same_response_pct >= 0.8)\n",
    "            else:\n",
    "                straightline_flags.append(False)\n",
    "\n",
    "        pattern_flags['flag_potential_straightlining'] = pd.Series(straightline_flags, index=df.index)\n",
    "\n",
    "    return pattern_flags\n",
    "\n",
    "def apply_universal_cleaning(df, cleaning_log):\n",
    "    \"\"\"\n",
    "    Apply universal data cleaning rules that work across all surveys.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Flagged data\n",
    "    cleaning_log : list\n",
    "        Cleaning log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Cleaned data\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Applying Universal Cleaning Rules ---\")\n",
    "\n",
    "    cleaned_data = df.copy()\n",
    "    changes_made = 0\n",
    "\n",
    "    # Rule 1: Standardize NA representations\n",
    "    na_patterns = {\n",
    "        r'^\\s*n\\s*a\\s*$': np.nan,           # \"n a\", \"N A\", \" na \", etc.\n",
    "        r'^\\s*n/a\\s*$': np.nan,             # \"n/a\", \"N/A\", \" n/a \", etc.\n",
    "        r'^\\s*na\\s*$': np.nan,              # \"na\", \"NA\", \" na \", etc.\n",
    "        r'^\\s*none\\s*$': np.nan,            # \"none\", \"None\", \" none \", etc.\n",
    "        r'^\\s*null\\s*$': np.nan,            # \"null\", \"Null\", etc.\n",
    "        r'^\\s*$': np.nan                    # Empty strings and whitespace-only\n",
    "    }\n",
    "\n",
    "    initial_nulls = cleaned_data.isnull().sum().sum()\n",
    "\n",
    "    for pattern, replacement in na_patterns.items():\n",
    "        for col in cleaned_data.columns:\n",
    "            if cleaned_data[col].dtype == 'object':\n",
    "                mask = cleaned_data[col].astype(str).str.match(pattern, case=False, na=False)\n",
    "                if mask.any():\n",
    "                    cleaned_data.loc[mask, col] = replacement\n",
    "                    changes_made += mask.sum()\n",
    "\n",
    "    final_nulls = cleaned_data.isnull().sum().sum()\n",
    "    na_changes = final_nulls - initial_nulls\n",
    "\n",
    "    if na_changes > 0:\n",
    "        print(f\"Standardized NA representations: +{na_changes} null values created\")\n",
    "        cleaning_log.append(f\"Standardized {na_changes} NA representations to null values\")\n",
    "\n",
    "    # Rule 2: Standardize Yes/No responses\n",
    "    yes_no_patterns = {\n",
    "        r'^\\s*yes\\s*\\.?\\s*$': 'Yes',        # \"yes\", \"Yes.\", \" yes \", etc.\n",
    "        r'^\\s*no\\s*\\.?\\s*$': 'No',          # \"no\", \"No.\", \" no \", etc.\n",
    "        r'^\\s*y\\s*$': 'Yes',                # \"y\", \"Y\"\n",
    "        r'^\\s*n\\s*$': 'No'                  # \"n\", \"N\" (but be careful not to catch legitimate \"n\")\n",
    "    }\n",
    "\n",
    "    yes_no_changes = 0\n",
    "    for pattern, replacement in yes_no_patterns.items():\n",
    "        for col in cleaned_data.columns:\n",
    "            if cleaned_data[col].dtype == 'object':\n",
    "                mask = cleaned_data[col].astype(str).str.match(pattern, case=False, na=False)\n",
    "                if mask.any():\n",
    "                    cleaned_data.loc[mask, col] = replacement\n",
    "                    yes_no_changes += mask.sum()\n",
    "\n",
    "    if yes_no_changes > 0:\n",
    "        print(f\"Standardized Yes/No responses: {yes_no_changes} changes made\")\n",
    "        cleaning_log.append(f\"Standardized {yes_no_changes} Yes/No response formats\")\n",
    "\n",
    "    print(f\"Universal cleaning complete: {changes_made + yes_no_changes} total changes\")\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def standardize_column_names(df, metadata_cols, question_cols, cleaning_log):\n",
    "    \"\"\"\n",
    "    Standardize column names for consistent analysis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Cleaned data\n",
    "    metadata_cols : list\n",
    "        List of metadata column names\n",
    "    question_cols : list\n",
    "        List of question column names\n",
    "    cleaning_log : list\n",
    "        Cleaning log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Data with standardized column names\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Standardizing Column Names ---\")\n",
    "\n",
    "    final_data = df.copy()\n",
    "\n",
    "    # Create column name mapping\n",
    "    column_mapping = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Clean column names: remove special characters, standardize spacing\n",
    "        clean_name = re.sub(r'[^\\w\\s]', '_', col)  # Replace special chars with underscore\n",
    "        clean_name = re.sub(r'\\s+', '_', clean_name)  # Replace spaces with underscore\n",
    "        clean_name = re.sub(r'_+', '_', clean_name)  # Replace multiple underscores with single\n",
    "        clean_name = clean_name.strip('_').lower()  # Remove leading/trailing underscores and lowercase\n",
    "\n",
    "        # Ensure name is not empty\n",
    "        if not clean_name:\n",
    "            clean_name = f\"col_{df.columns.get_loc(col)}\"\n",
    "\n",
    "        column_mapping[col] = clean_name\n",
    "\n",
    "    # Apply column name changes\n",
    "    final_data.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    # Report changes\n",
    "    changes_made = sum(1 for old, new in column_mapping.items() if old != new)\n",
    "    print(f\"Standardized {changes_made} column names\")\n",
    "\n",
    "    # Show examples of changes\n",
    "    if changes_made > 0:\n",
    "        print(\"Sample column name changes:\")\n",
    "        examples_shown = 0\n",
    "        for old, new in column_mapping.items():\n",
    "            if old != new and examples_shown < 5:\n",
    "                print(f\"  '{old}' → '{new}'\")\n",
    "                examples_shown += 1\n",
    "        if changes_made > 5:\n",
    "            print(f\"  ... and {changes_made - 5} more changes\")\n",
    "\n",
    "    cleaning_log.append(f\"Standardized {changes_made} column names for consistency\")\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def generate_cleaning_summary(initial_count, final_data, cleaning_log):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of cleaning operations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    initial_count : int\n",
    "        Initial number of responses\n",
    "    final_data : pandas.DataFrame\n",
    "        Final cleaned data\n",
    "    cleaning_log : list\n",
    "        Log of all cleaning operations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Summary statistics\n",
    "    \"\"\"\n",
    "\n",
    "    final_count = len(final_data)\n",
    "    flag_columns = [col for col in final_data.columns if col.startswith('flag_')]\n",
    "\n",
    "    summary = {\n",
    "        'initial_responses': initial_count,\n",
    "        'final_responses': final_count,\n",
    "        'responses_removed': initial_count - final_count,\n",
    "        'removal_rate_pct': ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0,\n",
    "        'quality_flags_created': len(flag_columns),\n",
    "        'flagged_responses': final_data[flag_columns].any(axis=1).sum() if flag_columns else 0,\n",
    "        'clean_responses': final_count - (final_data[flag_columns].any(axis=1).sum() if flag_columns else 0),\n",
    "        'cleaning_operations': len(cleaning_log)\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'structure_result' exists from Step 2a\n",
    "        cleaning_result = clean_and_filter_responses(structure_result)\n",
    "\n",
    "        cleaned_data = cleaning_result['cleaned_data']\n",
    "        summary = cleaning_result['summary_stats']\n",
    "\n",
    "        print(f\"\\n=== Step 2b Summary ===\")\n",
    "        print(f\"Data cleaning completed:\")\n",
    "        print(f\"  Initial responses: {summary['initial_responses']:,}\")\n",
    "        print(f\"  Final responses: {summary['final_responses']:,}\")\n",
    "        print(f\"  Responses removed: {summary['responses_removed']} ({summary['removal_rate_pct']:.1f}%)\")\n",
    "        print(f\"  Quality flags created: {summary['quality_flags_created']}\")\n",
    "        print(f\"  Flagged responses: {summary['flagged_responses']} ({summary['flagged_responses']/summary['final_responses']*100:.1f}%)\")\n",
    "        print(f\"  Clean responses: {summary['clean_responses']} ({summary['clean_responses']/summary['final_responses']*100:.1f}%)\")\n",
    "        print(f\"  Final data shape: {cleaned_data.shape[0]:,} × {cleaned_data.shape[1]:,}\")\n",
    "\n",
    "        print(\"\\n✓ Step 2b Complete: Data cleaned and quality flags created\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"❌ Please run Step 2a first to create the 'structure_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 2b: {str(e)}\")"
   ],
   "id": "f709bfb13cc45f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Data Type Optimization and Validation",
   "id": "efe9bab2b7d75d9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Intelligent Data Type Detection",
   "id": "c848b5d5b7da9dcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3a: Intelligent Data Type Detection\n",
    "# Objective: Automatically detect and assign appropriate data types based on content analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_and_assign_data_types(cleaning_result):\n",
    "    \"\"\"\n",
    "    Intelligently detect and assign appropriate data types for all columns.\n",
    "    Works with any survey content without hardcoded assumptions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cleaning_result : dict\n",
    "        Result from Step 2b containing cleaned_data and metadata\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains typed_data, type_analysis, conversion_log, and validation_results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 3a: Intelligent Data Type Detection ===\")\n",
    "\n",
    "    df = cleaning_result['cleaned_data'].copy()\n",
    "    initial_dtypes = df.dtypes.to_dict()\n",
    "\n",
    "    # Initialize tracking structures\n",
    "    type_analysis = {\n",
    "        'columns_analyzed': len(df.columns),\n",
    "        'conversions_attempted': 0,\n",
    "        'conversions_successful': 0,\n",
    "        'columns_by_final_type': {},\n",
    "        'problematic_columns': []\n",
    "    }\n",
    "\n",
    "    conversion_log = []\n",
    "\n",
    "    # Skip flag columns and ID columns from type conversion\n",
    "    skip_columns = get_columns_to_skip(df)\n",
    "\n",
    "    # Analyze each column for optimal data type\n",
    "    typed_data = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in skip_columns:\n",
    "            conversion_log.append(f\"{col}: Skipped (administrative/flag column)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing column: {col}\")\n",
    "        type_result = analyze_column_for_type(df[col], col)\n",
    "\n",
    "        if type_result['recommended_type'] != 'object':\n",
    "            type_analysis['conversions_attempted'] += 1\n",
    "\n",
    "            # Attempt the conversion\n",
    "            conversion_success = apply_type_conversion(typed_data, col, type_result, conversion_log)\n",
    "\n",
    "            if conversion_success:\n",
    "                type_analysis['conversions_successful'] += 1\n",
    "            else:\n",
    "                type_analysis['problematic_columns'].append(col)\n",
    "\n",
    "    # Categorize final types\n",
    "    final_dtypes = typed_data.dtypes.to_dict()\n",
    "    type_analysis['columns_by_final_type'] = categorize_final_types(final_dtypes)\n",
    "\n",
    "    # Validate type assignments\n",
    "    validation_results = validate_type_assignments(typed_data, conversion_log)\n",
    "\n",
    "    return {\n",
    "        'typed_data': typed_data,\n",
    "        'type_analysis': type_analysis,\n",
    "        'conversion_log': conversion_log,\n",
    "        'validation_results': validation_results,\n",
    "        'initial_dtypes': initial_dtypes,\n",
    "        'final_dtypes': final_dtypes\n",
    "    }\n",
    "\n",
    "def get_columns_to_skip(df):\n",
    "    \"\"\"\n",
    "    Identify columns that should not undergo type conversion.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data to analyze\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list : Column names to skip\n",
    "    \"\"\"\n",
    "\n",
    "    skip_patterns = [\n",
    "        r'^flag_',           # Quality flags\n",
    "        r'.*id$',            # ID columns\n",
    "        r'.*address$',       # IP addresses\n",
    "        r'^responseid$',     # Response IDs\n",
    "        r'^ipaddress$'       # IP addresses\n",
    "    ]\n",
    "\n",
    "    skip_columns = []\n",
    "    for col in df.columns:\n",
    "        for pattern in skip_patterns:\n",
    "            if re.match(pattern, col, re.IGNORECASE):\n",
    "                skip_columns.append(col)\n",
    "                break\n",
    "\n",
    "    return skip_columns\n",
    "\n",
    "def analyze_column_for_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Analyze a single column to determine the optimal data type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Column data to analyze\n",
    "    col_name : str\n",
    "        Name of the column\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Analysis results with recommended type and confidence\n",
    "    \"\"\"\n",
    "\n",
    "    # Get non-null values for analysis\n",
    "    non_null_data = series.dropna()\n",
    "\n",
    "    if len(non_null_data) == 0:\n",
    "        return {\n",
    "            'recommended_type': 'object',\n",
    "            'confidence': 'high',\n",
    "            'reason': 'all_null',\n",
    "            'sample_values': []\n",
    "        }\n",
    "\n",
    "    unique_values = non_null_data.unique()\n",
    "    sample_values = list(unique_values[:5])\n",
    "\n",
    "    # Date/Time Detection\n",
    "    datetime_result = detect_datetime_type(non_null_data, col_name)\n",
    "    if datetime_result['is_datetime']:\n",
    "        return {\n",
    "            'recommended_type': 'datetime',\n",
    "            'confidence': datetime_result['confidence'],\n",
    "            'reason': datetime_result['reason'],\n",
    "            'sample_values': sample_values\n",
    "        }\n",
    "\n",
    "    # Numeric Detection\n",
    "    numeric_result = detect_numeric_type(non_null_data, col_name)\n",
    "    if numeric_result['is_numeric']:\n",
    "        return {\n",
    "            'recommended_type': numeric_result['numeric_subtype'],\n",
    "            'confidence': numeric_result['confidence'],\n",
    "            'reason': numeric_result['reason'],\n",
    "            'sample_values': sample_values,\n",
    "            'contamination_rate': numeric_result.get('contamination_rate', 0)\n",
    "        }\n",
    "\n",
    "    # Boolean/Binary Detection\n",
    "    boolean_result = detect_boolean_type(non_null_data, col_name)\n",
    "    if boolean_result['is_boolean']:\n",
    "        return {\n",
    "            'recommended_type': 'category',\n",
    "            'confidence': boolean_result['confidence'],\n",
    "            'reason': boolean_result['reason'],\n",
    "            'sample_values': sample_values\n",
    "        }\n",
    "\n",
    "    # Categorical Detection\n",
    "    categorical_result = detect_categorical_type(non_null_data, col_name)\n",
    "    if categorical_result['is_categorical']:\n",
    "        return {\n",
    "            'recommended_type': 'category',\n",
    "            'confidence': categorical_result['confidence'],\n",
    "            'reason': categorical_result['reason'],\n",
    "            'sample_values': sample_values\n",
    "        }\n",
    "\n",
    "    # Default to object (text)\n",
    "    return {\n",
    "        'recommended_type': 'object',\n",
    "        'confidence': 'high',\n",
    "        'reason': 'free_text_or_complex',\n",
    "        'sample_values': sample_values\n",
    "    }\n",
    "\n",
    "def detect_datetime_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect if column contains date/time data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check column name patterns first\n",
    "    datetime_name_patterns = [r'date', r'time', r'timestamp']\n",
    "    name_suggests_datetime = any(re.search(pattern, col_name, re.IGNORECASE)\n",
    "                                for pattern in datetime_name_patterns)\n",
    "\n",
    "    if not name_suggests_datetime:\n",
    "        return {'is_datetime': False, 'confidence': 'low', 'reason': 'name_pattern_mismatch'}\n",
    "\n",
    "    # Try to parse as datetime\n",
    "    try:\n",
    "        parsed = pd.to_datetime(series, errors='coerce')\n",
    "        success_rate = parsed.notna().sum() / len(series)\n",
    "\n",
    "        if success_rate >= 0.8:\n",
    "            return {\n",
    "                'is_datetime': True,\n",
    "                'confidence': 'high' if success_rate >= 0.95 else 'medium',\n",
    "                'reason': f'{success_rate:.0%}_successful_datetime_parsing'\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {'is_datetime': False, 'confidence': 'low', 'reason': 'datetime_parsing_failed'}\n",
    "\n",
    "def detect_numeric_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect if column contains numeric data, handling contamination.\n",
    "    \"\"\"\n",
    "\n",
    "    # Attempt numeric conversion\n",
    "    try:\n",
    "        numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "        numeric_count = numeric_series.notna().sum()\n",
    "        total_count = series.notna().sum()\n",
    "\n",
    "        if total_count == 0:\n",
    "            return {'is_numeric': False, 'confidence': 'high', 'reason': 'no_data'}\n",
    "\n",
    "        success_rate = numeric_count / total_count\n",
    "        contamination_rate = 1 - success_rate\n",
    "\n",
    "        # High success rate - clearly numeric\n",
    "        if success_rate >= 0.9:\n",
    "            # Determine if integer or float\n",
    "            if numeric_series.dropna().apply(lambda x: x == int(x)).all():\n",
    "                subtype = 'int64'\n",
    "            else:\n",
    "                subtype = 'float64'\n",
    "\n",
    "            return {\n",
    "                'is_numeric': True,\n",
    "                'numeric_subtype': subtype,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'{success_rate:.0%}_numeric_with_{contamination_rate:.0%}_contamination',\n",
    "                'contamination_rate': contamination_rate\n",
    "            }\n",
    "\n",
    "        # Medium success rate - might be numeric with contamination\n",
    "        elif success_rate >= 0.7:\n",
    "            return {\n",
    "                'is_numeric': True,\n",
    "                'numeric_subtype': 'float64',  # Use float to handle mixed cases\n",
    "                'confidence': 'medium',\n",
    "                'reason': f'{success_rate:.0%}_numeric_contaminated',\n",
    "                'contamination_rate': contamination_rate\n",
    "            }\n",
    "\n",
    "        # Low success rate - not primarily numeric\n",
    "        else:\n",
    "            return {\n",
    "                'is_numeric': False,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'only_{success_rate:.0%}_numeric'\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'is_numeric': False,\n",
    "            'confidence': 'high',\n",
    "            'reason': f'numeric_conversion_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "def detect_boolean_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect binary/boolean columns (Yes/No, True/False, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    unique_values = set(str(val).lower().strip() for val in series.unique())\n",
    "\n",
    "    # Common boolean patterns\n",
    "    boolean_patterns = [\n",
    "        {'yes', 'no'},\n",
    "        {'true', 'false'},\n",
    "        {'1', '0'},\n",
    "        {'y', 'n'},\n",
    "        {'on', 'off'},\n",
    "        {'enabled', 'disabled'}\n",
    "    ]\n",
    "\n",
    "    for pattern in boolean_patterns:\n",
    "        if unique_values.issubset(pattern) and len(unique_values) >= 2:\n",
    "            return {\n",
    "                'is_boolean': True,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'binary_values_{list(unique_values)}'\n",
    "            }\n",
    "\n",
    "    # Single value that could be boolean (all Yes, all No, etc.)\n",
    "    if len(unique_values) == 1 and list(unique_values)[0] in ['yes', 'no', 'true', 'false', '1', '0']:\n",
    "        return {\n",
    "            'is_boolean': True,\n",
    "            'confidence': 'medium',\n",
    "            'reason': f'single_boolean_value_{list(unique_values)[0]}'\n",
    "        }\n",
    "\n",
    "    return {'is_boolean': False, 'confidence': 'high', 'reason': 'not_binary_pattern'}\n",
    "\n",
    "def detect_categorical_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect categorical columns based on repetition patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_count = len(series.unique())\n",
    "    total_count = len(series)\n",
    "\n",
    "    if total_count == 0:\n",
    "        return {'is_categorical': False, 'confidence': 'high', 'reason': 'no_data'}\n",
    "\n",
    "    # Calculate repetition ratio\n",
    "    repetition_ratio = total_count / unique_count if unique_count > 0 else 0\n",
    "\n",
    "    # Categorical if:\n",
    "    # 1. Few unique values with high repetition\n",
    "    # 2. Reasonable number of categories (not too many, not too few)\n",
    "\n",
    "    if unique_count <= 2:\n",
    "        # Very few categories - likely categorical\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'high',\n",
    "            'reason': f'{unique_count}_unique_values_high_repetition'\n",
    "        }\n",
    "    elif unique_count <= 10 and repetition_ratio >= 2:\n",
    "        # Moderate categories with good repetition\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'high' if repetition_ratio >= 5 else 'medium',\n",
    "            'reason': f'{unique_count}_categories_repetition_{repetition_ratio:.1f}x'\n",
    "        }\n",
    "    elif unique_count <= 20 and repetition_ratio >= 5:\n",
    "        # More categories but very high repetition\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'medium',\n",
    "            'reason': f'{unique_count}_categories_high_repetition_{repetition_ratio:.1f}x'\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'is_categorical': False,\n",
    "        'confidence': 'high',\n",
    "        'reason': f'too_many_unique_values_{unique_count}_or_low_repetition_{repetition_ratio:.1f}x'\n",
    "    }\n",
    "\n",
    "def apply_type_conversion(df, col_name, type_result, conversion_log):\n",
    "    \"\"\"\n",
    "    Apply the recommended type conversion to a column.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to modify\n",
    "    col_name : str\n",
    "        Column to convert\n",
    "    type_result : dict\n",
    "        Type analysis results\n",
    "    conversion_log : list\n",
    "        Log of conversions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    bool : True if conversion successful\n",
    "    \"\"\"\n",
    "\n",
    "    recommended_type = type_result['recommended_type']\n",
    "\n",
    "    try:\n",
    "        if recommended_type == 'datetime':\n",
    "            df[col_name] = pd.to_datetime(df[col_name], errors='coerce')\n",
    "            conversion_log.append(f\"{col_name}: Converted to datetime - {type_result['reason']}\")\n",
    "            print(f\"  ✓ Converted to datetime\")\n",
    "            return True\n",
    "\n",
    "        elif recommended_type in ['int64', 'float64']:\n",
    "            original_nulls = df[col_name].isnull().sum()\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "            new_nulls = df[col_name].isnull().sum()\n",
    "            contamination_nulls = new_nulls - original_nulls\n",
    "\n",
    "            if recommended_type == 'int64':\n",
    "                # Only convert to int if no fractional parts\n",
    "                if df[col_name].dropna().apply(lambda x: x == int(x) if pd.notna(x) else True).all():\n",
    "                    df[col_name] = df[col_name].astype('Int64')  # Nullable integer\n",
    "                else:\n",
    "                    recommended_type = 'float64'  # Fall back to float\n",
    "\n",
    "            conversion_log.append(f\"{col_name}: Converted to {recommended_type} - {type_result['reason']} - {contamination_nulls} values became null\")\n",
    "            print(f\"  ✓ Converted to {recommended_type} ({contamination_nulls} contaminated values → null)\")\n",
    "            return True\n",
    "\n",
    "        elif recommended_type == 'category':\n",
    "            df[col_name] = df[col_name].astype('category')\n",
    "            conversion_log.append(f\"{col_name}: Converted to category - {type_result['reason']}\")\n",
    "            print(f\"  ✓ Converted to category\")\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            conversion_log.append(f\"{col_name}: Kept as object - {type_result['reason']}\")\n",
    "            print(f\"  → Kept as object ({type_result['reason']})\")\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        conversion_log.append(f\"{col_name}: Conversion to {recommended_type} FAILED - {str(e)}\")\n",
    "        print(f\"  ✗ Conversion to {recommended_type} failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def categorize_final_types(dtypes_dict):\n",
    "    \"\"\"\n",
    "    Categorize the final data types for summary reporting.\n",
    "    \"\"\"\n",
    "\n",
    "    type_categories = {\n",
    "        'datetime': [],\n",
    "        'numeric': [],\n",
    "        'categorical': [],\n",
    "        'text': [],\n",
    "        'administrative': []\n",
    "    }\n",
    "\n",
    "    for col, dtype in dtypes_dict.items():\n",
    "        dtype_str = str(dtype)\n",
    "\n",
    "        if col.startswith('flag_') or 'id' in col.lower():\n",
    "            type_categories['administrative'].append(col)\n",
    "        elif 'datetime' in dtype_str:\n",
    "            type_categories['datetime'].append(col)\n",
    "        elif dtype_str in ['int64', 'Int64', 'float64', 'Float64']:\n",
    "            type_categories['numeric'].append(col)\n",
    "        elif dtype_str == 'category':\n",
    "            type_categories['categorical'].append(col)\n",
    "        else:\n",
    "            type_categories['text'].append(col)\n",
    "\n",
    "    return type_categories\n",
    "\n",
    "def validate_type_assignments(df, conversion_log):\n",
    "    \"\"\"\n",
    "    Validate that type assignments were successful and reasonable.\n",
    "    \"\"\"\n",
    "\n",
    "    validation_results = {\n",
    "        'datetime_columns': [],\n",
    "        'numeric_columns': [],\n",
    "        'categorical_columns': [],\n",
    "        'potential_issues': []\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            validation_results['datetime_columns'].append({\n",
    "                'column': col,\n",
    "                'null_percentage': null_pct\n",
    "            })\n",
    "\n",
    "            if null_pct > 50:\n",
    "                validation_results['potential_issues'].append(f\"{col}: High null rate ({null_pct:.1f}%) after datetime conversion\")\n",
    "\n",
    "        elif pd.api.types.is_numeric_dtype(dtype):\n",
    "            null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            validation_results['numeric_columns'].append({\n",
    "                'column': col,\n",
    "                'null_percentage': null_pct,\n",
    "                'min_value': df[col].min(),\n",
    "                'max_value': df[col].max()\n",
    "            })\n",
    "\n",
    "            if null_pct > 30:\n",
    "                validation_results['potential_issues'].append(f\"{col}: High contamination ({null_pct:.1f}% null) after numeric conversion\")\n",
    "\n",
    "        elif isinstance(dtype, pd.CategoricalDtype):\n",
    "            n_categories = len(df[col].cat.categories)\n",
    "            validation_results['categorical_columns'].append({\n",
    "                'column': col,\n",
    "                'n_categories': n_categories\n",
    "            })\n",
    "\n",
    "            if n_categories > 20:\n",
    "                validation_results['potential_issues'].append(f\"{col}: Many categories ({n_categories}) - might not be truly categorical\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'cleaning_result' exists from Step 2b\n",
    "        typing_result = detect_and_assign_data_types(cleaning_result)\n",
    "\n",
    "        typed_data = typing_result['typed_data']\n",
    "        analysis = typing_result['type_analysis']\n",
    "        validation = typing_result['validation_results']\n",
    "\n",
    "        print(f\"\\n=== Step 3a Summary ===\")\n",
    "        print(f\"Data type detection completed:\")\n",
    "        print(f\"  Columns analyzed: {analysis['columns_analyzed']}\")\n",
    "        print(f\"  Conversions attempted: {analysis['conversions_attempted']}\")\n",
    "        print(f\"  Conversions successful: {analysis['conversions_successful']}\")\n",
    "        print(f\"  Problematic columns: {len(analysis['problematic_columns'])}\")\n",
    "\n",
    "        print(f\"\\nFinal type distribution:\")\n",
    "        for type_category, columns in analysis['columns_by_final_type'].items():\n",
    "            if columns:\n",
    "                print(f\"  {type_category.title()}: {len(columns)} columns\")\n",
    "\n",
    "        if validation['potential_issues']:\n",
    "            print(f\"\\nPotential issues detected:\")\n",
    "            for issue in validation['potential_issues'][:5]:\n",
    "                print(f\"  - {issue}\")\n",
    "            if len(validation['potential_issues']) > 5:\n",
    "                print(f\"  ... and {len(validation['potential_issues']) - 5} more issues\")\n",
    "\n",
    "        print(f\"\\nData shape: {typed_data.shape[0]:,} × {typed_data.shape[1]:,}\")\n",
    "        print(\"\\n✓ Step 3a Complete: Data types detected and assigned\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"❌ Please run Step 2b first to create the 'cleaning_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 3a: {str(e)}\")"
   ],
   "id": "e9fce0c3cf959b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Type Validation and Optimization",
   "id": "a4414a1bd4ee2b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3b: Data Type Validation and Optimization\n",
    "# Objective: Validate type assignments, handle edge cases, and optimize final data structure\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_and_optimize_data_types(typing_result):\n",
    "    \"\"\"\n",
    "    Validate type assignments, handle contaminated columns, and optimize the final data structure.\n",
    "    Creates cleaned versions of contaminated columns while preserving originals.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    typing_result : dict\n",
    "        Result from Step 3a containing typed_data and analysis results\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains optimized_data, validation_report, contamination_handling, and final_summary\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 3b: Data Type Validation and Optimization ===\")\n",
    "\n",
    "    df = typing_result['typed_data'].copy()\n",
    "    type_analysis = typing_result['type_analysis']\n",
    "    validation_results = typing_result['validation_results']\n",
    "\n",
    "    # Initialize optimization tracking\n",
    "    optimization_log = []\n",
    "    contamination_handling = {}\n",
    "\n",
    "    # Step 1: Handle highly contaminated numeric columns\n",
    "    contaminated_columns = handle_contaminated_numerics(df, validation_results, optimization_log, contamination_handling)\n",
    "\n",
    "    # Step 2: Optimize categorical columns\n",
    "    optimized_categoricals = optimize_categorical_columns(df, validation_results, optimization_log)\n",
    "\n",
    "    # Step 3: Create summary variables for complex multi-part questions\n",
    "    summary_variables = create_summary_variables(df, optimization_log)\n",
    "\n",
    "    # Step 4: Final data validation\n",
    "    final_validation = perform_final_validation(df, optimization_log)\n",
    "\n",
    "    # Generate comprehensive validation report\n",
    "    validation_report = generate_validation_report(df, type_analysis, validation_results,\n",
    "                                                 contamination_handling, optimization_log)\n",
    "\n",
    "    return {\n",
    "        'optimized_data': df,\n",
    "        'validation_report': validation_report,\n",
    "        'contamination_handling': contamination_handling,\n",
    "        'optimization_log': optimization_log,\n",
    "        'final_validation': final_validation\n",
    "    }\n",
    "\n",
    "def handle_contaminated_numerics(df, validation_results, optimization_log, contamination_handling):\n",
    "    \"\"\"\n",
    "    Handle numeric columns with high contamination rates by creating clean versions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with assigned types\n",
    "    validation_results : dict\n",
    "        Validation results from Step 3a\n",
    "    optimization_log : list\n",
    "        Log of optimization steps\n",
    "    contamination_handling : dict\n",
    "        Tracking contamination handling\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list : Names of contaminated columns handled\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Handling Contaminated Numeric Columns ---\")\n",
    "\n",
    "    contaminated_columns = []\n",
    "\n",
    "    for col_info in validation_results.get('numeric_columns', []):\n",
    "        col_name = col_info['column']\n",
    "        null_pct = col_info['null_percentage']\n",
    "\n",
    "        # If contamination is high (>20% null after conversion), create a cleaned version\n",
    "        if null_pct > 20:\n",
    "            contaminated_columns.append(col_name)\n",
    "\n",
    "            print(f\"Processing contaminated column: {col_name} ({null_pct:.1f}% null)\")\n",
    "\n",
    "            # Create cleaned version with suffix\n",
    "            clean_col_name = f\"{col_name}_clean\"\n",
    "\n",
    "            # Start with the numeric version (already converted)\n",
    "            df[clean_col_name] = df[col_name].copy()\n",
    "\n",
    "            # Apply additional cleaning based on column patterns\n",
    "            if 'year' in col_name.lower() or col_name.lower() in ['q6', 'establishment', 'founded']:\n",
    "                # Handle year columns specially\n",
    "                cleaned_years = clean_year_column(df[col_name], col_name)\n",
    "                df[clean_col_name] = cleaned_years\n",
    "\n",
    "                # Count improvements\n",
    "                original_valid = df[col_name].notna().sum()\n",
    "                cleaned_valid = df[clean_col_name].notna().sum()\n",
    "\n",
    "                contamination_handling[col_name] = {\n",
    "                    'original_valid': original_valid,\n",
    "                    'cleaned_valid': cleaned_valid,\n",
    "                    'improvement': cleaned_valid - original_valid,\n",
    "                    'clean_column_created': clean_col_name,\n",
    "                    'cleaning_method': 'year_range_validation'\n",
    "                }\n",
    "\n",
    "                print(f\"  Created {clean_col_name}: {cleaned_valid} valid values (+{cleaned_valid - original_valid} improvement)\")\n",
    "                optimization_log.append(f\"Created cleaned year column {clean_col_name} with range validation\")\n",
    "\n",
    "            else:\n",
    "                # Generic numeric cleaning\n",
    "                contamination_handling[col_name] = {\n",
    "                    'original_valid': df[col_name].notna().sum(),\n",
    "                    'cleaned_valid': df[col_name].notna().sum(),\n",
    "                    'improvement': 0,\n",
    "                    'clean_column_created': clean_col_name,\n",
    "                    'cleaning_method': 'preserved_as_is'\n",
    "                }\n",
    "\n",
    "                optimization_log.append(f\"Preserved contaminated numeric column {col_name} as {clean_col_name}\")\n",
    "\n",
    "    if not contaminated_columns:\n",
    "        print(\"No highly contaminated numeric columns found\")\n",
    "\n",
    "    return contaminated_columns\n",
    "\n",
    "def clean_year_column(series, col_name):\n",
    "    \"\"\"\n",
    "    Specialized cleaning for year columns with reasonable range validation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Year column data\n",
    "    col_name : str\n",
    "        Column name for context\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series : Cleaned year data\n",
    "    \"\"\"\n",
    "\n",
    "    # Define reasonable year range (adjust as needed)\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    min_reasonable_year = 1800  # Adjust based on context\n",
    "    max_reasonable_year = current_year + 5  # Allow slight future dates\n",
    "\n",
    "    cleaned_series = series.copy()\n",
    "\n",
    "    # Apply range validation\n",
    "    out_of_range_mask = (cleaned_series < min_reasonable_year) | (cleaned_series > max_reasonable_year)\n",
    "    out_of_range_count = out_of_range_mask.sum()\n",
    "\n",
    "    if out_of_range_count > 0:\n",
    "        print(f\"    Removing {out_of_range_count} out-of-range years (not between {min_reasonable_year}-{max_reasonable_year})\")\n",
    "        cleaned_series.loc[out_of_range_mask] = np.nan\n",
    "\n",
    "    return cleaned_series\n",
    "\n",
    "def optimize_categorical_columns(df, validation_results, optimization_log):\n",
    "    \"\"\"\n",
    "    Optimize categorical columns by handling issues like too many categories.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with assigned types\n",
    "    validation_results : dict\n",
    "        Validation results\n",
    "    optimization_log : list\n",
    "        Optimization log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Information about categorical optimizations\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Optimizing Categorical Columns ---\")\n",
    "\n",
    "    optimized_categoricals = {}\n",
    "\n",
    "    for col_info in validation_results.get('categorical_columns', []):\n",
    "        col_name = col_info['column']\n",
    "        n_categories = col_info['n_categories']\n",
    "\n",
    "        if n_categories > 20:\n",
    "            print(f\"Column {col_name} has {n_categories} categories - considering optimization\")\n",
    "\n",
    "            # For now, keep as-is but flag for review\n",
    "            optimized_categoricals[col_name] = {\n",
    "                'original_categories': n_categories,\n",
    "                'action': 'flagged_for_review',\n",
    "                'recommendation': 'Consider if this should be categorical or text'\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"{col_name}: Flagged - {n_categories} categories may be too many for categorical\")\n",
    "\n",
    "        elif n_categories == 1:\n",
    "            print(f\"Column {col_name} has only 1 category - converting to constant\")\n",
    "\n",
    "            # Convert single-category columns to a simple constant\n",
    "            unique_val = df[col_name].cat.categories[0]\n",
    "            df[col_name + '_constant'] = unique_val\n",
    "\n",
    "            optimized_categoricals[col_name] = {\n",
    "                'original_categories': 1,\n",
    "                'action': 'converted_to_constant',\n",
    "                'constant_value': unique_val\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"{col_name}: Converted to constant value '{unique_val}'\")\n",
    "\n",
    "    if not optimized_categoricals:\n",
    "        print(\"All categorical columns are appropriately sized\")\n",
    "\n",
    "    return optimized_categoricals\n",
    "\n",
    "def create_summary_variables(df, optimization_log):\n",
    "    \"\"\"\n",
    "    Create summary variables for complex multi-part questions (like Q2 series).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Optimized data\n",
    "    optimization_log : list\n",
    "        Optimization log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Information about created summary variables\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Creating Summary Variables ---\")\n",
    "\n",
    "    summary_variables = {}\n",
    "\n",
    "    # Identify question series (like Q2#1, Q2#2, etc.)\n",
    "    question_series = identify_question_series(df.columns)\n",
    "\n",
    "    for series_name, columns in question_series.items():\n",
    "        if len(columns) >= 3:  # Only create summaries for substantial series\n",
    "            print(f\"Creating summary for {series_name} series ({len(columns)} columns)\")\n",
    "\n",
    "            # Create response count summary\n",
    "            response_count_col = f\"{series_name}_response_count\"\n",
    "            df[response_count_col] = df[columns].notna().sum(axis=1)\n",
    "\n",
    "            # Create completion rate for this series\n",
    "            completion_rate_col = f\"{series_name}_completion_rate\"\n",
    "            df[completion_rate_col] = df[columns].notna().mean(axis=1) * 100\n",
    "\n",
    "            summary_variables[series_name] = {\n",
    "                'component_columns': columns,\n",
    "                'response_count_column': response_count_col,\n",
    "                'completion_rate_column': completion_rate_col,\n",
    "                'total_components': len(columns)\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"Created summary variables for {series_name}: {response_count_col}, {completion_rate_col}\")\n",
    "\n",
    "    if not summary_variables:\n",
    "        print(\"No substantial question series identified for summary creation\")\n",
    "\n",
    "    return summary_variables\n",
    "\n",
    "def identify_question_series(columns):\n",
    "    \"\"\"\n",
    "    Identify related question series based on naming patterns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    columns : list\n",
    "        List of column names\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Series name -> list of columns\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    series_patterns = {}\n",
    "\n",
    "    for col in columns:\n",
    "        # Look for patterns like q2_1_*, q2_2_*, etc.\n",
    "        match = re.match(r'^(q\\d+)(?:_\\d+)*', col)\n",
    "        if match:\n",
    "            base_pattern = match.group(1)\n",
    "            if base_pattern not in series_patterns:\n",
    "                series_patterns[base_pattern] = []\n",
    "            series_patterns[base_pattern].append(col)\n",
    "\n",
    "    # Only return series with multiple components\n",
    "    return {k: v for k, v in series_patterns.items() if len(v) > 1}\n",
    "\n",
    "def perform_final_validation(df, optimization_log):\n",
    "    \"\"\"\n",
    "    Perform final validation checks on the optimized data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Final optimized data\n",
    "    optimization_log : list\n",
    "        Optimization log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Final validation results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Performing Final Validation ---\")\n",
    "\n",
    "    final_validation = {\n",
    "        'total_columns': len(df.columns),\n",
    "        'total_rows': len(df),\n",
    "        'data_types': df.dtypes.value_counts().to_dict(),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "        'null_percentages': {},\n",
    "        'validation_warnings': []\n",
    "    }\n",
    "\n",
    "    # Calculate null percentages for key columns\n",
    "    for col in df.columns:\n",
    "        null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "        if null_pct > 50:  # Only report high null rates\n",
    "            final_validation['null_percentages'][col] = null_pct\n",
    "\n",
    "    # Check for validation warnings\n",
    "    if len(final_validation['null_percentages']) > 5:\n",
    "        final_validation['validation_warnings'].append(f\"Many columns ({len(final_validation['null_percentages'])}) have >50% missing data\")\n",
    "\n",
    "    if final_validation['memory_usage_mb'] > 100:\n",
    "        final_validation['validation_warnings'].append(f\"High memory usage: {final_validation['memory_usage_mb']:.1f} MB\")\n",
    "\n",
    "    print(f\"Final validation complete:\")\n",
    "    print(f\"  Data shape: {df.shape[0]:,} × {df.shape[1]:,}\")\n",
    "    print(f\"  Memory usage: {final_validation['memory_usage_mb']:.1f} MB\")\n",
    "\n",
    "    if final_validation['validation_warnings']:\n",
    "        print(f\"  Warnings: {len(final_validation['validation_warnings'])}\")\n",
    "\n",
    "    optimization_log.append(f\"Final validation: {df.shape[0]} rows × {df.shape[1]} columns, {final_validation['memory_usage_mb']:.1f} MB\")\n",
    "\n",
    "    return final_validation\n",
    "\n",
    "def generate_validation_report(df, type_analysis, validation_results, contamination_handling, optimization_log):\n",
    "    \"\"\"\n",
    "    Generate comprehensive validation report.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive validation report\n",
    "    \"\"\"\n",
    "\n",
    "    report = {\n",
    "        'data_overview': {\n",
    "            'shape': df.shape,\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        },\n",
    "        'type_distribution': type_analysis['columns_by_final_type'],\n",
    "        'contamination_summary': contamination_handling,\n",
    "        'validation_issues': validation_results.get('potential_issues', []),\n",
    "        'optimization_steps': len(optimization_log),\n",
    "        'ready_for_analysis': len(validation_results.get('potential_issues', [])) < 5\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'typing_result' exists from Step 3a\n",
    "        optimization_result = validate_and_optimize_data_types(typing_result)\n",
    "\n",
    "        optimized_data = optimization_result['optimized_data']\n",
    "        validation_report = optimization_result['validation_report']\n",
    "        contamination_handling = optimization_result['contamination_handling']\n",
    "\n",
    "        print(f\"\\n=== Step 3b Summary ===\")\n",
    "        print(f\"Data type validation and optimization completed:\")\n",
    "        print(f\"  Final data shape: {optimized_data.shape[0]:,} × {optimized_data.shape[1]:,}\")\n",
    "        print(f\"  Memory usage: {validation_report['data_overview']['memory_usage_mb']:.1f} MB\")\n",
    "        print(f\"  Contaminated columns handled: {len(contamination_handling)}\")\n",
    "        print(f\"  Optimization steps performed: {validation_report['optimization_steps']}\")\n",
    "\n",
    "        if contamination_handling:\n",
    "            print(f\"\\nContamination handling results:\")\n",
    "            for col, info in contamination_handling.items():\n",
    "                improvement = info.get('improvement', 0)\n",
    "                if improvement > 0:\n",
    "                    print(f\"  {col}: +{improvement} valid values recovered\")\n",
    "                else:\n",
    "                    print(f\"  {col}: Preserved with cleaning flags\")\n",
    "\n",
    "        print(f\"  Ready for analysis: {'Yes' if validation_report['ready_for_analysis'] else 'No'}\")\n",
    "\n",
    "        print(\"\\n✓ Step 3b Complete: Data validated and optimized for analysis\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"❌ Please run Step 3a first to create the 'typing_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 3b: {str(e)}\")"
   ],
   "id": "9a26615851dd32c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: Generate Analysis-Ready Outputs",
   "id": "1be88f04d233ea44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate Final Datasets and Documentation",
   "id": "fdc89c3b25215b22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4a: Generate Final Datasets and Documentation\n",
    "# Objective: Create analysis-ready exports with comprehensive documentation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def generate_final_datasets(optimization_result, structure_result):\n",
    "    \"\"\"\n",
    "    Generate final analysis-ready datasets with comprehensive documentation.\n",
    "    Creates multiple output formats optimized for different use cases.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    optimization_result : dict\n",
    "        Result from Step 3b containing optimized data and validation\n",
    "    structure_result : dict\n",
    "        Result from Step 2a containing original codebook and metadata\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains paths to created files and summary information\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 4a: Generating Final Datasets and Documentation ===\")\n",
    "\n",
    "    # --- ADDED: GUI for selecting output directory ---\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    print(\"Opening directory selector for saving outputs...\")\n",
    "    output_dir_str = filedialog.askdirectory(title=\"Select a Folder to Save Output Files\")\n",
    "\n",
    "    if not output_dir_str:\n",
    "        output_dir = Path('outputs')\n",
    "        print(f\"No directory selected. Saving files to a new folder: '{output_dir}'\")\n",
    "    else:\n",
    "        output_dir = Path(output_dir_str)\n",
    "        print(f\"Files will be saved to: '{output_dir}'\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # --- END ADDED BLOCK ---\n",
    "\n",
    "    optimized_data = optimization_result['optimized_data']\n",
    "    validation_report = optimization_result['validation_report']\n",
    "    contamination_handling = optimization_result['contamination_handling']\n",
    "\n",
    "    generated_files = {}\n",
    "    export_summary = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_responses': len(optimized_data),\n",
    "        'total_variables': len(optimized_data.columns),\n",
    "        'files_created': []\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCreating final datasets from {len(optimized_data):,} responses × {len(optimized_data.columns):,} variables\")\n",
    "\n",
    "    # --- MODIFIED: Pass output_dir to each function ---\n",
    "    analysis_dataset = create_analysis_dataset(optimized_data, generated_files, export_summary, output_dir)\n",
    "    comprehensive_codebook = create_comprehensive_codebook(optimized_data, structure_result, optimization_result, generated_files, export_summary, output_dir)\n",
    "    quality_report = create_data_quality_report(optimization_result, generated_files, export_summary, output_dir)\n",
    "    variable_summaries = create_variable_summaries(optimized_data, generated_files, export_summary, output_dir)\n",
    "    metadata_file = create_metadata_file(optimization_result, structure_result, export_summary, generated_files, output_dir)\n",
    "\n",
    "    return {\n",
    "        'generated_files': generated_files,\n",
    "        'export_summary': export_summary,\n",
    "        'analysis_dataset': analysis_dataset,\n",
    "        'codebook': comprehensive_codebook,\n",
    "        'quality_report': quality_report\n",
    "    }\n",
    "\n",
    "def create_analysis_dataset(df, generated_files, export_summary, output_dir):\n",
    "    print(\"\\n--- Creating Main Analysis Dataset ---\")\n",
    "    analysis_data = df.copy()\n",
    "\n",
    "    metadata_cols = [col for col in df.columns if any(meta in col.lower() for meta in ['date', 'status', 'progress', 'duration', 'finished', 'recipient', 'location', 'distribution', 'language', 'ipaddress', 'responseid'])]\n",
    "    flag_cols = [col for col in df.columns if col.startswith('flag_')]\n",
    "    original_question_cols = [col for col in df.columns if col.startswith('q') and not col.endswith(('_clean', '_constant')) and not any(suffix in col for suffix in ['_response_count', '_completion_rate'])]\n",
    "    derived_cols = [col for col in df.columns if any(suffix in col for suffix in ['_clean', '_constant', '_response_count', '_completion_rate'])]\n",
    "    other_cols = [col for col in df.columns if col not in metadata_cols + flag_cols + original_question_cols + derived_cols]\n",
    "    ordered_columns = metadata_cols + flag_cols + original_question_cols + derived_cols + other_cols\n",
    "    analysis_data = analysis_data[ordered_columns]\n",
    "\n",
    "    filepath = output_dir / 'qualtrics_analysis_ready.csv'\n",
    "    analysis_data.to_csv(filepath, index=False)\n",
    "    generated_files['analysis_dataset'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "\n",
    "    analysis_info = {\n",
    "        'filename': str(filepath),\n",
    "        'shape': analysis_data.shape,\n",
    "        'column_organization': {\n",
    "            'metadata_columns': len(metadata_cols),\n",
    "            'quality_flags': len(flag_cols),\n",
    "            'original_questions': len(original_question_cols),\n",
    "            'derived_variables': len(derived_cols),\n",
    "            'other_columns': len(other_cols)\n",
    "        }\n",
    "    }\n",
    "    print(f\"Created {filepath.name}:\")\n",
    "    print(f\"  Shape: {analysis_data.shape[0]:,} × {analysis_data.shape[1]:,}\")\n",
    "    return analysis_info\n",
    "\n",
    "def create_comprehensive_codebook(df, structure_result, optimization_result, generated_files, export_summary, output_dir):\n",
    "    print(\"\\n--- Creating Comprehensive Codebook ---\")\n",
    "    original_codebook = structure_result.get('codebook', pd.DataFrame())\n",
    "    codebook_data = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        non_null_count = df[col].notna().sum()\n",
    "        null_count = df[col].isnull().sum()\n",
    "        response_rate = (non_null_count / len(df)) * 100\n",
    "        question_text = col\n",
    "        if len(original_codebook) > 0 and col in original_codebook.columns:\n",
    "            try:\n",
    "                question_text = original_codebook[col].iloc[0] if len(original_codebook) > 0 else col\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        codebook_entry = {\n",
    "            'variable_name': col,\n",
    "            'variable_label': str(question_text)[:100] + '...' if len(str(question_text)) > 100 else str(question_text),\n",
    "            'variable_type': dtype,\n",
    "            'variable_category': categorize_variable(col, dtype),\n",
    "            'total_responses': len(df),\n",
    "            'valid_responses': non_null_count,\n",
    "            'missing_responses': null_count,\n",
    "            'response_rate_percent': round(response_rate, 1),\n",
    "            'value_information': get_value_information(df[col], dtype)\n",
    "        }\n",
    "        codebook_data.append(codebook_entry)\n",
    "\n",
    "    codebook_df = pd.DataFrame(codebook_data)\n",
    "\n",
    "    filepath = output_dir / 'qualtrics_codebook_comprehensive.csv'\n",
    "    codebook_df.to_csv(filepath, index=False)\n",
    "    generated_files['codebook'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "\n",
    "    print(f\"Created {filepath.name} with {len(codebook_df)} variable definitions\")\n",
    "    return {'filename': str(filepath), 'variables_documented': len(codebook_df)}\n",
    "\n",
    "def create_data_quality_report(optimization_result, generated_files, export_summary, output_dir):\n",
    "    print(\"\\n--- Creating Data Quality Report ---\")\n",
    "    validation_report = optimization_result['validation_report']\n",
    "    quality_report = {\n",
    "        'report_metadata': {\n",
    "            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'data_shape': validation_report['data_overview']['shape'],\n",
    "            'memory_usage_mb': validation_report['data_overview']['memory_usage_mb']\n",
    "        },\n",
    "        'data_type_summary': validation_report['type_distribution'],\n",
    "        'contamination_summary': optimization_result['contamination_handling'],\n",
    "        'validation_issues': validation_report.get('validation_issues', []),\n",
    "        'optimization_log': optimization_result['optimization_log'],\n",
    "        'ready_for_analysis': validation_report['ready_for_analysis']\n",
    "    }\n",
    "\n",
    "    filepath = output_dir / 'qualtrics_data_quality_report.json'\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2, default=str)\n",
    "\n",
    "    generated_files['quality_report'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "\n",
    "    print(f\"Created {filepath.name}\")\n",
    "    return {'filename': str(filepath)}\n",
    "\n",
    "def create_variable_summaries(df, generated_files, export_summary, output_dir):\n",
    "    print(\"\\n--- Creating Variable Summary Statistics ---\")\n",
    "    summaries = {}\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_summary = df[numeric_cols].describe()\n",
    "        numeric_summary.loc['response_rate'] = ((df[numeric_cols].notna().sum() / len(df)) * 100).round(1)\n",
    "        summaries['numeric_variables'] = numeric_summary\n",
    "\n",
    "    categorical_cols = [col for col in df.columns if isinstance(df[col].dtype, pd.CategoricalDtype)]\n",
    "    if len(categorical_cols) > 0:\n",
    "        categorical_summary = []\n",
    "        for col in categorical_cols:\n",
    "            value_counts = df[col].value_counts()\n",
    "            summary = {\n",
    "                'variable': col, 'total_responses': len(df), 'valid_responses': df[col].notna().sum(),\n",
    "                'categories': len(df[col].cat.categories),\n",
    "                'top_category': value_counts.index[0] if len(value_counts) > 0 else 'None',\n",
    "                'top_category_count': value_counts.iloc[0] if len(value_counts) > 0 else 0\n",
    "            }\n",
    "            categorical_summary.append(summary)\n",
    "        summaries['categorical_variables'] = pd.DataFrame(categorical_summary)\n",
    "\n",
    "    if summaries:\n",
    "        filepath = output_dir / 'qualtrics_variable_summaries.xlsx'\n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            for sheet_name, summary_df in summaries.items():\n",
    "                summary_df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "        generated_files['variable_summaries'] = str(filepath)\n",
    "        export_summary['files_created'].append(str(filepath))\n",
    "        print(f\"Created {filepath.name} with {len(summaries)} summary sheets\")\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def create_metadata_file(optimization_result, structure_result, export_summary, generated_files, output_dir):\n",
    "    print(\"\\n--- Creating Processing Metadata ---\")\n",
    "    metadata = {\n",
    "        'processing_pipeline': { 'step_1': 'Data loading', 'step_2a': 'Structure separation', 'step_2b': 'Cleaning', 'step_3a': 'Type detection', 'step_3b': 'Validation', 'step_4a': 'Output generation' },\n",
    "        'data_transformations': {\n",
    "            'test_responses_removed': structure_result.get('structure_analysis', {}).get('test_responses', 0),\n",
    "            'quality_flags_created': len([col for col in optimization_result['optimized_data'].columns if col.startswith('flag_')]),\n",
    "            'contaminated_columns_handled': len(optimization_result.get('contamination_handling', {})),\n",
    "            'summary_variables_created': len([col for col in optimization_result['optimized_data'].columns if any(suffix in col for suffix in ['_count', '_rate', '_clean', '_constant'])])\n",
    "        },\n",
    "        'final_dataset_characteristics': { 'total_responses': len(optimization_result['optimized_data']), 'total_variables': len(optimization_result['optimized_data'].columns), 'ready_for_analysis': optimization_result['validation_report']['ready_for_analysis'] },\n",
    "        'files_generated': generated_files,\n",
    "        'processing_timestamp': export_summary['timestamp']\n",
    "    }\n",
    "\n",
    "    filepath = output_dir / 'qualtrics_processing_metadata.json'\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "    generated_files['processing_metadata'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "    print(f\"Created {filepath.name} with complete processing documentation\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def categorize_variable(col_name, dtype):\n",
    "    if col_name.startswith('flag_'): return 'Quality Flag'\n",
    "    if any(meta in col_name.lower() for meta in ['date', 'time']): return 'Metadata - Temporal'\n",
    "    if any(meta in col_name.lower() for meta in ['status', 'progress', 'duration', 'finished']): return 'Metadata - Administrative'\n",
    "    if any(meta in col_name.lower() for meta in ['recipient', 'location', 'ip', 'distribution', 'language']): return 'Metadata - Technical'\n",
    "    if col_name.startswith('q') and not any(suffix in col_name for suffix in ['_clean', '_constant', '_count', '_rate']): return 'Survey Question'\n",
    "    if col_name.endswith('_clean'): return 'Derived - Cleaned Variable'\n",
    "    if col_name.endswith('_constant'): return 'Derived - Constant Value'\n",
    "    if any(suffix in col_name for suffix in ['_count', '_rate']): return 'Derived - Summary Metric'\n",
    "    return 'Other'\n",
    "\n",
    "def get_value_information(series, dtype):\n",
    "    if series.notna().sum() == 0: return \"All missing values\"\n",
    "    try:\n",
    "        if 'datetime' in dtype:\n",
    "            min_date, max_date = series.min(), series.max()\n",
    "            return f\"Date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\"\n",
    "        elif 'int' in dtype.lower() or 'float' in dtype.lower():\n",
    "            return f\"Range: {series.min()} to {series.max()}, Median: {series.median()}\"\n",
    "        elif 'category' in dtype:\n",
    "            counts = series.value_counts().head(5).to_dict()\n",
    "            return f\"Top 5 categories: {counts}\" if len(series.cat.categories) > 5 else f\"Categories: {counts}\"\n",
    "        else:\n",
    "            return f\"{series.nunique()} unique values, Sample: {list(series.dropna().unique()[:3])}\"\n",
    "    except Exception as e:\n",
    "        return f\"Value analysis error: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'optimization_result' and 'structure_result' exist from previous steps\n",
    "        # You would run Step 1, 2a, 2b, 3a, and 3b before this step to generate these variables.\n",
    "        final_result = generate_final_datasets(optimization_result, structure_result)\n",
    "\n",
    "        generated_files = final_result['generated_files']\n",
    "        export_summary = final_result['export_summary']\n",
    "\n",
    "        print(f\"\\n=== Step 4a Summary ===\")\n",
    "        print(f\"Final dataset generation completed:\")\n",
    "        print(f\"  Processing timestamp: {export_summary['timestamp']}\")\n",
    "        print(f\"  Total responses processed: {export_summary['total_responses']:,}\")\n",
    "        print(f\"  Total variables created: {export_summary['total_variables']:,}\")\n",
    "        print(f\"  Files generated: {len(export_summary['files_created'])}\\n\")\n",
    "\n",
    "        print(\"Files created in your selected directory:\")\n",
    "        for file_type, filepath in generated_files.items():\n",
    "            print(f\"  - {Path(filepath).name}\")\n",
    "\n",
    "        print(\"\\n✓ Step 4a Complete: All analysis-ready files generated\")\n",
    "        print(\"The Qualtrics processing pipeline is now complete!\")\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"❌ Please run the previous steps (1 through 3b) first to create the required input variables for this step.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Step 4a: {str(e)}\")"
   ],
   "id": "b2c87d30c7136ce6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
