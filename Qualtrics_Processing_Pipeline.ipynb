{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Qualtrics Processing Pipeline",
   "id": "45e267f26374d754"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Load Qualtrics Excel File",
   "id": "c9ccc1b6a7459ed2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Universal Qualtrics Excel File Loader",
   "id": "33399e8a4f8bb59b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T13:40:40.818897Z",
     "start_time": "2025-08-29T13:40:38.338519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Universal Qualtrics Excel File Loader with Question Name Mapping\n",
    "# This code is designed to work with any Qualtrics export regardless of survey content\n",
    "# ENHANCED: Now captures and preserves the mapping between column IDs and question text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import re\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "def load_qualtrics_export(file_path=None, sheet_name=0):\n",
    "    \"\"\"\n",
    "    Load a Qualtrics Excel export file with a GUI selector and robust error handling.\n",
    "    ENHANCED: Now captures question name mapping from row 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str or Path, optional\n",
    "        Path to the Excel file. If None, a GUI file selector will open.\n",
    "    sheet_name : str or int, default 0\n",
    "        Sheet name or index to load from the Excel file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains 'raw_data' (DataFrame), 'file_info' (dict), 'quality_check' (dict),\n",
    "           and 'question_mapping' (dict) mapping column names to question text\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 1: Loading Qualtrics Export ===\")\n",
    "\n",
    "    # If no file_path is provided, open a GUI file selector\n",
    "    if file_path is None:\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the main tkinter window\n",
    "\n",
    "        print(\"Opening file selector...\")\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select the Qualtrics Excel Export\",\n",
    "            filetypes=[(\"Excel Files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "\n",
    "        if not file_path:  # Handle case where user closes the dialog\n",
    "            raise FileNotFoundError(\"No file selected. Please run the script again.\")\n",
    "\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    print(f\"Loading: {file_path.name}\")\n",
    "\n",
    "    # Load the Excel file with error handling\n",
    "    try:\n",
    "        # First, get sheet information\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        print(f\"Available sheets: {sheet_names}\")\n",
    "\n",
    "        # Determine which sheet to load\n",
    "        if isinstance(sheet_name, str) and sheet_name not in sheet_names:\n",
    "            print(f\"Warning: Sheet '{sheet_name}' not found. Using first sheet: '{sheet_names[0]}'\")\n",
    "            sheet_name = 0\n",
    "        elif isinstance(sheet_name, int) and sheet_name >= len(sheet_names):\n",
    "            print(f\"Warning: Sheet index {sheet_name} out of range. Using first sheet: '{sheet_names[0]}'\")\n",
    "            sheet_name = 0\n",
    "\n",
    "        actual_sheet = sheet_names[sheet_name] if isinstance(sheet_name, int) else sheet_name\n",
    "        print(f\"Loading sheet: '{actual_sheet}'\")\n",
    "\n",
    "        # Load the data\n",
    "        raw_df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load Excel file: {str(e)}\")\n",
    "\n",
    "    # File information\n",
    "    file_info = {\n",
    "        'filename': file_path.name,\n",
    "        'file_size_mb': file_path.stat().st_size / (1024 * 1024),\n",
    "        'sheet_loaded': actual_sheet,\n",
    "        'available_sheets': sheet_names,\n",
    "        'raw_shape': raw_df.shape\n",
    "    }\n",
    "\n",
    "    print(f\"File loaded successfully:\")\n",
    "    print(f\"  Size: {file_info['file_size_mb']:.1f} MB\")\n",
    "    print(f\"  Dimensions: {raw_df.shape[0]:,} rows x {raw_df.shape[1]:,} columns\")\n",
    "\n",
    "    # Quality checks to identify Qualtrics structure\n",
    "    quality_check = analyze_qualtrics_structure(raw_df)\n",
    "\n",
    "    # ENHANCED: Extract question mapping\n",
    "    question_mapping = extract_question_mapping(raw_df, quality_check)\n",
    "\n",
    "    return {\n",
    "        'raw_data': raw_df,\n",
    "        'file_info': file_info,\n",
    "        'quality_check': quality_check,\n",
    "        'question_mapping': question_mapping  # NEW: Added question mapping\n",
    "    }\n",
    "\n",
    "def extract_question_mapping(df, quality_check):\n",
    "    \"\"\"\n",
    "    Extract the mapping between column IDs and question text from row 0.\n",
    "    NEW FUNCTION: Creates a comprehensive mapping for later use.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw Qualtrics data\n",
    "    quality_check : dict\n",
    "        Quality check results from structure analysis\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive question mapping with structure:\n",
    "           {\n",
    "               'column_to_question': {column_id: question_text},\n",
    "               'has_question_text': bool,\n",
    "               'question_text_source': str,\n",
    "               'mapping_quality': str\n",
    "           }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Extracting Question Mapping ===\")\n",
    "\n",
    "    mapping = {\n",
    "        'column_to_question': {},\n",
    "        'has_question_text': False,\n",
    "        'question_text_source': 'none',\n",
    "        'mapping_quality': 'not_available',\n",
    "        'question_categories': {}\n",
    "    }\n",
    "\n",
    "    # Check if we have question text in row 0\n",
    "    if quality_check.get('is_qualtrics_format') and len(df) > 0:\n",
    "        row_0 = df.iloc[0]\n",
    "\n",
    "        # Create the mapping for each column\n",
    "        for col in df.columns:\n",
    "            if col in row_0.index:\n",
    "                question_text = row_0[col]\n",
    "\n",
    "                # Handle various data types and null values\n",
    "                if pd.notna(question_text):\n",
    "                    question_text_str = str(question_text).strip()\n",
    "\n",
    "                    # Only map if we have meaningful text\n",
    "                    if question_text_str and len(question_text_str) > 0:\n",
    "                        mapping['column_to_question'][col] = question_text_str\n",
    "\n",
    "                        # Categorize the question\n",
    "                        if col.startswith('Q') or col.startswith('q'):\n",
    "                            category = 'survey_question'\n",
    "                        elif col in quality_check.get('standard_qualtrics_columns', []):\n",
    "                            category = 'metadata'\n",
    "                        else:\n",
    "                            category = 'other'\n",
    "\n",
    "                        if category not in mapping['question_categories']:\n",
    "                            mapping['question_categories'][category] = []\n",
    "                        mapping['question_categories'][category].append(col)\n",
    "                else:\n",
    "                    # No question text for this column - use column name as fallback\n",
    "                    mapping['column_to_question'][col] = col\n",
    "            else:\n",
    "                # Column not in row 0 - use column name\n",
    "                mapping['column_to_question'][col] = col\n",
    "\n",
    "        # Assess the quality of the mapping\n",
    "        total_columns = len(df.columns)\n",
    "        mapped_with_text = sum(1 for k, v in mapping['column_to_question'].items()\n",
    "                              if k != v and len(v) > len(k))\n",
    "\n",
    "        if mapped_with_text > total_columns * 0.7:\n",
    "            mapping['has_question_text'] = True\n",
    "            mapping['question_text_source'] = 'row_0'\n",
    "            mapping['mapping_quality'] = 'high'\n",
    "            print(f\"[SUCCESS] High quality question mapping extracted from row 0\")\n",
    "            print(f\"  {mapped_with_text}/{total_columns} columns have descriptive question text\")\n",
    "        elif mapped_with_text > total_columns * 0.3:\n",
    "            mapping['has_question_text'] = True\n",
    "            mapping['question_text_source'] = 'row_0_partial'\n",
    "            mapping['mapping_quality'] = 'medium'\n",
    "            print(f\"[SUCCESS] Partial question mapping extracted from row 0\")\n",
    "            print(f\"  {mapped_with_text}/{total_columns} columns have descriptive question text\")\n",
    "        else:\n",
    "            mapping['has_question_text'] = False\n",
    "            mapping['question_text_source'] = 'column_names_only'\n",
    "            mapping['mapping_quality'] = 'low'\n",
    "            print(f\"[WARNING] Limited question text available - using column names\")\n",
    "            print(f\"  Only {mapped_with_text}/{total_columns} columns have descriptive text\")\n",
    "    else:\n",
    "        # No row 0 or not Qualtrics format - use column names\n",
    "        for col in df.columns:\n",
    "            mapping['column_to_question'][col] = col\n",
    "        mapping['question_text_source'] = 'column_names_only'\n",
    "        print(\"[WARNING] No question text row found - using column names as labels\")\n",
    "\n",
    "    # Print mapping summary\n",
    "    print(f\"\\nQuestion Mapping Summary:\")\n",
    "    for category, columns in mapping['question_categories'].items():\n",
    "        print(f\"  {category.replace('_', ' ').title()}: {len(columns)} columns\")\n",
    "\n",
    "    # Show sample mappings\n",
    "    print(f\"\\nSample Question Mappings (first 3):\")\n",
    "    sample_count = 0\n",
    "    for col, question in list(mapping['column_to_question'].items())[:10]:\n",
    "        if col != question and col.startswith(('Q', 'q')):\n",
    "            question_preview = question[:80] + \"...\" if len(question) > 80 else question\n",
    "            print(f\"  {col} -> {question_preview}\")\n",
    "            sample_count += 1\n",
    "            if sample_count >= 3:\n",
    "                break\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def analyze_qualtrics_structure(df):\n",
    "    \"\"\"\n",
    "    Analyze the loaded DataFrame to identify Qualtrics-specific patterns.\n",
    "    (Unchanged from original)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw loaded data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality check results and structural analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Analyzing Qualtrics Structure ===\")\n",
    "\n",
    "    # Initialize quality check results\n",
    "    quality_check = {\n",
    "        'is_qualtrics_format': False,\n",
    "        'header_row_index': None,\n",
    "        'data_start_row': None,\n",
    "        'standard_qualtrics_columns': [],\n",
    "        'question_columns': [],\n",
    "        'total_columns': len(df.columns),\n",
    "        'potential_issues': []\n",
    "    }\n",
    "\n",
    "    # Standard Qualtrics metadata columns (regardless of survey content)\n",
    "    standard_columns = [\n",
    "        'StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "        'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "        'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "        'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "        'DistributionChannel', 'UserLanguage'\n",
    "    ]\n",
    "\n",
    "    # Check if this looks like a Qualtrics export\n",
    "    columns_list = df.columns.tolist()\n",
    "    standard_found = [col for col in standard_columns if col in columns_list]\n",
    "    quality_check['standard_qualtrics_columns'] = standard_found\n",
    "\n",
    "    # Qualtrics exports typically have these key identifiers\n",
    "    qualtrics_indicators = ['ResponseId', 'StartDate', 'EndDate', 'Status']\n",
    "    indicators_found = sum(1 for indicator in qualtrics_indicators if indicator in columns_list)\n",
    "\n",
    "    if indicators_found >= 3:\n",
    "        quality_check['is_qualtrics_format'] = True\n",
    "        print(\"[SUCCESS] Confirmed Qualtrics export format\")\n",
    "    else:\n",
    "        quality_check['potential_issues'].append(\"Does not appear to be standard Qualtrics export format\")\n",
    "        print(\"[WARNING] File may not be a standard Qualtrics export\")\n",
    "\n",
    "    # Identify question columns (typically start with Q followed by number)\n",
    "    question_pattern = re.compile(r'^Q\\d+', re.IGNORECASE)\n",
    "    question_columns = [col for col in columns_list if question_pattern.match(str(col))]\n",
    "    quality_check['question_columns'] = question_columns\n",
    "\n",
    "    print(f\"Standard Qualtrics columns found: {len(standard_found)}\")\n",
    "    print(f\"Question columns identified: {len(question_columns)}\")\n",
    "\n",
    "    # Analyze row structure for header/data separation\n",
    "    if quality_check['is_qualtrics_format']:\n",
    "        header_analysis = analyze_header_structure(df)\n",
    "        quality_check.update(header_analysis)\n",
    "\n",
    "    # Check for common issues\n",
    "    if df.shape[0] < 2:\n",
    "        quality_check['potential_issues'].append(\"Very few rows - may not contain response data\")\n",
    "\n",
    "    if df.isnull().all().sum() > len(df.columns) * 0.5:\n",
    "        quality_check['potential_issues'].append(\"Many completely empty columns detected\")\n",
    "\n",
    "    # Report findings\n",
    "    if quality_check['potential_issues']:\n",
    "        print(\"\\n[WARNING] Potential Issues Detected:\")\n",
    "        for issue in quality_check['potential_issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"[SUCCESS] No structural issues detected\")\n",
    "\n",
    "    return quality_check\n",
    "\n",
    "def analyze_header_structure(df):\n",
    "    \"\"\"\n",
    "    Analyze the DataFrame to identify where the header row and data rows are.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    header_info = {\n",
    "        'header_row_index': 0,  # Qualtrics question text is typically in row 0\n",
    "        'data_start_row': 1,    # Response data typically starts at row 1\n",
    "        'response_type_column': None,\n",
    "        'preview_responses_detected': False\n",
    "    }\n",
    "\n",
    "    # Check if there's a Status column to identify preview responses\n",
    "    if 'Status' in df.columns:\n",
    "        header_info['response_type_column'] = 'Status'\n",
    "\n",
    "        # Look for preview responses in first few rows\n",
    "        status_values = df['Status'].head(10).dropna().unique()\n",
    "        if any('preview' in str(val).lower() for val in status_values):\n",
    "            header_info['preview_responses_detected'] = True\n",
    "            print(\"[SUCCESS] Preview responses detected in Status column\")\n",
    "\n",
    "    # Verify our assumptions by checking if row 0 contains question text\n",
    "    if len(df) > 0:\n",
    "        row_0_sample = df.iloc[0].dropna().head(3).tolist()\n",
    "        avg_text_length = np.mean([len(str(val)) for val in row_0_sample]) if row_0_sample else 0\n",
    "\n",
    "        if avg_text_length > 50:  # Long text suggests question descriptions\n",
    "            print(\"[SUCCESS] Row 0 appears to contain question text (header row)\")\n",
    "        else:\n",
    "            print(\"[WARNING] Row 0 may not contain typical Qualtrics question text\")\n",
    "            header_info.setdefault('potential_issues', []).append(\"Row 0 structure atypical for Qualtrics\")\n",
    "\n",
    "    return header_info\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the Qualtrics file\n",
    "        result = load_qualtrics_export()\n",
    "\n",
    "        raw_data = result['raw_data']\n",
    "        file_info = result['file_info']\n",
    "        quality_check = result['quality_check']\n",
    "        question_mapping = result['question_mapping']  # NEW: Access question mapping\n",
    "\n",
    "        print(f\"\\n=== Loading Summary ===\")\n",
    "        print(f\"File: {file_info['filename']}\")\n",
    "        print(f\"Qualtrics format: {quality_check['is_qualtrics_format']}\")\n",
    "        print(f\"Total responses: {raw_data.shape[0]:,}\")\n",
    "        print(f\"Total columns: {raw_data.shape[1]:,}\")\n",
    "        print(f\"Question columns: {len(quality_check['question_columns'])}\")\n",
    "        print(f\"Standard metadata columns: {len(quality_check['standard_qualtrics_columns'])}\")\n",
    "\n",
    "        # NEW: Display question mapping quality\n",
    "        print(f\"\\nQuestion Mapping:\")\n",
    "        print(f\"  Mapping quality: {question_mapping['mapping_quality']}\")\n",
    "        print(f\"  Question text available: {question_mapping['has_question_text']}\")\n",
    "        print(f\"  Source: {question_mapping['question_text_source']}\")\n",
    "\n",
    "        # Show a sample of the data structure\n",
    "        print(f\"\\n=== Data Structure Preview ===\")\n",
    "        print(\"First few column names:\")\n",
    "        for i, col in enumerate(raw_data.columns[:8]):\n",
    "            print(f\"  {i}: {col}\")\n",
    "        if len(raw_data.columns) > 8:\n",
    "            print(f\"  ... and {len(raw_data.columns) - 8} more columns\")\n",
    "\n",
    "        print(f\"\\nFirst row sample (likely question text):\")\n",
    "        sample_row = raw_data.iloc[0].head(5)\n",
    "        for col, val in sample_row.items():\n",
    "            val_preview = str(val)[:60] + \"...\" if len(str(val)) > 60 else str(val)\n",
    "            print(f\"  {col}: {val_preview}\")\n",
    "\n",
    "        print(\"\\n[SUCCESS] Step 1 Complete: File loaded and analyzed successfully with question mapping captured\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error in Step 1: {str(e)}\")\n",
    "        print(\"Please check your file path and ensure it's a valid Qualtrics Excel export.\")"
   ],
   "id": "19357fb645c8d502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Loading Qualtrics Export ===\n",
      "Opening file selector...\n",
      "Loading: Facilities & Equipment Research Emergency Management_August 21, 2025_14.11.xlsx\n",
      "Available sheets: ['Sheet0']\n",
      "Loading sheet: 'Sheet0'\n",
      "File loaded successfully:\n",
      "  Size: 0.1 MB\n",
      "  Dimensions: 193 rows x 57 columns\n",
      "\n",
      "=== Analyzing Qualtrics Structure ===\n",
      "[SUCCESS] Confirmed Qualtrics export format\n",
      "Standard Qualtrics columns found: 17\n",
      "Question columns identified: 40\n",
      "[SUCCESS] Preview responses detected in Status column\n",
      "[WARNING] Row 0 may not contain typical Qualtrics question text\n",
      "\n",
      "[WARNING] Potential Issues Detected:\n",
      "  - Row 0 structure atypical for Qualtrics\n",
      "\n",
      "=== Extracting Question Mapping ===\n",
      "[SUCCESS] High quality question mapping extracted from row 0\n",
      "  54/57 columns have descriptive question text\n",
      "\n",
      "Question Mapping Summary:\n",
      "  Metadata: 17 columns\n",
      "  Survey Question: 40 columns\n",
      "\n",
      "Sample Question Mappings (first 3):\n",
      "\n",
      "=== Loading Summary ===\n",
      "File: Facilities & Equipment Research Emergency Management_August 21, 2025_14.11.xlsx\n",
      "Qualtrics format: True\n",
      "Total responses: 193\n",
      "Total columns: 57\n",
      "Question columns: 40\n",
      "Standard metadata columns: 17\n",
      "\n",
      "Question Mapping:\n",
      "  Mapping quality: high\n",
      "  Question text available: True\n",
      "  Source: row_0\n",
      "\n",
      "=== Data Structure Preview ===\n",
      "First few column names:\n",
      "  0: StartDate\n",
      "  1: EndDate\n",
      "  2: Status\n",
      "  3: IPAddress\n",
      "  4: Progress\n",
      "  5: Duration (in seconds)\n",
      "  6: Finished\n",
      "  7: RecordedDate\n",
      "  ... and 49 more columns\n",
      "\n",
      "First row sample (likely question text):\n",
      "  StartDate: Start Date\n",
      "  EndDate: End Date\n",
      "  Status: Response Type\n",
      "  IPAddress: IP Address\n",
      "  Progress: Progress\n",
      "\n",
      "[SUCCESS] Step 1 Complete: File loaded and analyzed successfully with question mapping captured\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2:",
   "id": "f75e7db03a943b89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract and Analyze Data Structure with Question Mapping",
   "id": "4e781c0f8861a600"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T13:43:47.616669Z",
     "start_time": "2025-08-29T13:43:47.595230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2a: Extract and Analyze Data Structure with Question Mapping\n",
    "# ENHANCED: Now preserves and utilizes question mapping from Step 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_data_structure(result_from_step1):\n",
    "    \"\"\"\n",
    "    Analyze and extract the data structure from a Qualtrics export.\n",
    "    ENHANCED: Now preserves question mapping throughout the process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result_from_step1 : dict\n",
    "        Result dictionary from Step 1 containing raw_data, file_info, quality_check, and question_mapping\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains separated codebook, response_data, structure_analysis, and preserved question_mapping\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 2a: Extracting Data Structure ===\")\n",
    "\n",
    "    raw_df = result_from_step1['raw_data']\n",
    "    quality_check = result_from_step1['quality_check']\n",
    "    question_mapping = result_from_step1.get('question_mapping', {})  # NEW: Get question mapping\n",
    "\n",
    "    # Initialize structure analysis\n",
    "    structure_analysis = {\n",
    "        'codebook_source': None,\n",
    "        'response_data_start_row': None,\n",
    "        'header_type': None,\n",
    "        'total_rows_analyzed': len(raw_df),\n",
    "        'metadata_columns': [],\n",
    "        'question_columns': [],\n",
    "        'response_types_found': [],\n",
    "        'has_question_mapping': bool(question_mapping.get('column_to_question'))  # NEW\n",
    "    }\n",
    "\n",
    "    # Identify metadata vs question columns\n",
    "    metadata_patterns = [\n",
    "        'StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "        'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "        'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "        'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "        'DistributionChannel', 'UserLanguage'\n",
    "    ]\n",
    "\n",
    "    all_columns = raw_df.columns.tolist()\n",
    "    metadata_cols = [col for col in all_columns if col in metadata_patterns]\n",
    "    question_cols = [col for col in all_columns if col not in metadata_patterns]\n",
    "\n",
    "    structure_analysis['metadata_columns'] = metadata_cols\n",
    "    structure_analysis['question_columns'] = question_cols\n",
    "\n",
    "    print(f\"Identified {len(metadata_cols)} metadata columns\")\n",
    "    print(f\"Identified {len(question_cols)} question/data columns\")\n",
    "\n",
    "    # Determine header structure and codebook location\n",
    "    codebook_info = analyze_codebook_structure(raw_df, structure_analysis)\n",
    "    structure_analysis.update(codebook_info)\n",
    "\n",
    "    # ENHANCED: Create comprehensive codebook with question mapping\n",
    "    if structure_analysis['codebook_source'] == 'row_0':\n",
    "        # Traditional Qualtrics format - row 0 has question text\n",
    "        codebook_df = create_enhanced_codebook(raw_df.iloc[[0]], question_mapping, all_columns)\n",
    "        response_data = raw_df.iloc[1:].copy()\n",
    "        print(\"Extracted codebook from row 0 with question mapping\")\n",
    "\n",
    "    elif structure_analysis['codebook_source'] == 'column_names':\n",
    "        # Alternative format - column names are the questions\n",
    "        codebook_df = create_codebook_from_columns(all_columns, question_mapping)\n",
    "        response_data = raw_df.copy()\n",
    "        print(\"Using column names as codebook (no separate question text row)\")\n",
    "\n",
    "    elif structure_analysis['codebook_source'] == 'mixed':\n",
    "        # Hybrid format - some info in row 0, but not full questions\n",
    "        codebook_df = create_enhanced_codebook(raw_df.iloc[[0]], question_mapping, all_columns)\n",
    "        response_data = raw_df.iloc[1:].copy()\n",
    "        print(\"Extracted partial codebook from row 0 with question mapping (mixed format)\")\n",
    "\n",
    "    else:\n",
    "        # Fallback - treat as simple structure\n",
    "        codebook_df = create_codebook_from_columns(all_columns, question_mapping)\n",
    "        response_data = raw_df.copy()\n",
    "        print(\"Using fallback codebook structure with question mapping\")\n",
    "\n",
    "    # Reset response data index\n",
    "    response_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Analyze response types and quality\n",
    "    response_analysis = analyze_response_types(response_data, structure_analysis)\n",
    "    structure_analysis.update(response_analysis)\n",
    "\n",
    "    return {\n",
    "        'codebook': codebook_df,\n",
    "        'response_data': response_data,\n",
    "        'structure_analysis': structure_analysis,\n",
    "        'metadata_columns': metadata_cols,\n",
    "        'question_columns': question_cols,\n",
    "        'question_mapping': question_mapping  # NEW: Preserve question mapping\n",
    "    }\n",
    "\n",
    "def create_enhanced_codebook(codebook_row, question_mapping, all_columns):\n",
    "    \"\"\"\n",
    "    Create an enhanced codebook that combines row 0 data with question mapping.\n",
    "    NEW FUNCTION: Integrates question mapping into codebook creation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    codebook_row : pandas.DataFrame\n",
    "        Row 0 containing question text\n",
    "    question_mapping : dict\n",
    "        Question mapping from Step 1\n",
    "    all_columns : list\n",
    "        All column names\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Enhanced codebook with multiple information sources\n",
    "    \"\"\"\n",
    "\n",
    "    codebook_data = []\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    for col in all_columns:\n",
    "        # Get question text from multiple sources\n",
    "        row_0_text = None\n",
    "        if len(codebook_row) > 0 and col in codebook_row.columns:\n",
    "            row_0_value = codebook_row[col].iloc[0]\n",
    "            if pd.notna(row_0_value):\n",
    "                row_0_text = str(row_0_value).strip()\n",
    "\n",
    "        # Get mapped question text\n",
    "        mapped_text = column_to_question.get(col, col)\n",
    "\n",
    "        # Determine the best question text to use\n",
    "        if row_0_text and len(row_0_text) > len(col):\n",
    "            final_question_text = row_0_text\n",
    "            text_source = 'row_0'\n",
    "        elif mapped_text != col:\n",
    "            final_question_text = mapped_text\n",
    "            text_source = 'mapping'\n",
    "        else:\n",
    "            final_question_text = col\n",
    "            text_source = 'column_name'\n",
    "\n",
    "        # Determine question type\n",
    "        if col.startswith(('Q', 'q')):\n",
    "            question_type = 'survey_question'\n",
    "        elif col in ['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "                     'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "                     'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "                     'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "                     'DistributionChannel', 'UserLanguage']:\n",
    "            question_type = 'metadata'\n",
    "        else:\n",
    "            question_type = 'other'\n",
    "\n",
    "        codebook_data.append({\n",
    "            'column_id': col,\n",
    "            'question_text': final_question_text,\n",
    "            'text_source': text_source,\n",
    "            'question_type': question_type,\n",
    "            'text_length': len(final_question_text)\n",
    "        })\n",
    "\n",
    "    codebook_df = pd.DataFrame(codebook_data)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nCodebook Creation Summary:\")\n",
    "    source_counts = codebook_df['text_source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} columns\")\n",
    "\n",
    "    return codebook_df\n",
    "\n",
    "def create_codebook_from_columns(all_columns, question_mapping):\n",
    "    \"\"\"\n",
    "    Create a codebook when no row 0 question text is available.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_columns : list\n",
    "        All column names\n",
    "    question_mapping : dict\n",
    "        Question mapping from Step 1\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Codebook based on column names and mapping\n",
    "    \"\"\"\n",
    "\n",
    "    codebook_data = []\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    for col in all_columns:\n",
    "        mapped_text = column_to_question.get(col, col)\n",
    "\n",
    "        # Determine question type\n",
    "        if col.startswith(('Q', 'q')):\n",
    "            question_type = 'survey_question'\n",
    "        elif col in ['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "                     'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "                     'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "                     'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "                     'DistributionChannel', 'UserLanguage']:\n",
    "            question_type = 'metadata'\n",
    "        else:\n",
    "            question_type = 'other'\n",
    "\n",
    "        codebook_data.append({\n",
    "            'column_id': col,\n",
    "            'question_text': mapped_text,\n",
    "            'text_source': 'mapping' if mapped_text != col else 'column_name',\n",
    "            'question_type': question_type,\n",
    "            'text_length': len(mapped_text)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(codebook_data)\n",
    "\n",
    "def analyze_codebook_structure(df, structure_info):\n",
    "    \"\"\"\n",
    "    Determine how the codebook/question text is stored in this export.\n",
    "    (Mostly unchanged, minor formatting updates)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Analyzing Codebook Structure ---\")\n",
    "\n",
    "    codebook_info = {\n",
    "        'codebook_source': 'unknown',\n",
    "        'header_type': 'unknown',\n",
    "        'codebook_quality': 'unknown'\n",
    "    }\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"Warning: No data rows to analyze\")\n",
    "        return codebook_info\n",
    "\n",
    "    # Examine row 0 to see if it contains question text\n",
    "    row_0 = df.iloc[0]\n",
    "\n",
    "    # Calculate metrics to determine if row 0 has question text\n",
    "    non_null_values = row_0.dropna()\n",
    "    if len(non_null_values) == 0:\n",
    "        print(\"Row 0 is completely empty\")\n",
    "        codebook_info['codebook_source'] = 'column_names'\n",
    "        codebook_info['header_type'] = 'empty_row_0'\n",
    "        return codebook_info\n",
    "\n",
    "    # Analyze text characteristics of row 0\n",
    "    avg_length = np.mean([len(str(val)) for val in non_null_values])\n",
    "    max_length = max([len(str(val)) for val in non_null_values])\n",
    "\n",
    "    # Look for question-like patterns\n",
    "    question_patterns = 0\n",
    "    for val in non_null_values.head(10):  # Check first 10 non-null values\n",
    "        str_val = str(val).lower()\n",
    "        if any(pattern in str_val for pattern in ['which of', 'please', 'identify', 'provide', '?']):\n",
    "            question_patterns += 1\n",
    "\n",
    "    print(f\"Row 0 analysis:\")\n",
    "    print(f\"  Average text length: {avg_length:.1f} characters\")\n",
    "    print(f\"  Maximum text length: {max_length} characters\")\n",
    "    print(f\"  Question-like patterns found: {question_patterns}\")\n",
    "\n",
    "    # Decision logic for codebook source\n",
    "    if avg_length > 30 and question_patterns >= 2:\n",
    "        codebook_info['codebook_source'] = 'row_0'\n",
    "        codebook_info['header_type'] = 'full_questions'\n",
    "        codebook_info['codebook_quality'] = 'high'\n",
    "        print(\"[SUCCESS] Row 0 contains full question text\")\n",
    "\n",
    "    elif avg_length > 15 and max_length > 20:\n",
    "        codebook_info['codebook_source'] = 'mixed'\n",
    "        codebook_info['header_type'] = 'descriptive_headers'\n",
    "        codebook_info['codebook_quality'] = 'medium'\n",
    "        print(\"[SUCCESS] Row 0 contains descriptive headers (not full questions)\")\n",
    "\n",
    "    elif avg_length < 10:\n",
    "        codebook_info['codebook_source'] = 'column_names'\n",
    "        codebook_info['header_type'] = 'short_labels'\n",
    "        codebook_info['codebook_quality'] = 'low'\n",
    "        print(\"[SUCCESS] Row 0 contains short labels - using column names as codebook\")\n",
    "\n",
    "    else:\n",
    "        codebook_info['codebook_source'] = 'mixed'\n",
    "        codebook_info['header_type'] = 'mixed_format'\n",
    "        codebook_info['codebook_quality'] = 'medium'\n",
    "        print(\"[INFO] Mixed format detected - treating as partial codebook\")\n",
    "\n",
    "    return codebook_info\n",
    "\n",
    "def analyze_response_types(df, structure_info):\n",
    "    \"\"\"\n",
    "    Analyze the types of responses in the data (real vs test responses).\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Analyzing Response Types ---\")\n",
    "\n",
    "    response_info = {\n",
    "        'total_responses': len(df),\n",
    "        'response_types_found': [],\n",
    "        'test_responses': 0,\n",
    "        'genuine_responses': 0,\n",
    "        'response_type_column': None\n",
    "    }\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"No response data to analyze\")\n",
    "        return response_info\n",
    "\n",
    "    # Look for Status column to identify response types\n",
    "    status_columns = [col for col in df.columns if 'status' in col.lower()]\n",
    "\n",
    "    if status_columns:\n",
    "        status_col = status_columns[0]  # Use first status column found\n",
    "        response_info['response_type_column'] = status_col\n",
    "\n",
    "        # Analyze response types\n",
    "        status_counts = df[status_col].value_counts(dropna=False)\n",
    "        response_info['response_types_found'] = status_counts.to_dict()\n",
    "\n",
    "        print(f\"Response types found in '{status_col}':\")\n",
    "        for resp_type, count in status_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {resp_type}: {count} responses ({percentage:.1f}%)\")\n",
    "\n",
    "            # Categorize as test vs genuine\n",
    "            if pd.isna(resp_type):\n",
    "                continue\n",
    "            elif any(keyword in str(resp_type).lower() for keyword in ['preview', 'test', 'spam']):\n",
    "                response_info['test_responses'] += count\n",
    "            else:\n",
    "                response_info['genuine_responses'] += count\n",
    "\n",
    "    else:\n",
    "        print(\"No Status column found - assuming all responses are genuine\")\n",
    "        response_info['genuine_responses'] = len(df)\n",
    "\n",
    "    # Additional data quality checks\n",
    "    completion_info = analyze_completion_patterns(df)\n",
    "    response_info.update(completion_info)\n",
    "\n",
    "    return response_info\n",
    "\n",
    "def analyze_completion_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze completion patterns and data quality indicators.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    completion_info = {\n",
    "        'has_progress_data': False,\n",
    "        'has_duration_data': False,\n",
    "        'completion_rate_available': False,\n",
    "        'data_quality_indicators': []\n",
    "    }\n",
    "\n",
    "    # Check for progress tracking\n",
    "    progress_columns = [col for col in df.columns if 'progress' in col.lower()]\n",
    "    if progress_columns:\n",
    "        completion_info['has_progress_data'] = True\n",
    "        progress_col = progress_columns[0]\n",
    "\n",
    "        # Analyze completion rates\n",
    "        if df[progress_col].notna().sum() > 0:\n",
    "            completion_info['completion_rate_available'] = True\n",
    "            complete_responses = (df[progress_col] == 100).sum() if (df[progress_col] == 100).any() else 0\n",
    "            completion_rate = (complete_responses / len(df)) * 100\n",
    "            completion_info['completion_rate'] = completion_rate\n",
    "            print(f\"Survey completion rate: {completion_rate:.1f}%\")\n",
    "\n",
    "    # Check for duration data\n",
    "    duration_columns = [col for col in df.columns if 'duration' in col.lower()]\n",
    "    if duration_columns:\n",
    "        completion_info['has_duration_data'] = True\n",
    "        duration_col = duration_columns[0]\n",
    "\n",
    "        if df[duration_col].notna().sum() > 0:\n",
    "            median_duration = df[duration_col].median()\n",
    "            completion_info['median_duration_seconds'] = median_duration\n",
    "            print(f\"Median completion time: {median_duration/60:.1f} minutes\")\n",
    "\n",
    "    # Data quality flags\n",
    "    if completion_info.get('completion_rate', 100) < 70:\n",
    "        completion_info['data_quality_indicators'].append('Low completion rate')\n",
    "\n",
    "    return completion_info\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # This assumes 'result' exists from Step 1\n",
    "    try:\n",
    "        structure_result = extract_data_structure(result)\n",
    "\n",
    "        codebook = structure_result['codebook']\n",
    "        response_data = structure_result['response_data']\n",
    "        analysis = structure_result['structure_analysis']\n",
    "        question_mapping = structure_result['question_mapping']  # NEW: Access preserved mapping\n",
    "\n",
    "        print(f\"\\n=== Step 2a Summary ===\")\n",
    "        print(f\"Codebook source: {analysis['codebook_source']}\")\n",
    "        print(f\"Codebook shape: {codebook.shape}\")\n",
    "        print(f\"Response data shape: {response_data.shape}\")\n",
    "        print(f\"Total genuine responses: {analysis.get('genuine_responses', 'Unknown')}\")\n",
    "        print(f\"Total test responses: {analysis.get('test_responses', 0)}\")\n",
    "        print(f\"Question mapping preserved: {analysis.get('has_question_mapping', False)}\")\n",
    "\n",
    "        if analysis.get('completion_rate_available'):\n",
    "            print(f\"Completion rate: {analysis.get('completion_rate', 0):.1f}%\")\n",
    "\n",
    "        # NEW: Display codebook quality info\n",
    "        if len(codebook) > 0:\n",
    "            print(f\"\\nCodebook Quality:\")\n",
    "            print(f\"  Total entries: {len(codebook)}\")\n",
    "            if 'text_source' in codebook.columns:\n",
    "                source_dist = codebook['text_source'].value_counts()\n",
    "                for source, count in source_dist.items():\n",
    "                    print(f\"  {source}: {count} columns\")\n",
    "\n",
    "        print(\"\\n[SUCCESS] Step 2a Complete: Data structure extracted with question mapping preserved\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"[ERROR] Please run Step 1 first to create the 'result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error in Step 2a: {str(e)}\")"
   ],
   "id": "8f6b9e637b9a7a6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2a: Extracting Data Structure ===\n",
      "Identified 17 metadata columns\n",
      "Identified 40 question/data columns\n",
      "\n",
      "--- Analyzing Codebook Structure ---\n",
      "Row 0 analysis:\n",
      "  Average text length: 109.5 characters\n",
      "  Maximum text length: 384 characters\n",
      "  Question-like patterns found: 0\n",
      "[SUCCESS] Row 0 contains descriptive headers (not full questions)\n",
      "\n",
      "Codebook Creation Summary:\n",
      "  row_0: 54 columns\n",
      "  column_name: 3 columns\n",
      "Extracted partial codebook from row 0 with question mapping (mixed format)\n",
      "\n",
      "--- Analyzing Response Types ---\n",
      "Response types found in 'Status':\n",
      "  IP Address: 187 responses (97.4%)\n",
      "  Survey Preview: 5 responses (2.6%)\n",
      "Survey completion rate: 37.0%\n",
      "Median completion time: 6.8 minutes\n",
      "\n",
      "=== Step 2a Summary ===\n",
      "Codebook source: mixed\n",
      "Codebook shape: (57, 5)\n",
      "Response data shape: (192, 57)\n",
      "Total genuine responses: 187\n",
      "Total test responses: 5\n",
      "Question mapping preserved: True\n",
      "Completion rate: 37.0%\n",
      "\n",
      "Codebook Quality:\n",
      "  Total entries: 57\n",
      "  row_0: 54 columns\n",
      "  column_name: 3 columns\n",
      "\n",
      "[SUCCESS] Step 2a Complete: Data structure extracted with question mapping preserved\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean and Filter Response Data",
   "id": "9b70d95c953a08b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T13:59:09.401451Z",
     "start_time": "2025-08-29T13:59:09.078426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2b: Clean and Filter Response Data with Question Mapping Preservation\n",
    "# ENHANCED: Preserves question mapping through cleaning process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_and_filter_responses(structure_result):\n",
    "    \"\"\"\n",
    "    Clean the response data by removing test responses and applying universal cleaning rules.\n",
    "    ENHANCED: Preserves question mapping throughout the cleaning process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    structure_result : dict\n",
    "        Result from Step 2a containing codebook, response_data, structure_analysis, and question_mapping\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains cleaned_data, quality_flags, cleaning_log, summary_stats, and preserved question_mapping\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 2b: Cleaning and Filtering Response Data ===\")\n",
    "\n",
    "    response_data = structure_result['response_data'].copy()\n",
    "    analysis = structure_result['structure_analysis']\n",
    "    question_mapping = structure_result.get('question_mapping', {})  # NEW: Get question mapping\n",
    "    codebook = structure_result.get('codebook', pd.DataFrame())  # NEW: Get codebook\n",
    "\n",
    "    cleaning_log = []\n",
    "    initial_row_count = len(response_data)\n",
    "\n",
    "    print(f\"Starting with {initial_row_count:,} total responses\")\n",
    "    if question_mapping:\n",
    "        print(f\"Question mapping preserved for {len(question_mapping.get('column_to_question', {}))} columns\")\n",
    "\n",
    "    # Step 1: Remove test/preview responses\n",
    "    genuine_data = filter_test_responses(response_data, analysis, cleaning_log)\n",
    "\n",
    "    # Step 2: Create data quality flags (before removing any data)\n",
    "    flagged_data = create_quality_flags(genuine_data, analysis, cleaning_log)\n",
    "\n",
    "    # Step 3: Apply universal cleaning rules\n",
    "    cleaned_data = apply_universal_cleaning(flagged_data, cleaning_log)\n",
    "\n",
    "    # Step 4: Standardize column names (but preserve mapping)\n",
    "    final_data, updated_mapping = standardize_column_names_with_mapping(\n",
    "        cleaned_data,\n",
    "        structure_result['metadata_columns'],\n",
    "        structure_result['question_columns'],\n",
    "        cleaning_log,\n",
    "        question_mapping,\n",
    "        codebook\n",
    "    )\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = generate_cleaning_summary(initial_row_count, final_data, cleaning_log)\n",
    "\n",
    "    return {\n",
    "        'cleaned_data': final_data,\n",
    "        'cleaning_log': cleaning_log,\n",
    "        'summary_stats': summary_stats,\n",
    "        'original_columns': response_data.columns.tolist(),\n",
    "        'final_columns': final_data.columns.tolist(),\n",
    "        'question_mapping': updated_mapping,  # NEW: Preserve updated mapping\n",
    "        'codebook': codebook  # NEW: Preserve codebook\n",
    "    }\n",
    "\n",
    "def filter_test_responses(df, analysis, cleaning_log):\n",
    "    \"\"\"\n",
    "    Remove test responses (Survey Preview, Spam, etc.) while preserving genuine responses.\n",
    "    (Minor updates for better handling)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Filtering Test Responses ---\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Define test response patterns (case-insensitive)\n",
    "    test_patterns = [\n",
    "        'survey preview', 'preview', 'test', 'spam', 'survey test'\n",
    "    ]\n",
    "\n",
    "    if analysis.get('response_type_column'):\n",
    "        status_col = analysis['response_type_column']\n",
    "\n",
    "        # Identify test responses\n",
    "        test_mask = pd.Series([False] * len(df), index=df.index)\n",
    "\n",
    "        for pattern in test_patterns:\n",
    "            pattern_mask = df[status_col].astype(str).str.lower().str.contains(pattern, na=False)\n",
    "            test_mask = test_mask | pattern_mask\n",
    "\n",
    "        test_count = test_mask.sum()\n",
    "        genuine_data = df[~test_mask].copy().reset_index(drop=True)\n",
    "\n",
    "        cleaning_log.append(f\"Removed {test_count} test responses based on Status column patterns\")\n",
    "        print(f\"Removed {test_count} test responses ({test_count/initial_count*100:.1f}%)\")\n",
    "        print(f\"Retained {len(genuine_data)} genuine responses ({len(genuine_data)/initial_count*100:.1f}%)\")\n",
    "\n",
    "    else:\n",
    "        # No status column - assume all are genuine\n",
    "        genuine_data = df.copy()\n",
    "        cleaning_log.append(\"No status column found - retained all responses as genuine\")\n",
    "        print(\"No status column found - assuming all responses are genuine\")\n",
    "\n",
    "    return genuine_data\n",
    "\n",
    "def create_quality_flags(df, analysis, cleaning_log):\n",
    "    \"\"\"\n",
    "    Create data quality flags without removing data - for transparent analysis.\n",
    "    Enhanced with better handling of low completion rates.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Creating Data Quality Flags ---\")\n",
    "\n",
    "    flagged_data = df.copy()\n",
    "    flags_created = 0\n",
    "\n",
    "    # Flag 1: Survey completion status (especially important given 37% completion rate)\n",
    "    if analysis.get('has_progress_data'):\n",
    "        progress_cols = [col for col in df.columns if 'progress' in col.lower()]\n",
    "        if progress_cols:\n",
    "            progress_col = progress_cols[0]\n",
    "            flagged_data['flag_incomplete'] = df[progress_col] < 100\n",
    "            incomplete_count = flagged_data['flag_incomplete'].sum()\n",
    "            print(f\"Created flag_incomplete: {incomplete_count} responses ({incomplete_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "            # Additional flag for very low completion (< 50%)\n",
    "            flagged_data['flag_very_incomplete'] = df[progress_col] < 50\n",
    "            very_incomplete_count = flagged_data['flag_very_incomplete'].sum()\n",
    "            print(f\"Created flag_very_incomplete: {very_incomplete_count} responses ({very_incomplete_count/len(df)*100:.1f}%)\")\n",
    "            flags_created += 2\n",
    "\n",
    "    # Flag 2: Duration outliers\n",
    "    if analysis.get('has_duration_data'):\n",
    "        duration_cols = [col for col in df.columns if 'duration' in col.lower()]\n",
    "        if duration_cols:\n",
    "            duration_col = duration_cols[0]\n",
    "\n",
    "            # Convert to numeric if needed\n",
    "            duration_numeric = pd.to_numeric(df[duration_col], errors='coerce')\n",
    "\n",
    "            # Flag very short responses (< 60 seconds)\n",
    "            flagged_data['flag_duration_too_short'] = duration_numeric < 60\n",
    "            short_count = flagged_data['flag_duration_too_short'].sum()\n",
    "\n",
    "            # Flag very long responses (> 2 hours = 7200 seconds)\n",
    "            flagged_data['flag_duration_too_long'] = duration_numeric > 7200\n",
    "            long_count = flagged_data['flag_duration_too_long'].sum()\n",
    "\n",
    "            print(f\"Created flag_duration_too_short: {short_count} responses ({short_count/len(df)*100:.1f}%)\")\n",
    "            print(f\"Created flag_duration_too_long: {long_count} responses ({long_count/len(df)*100:.1f}%)\")\n",
    "            flags_created += 2\n",
    "\n",
    "    # Flag 3: Response pattern flags (straight-lining, etc.)\n",
    "    pattern_flags = create_response_pattern_flags(df, cleaning_log)\n",
    "    for flag_name, flag_data in pattern_flags.items():\n",
    "        flagged_data[flag_name] = flag_data\n",
    "        flag_count = flag_data.sum()\n",
    "        print(f\"Created {flag_name}: {flag_count} responses ({flag_count/len(df)*100:.1f}%)\")\n",
    "        flags_created += 1\n",
    "\n",
    "    # Summary of flagging\n",
    "    flag_columns = [col for col in flagged_data.columns if col.startswith('flag_')]\n",
    "    if flag_columns:\n",
    "        total_flagged = flagged_data[flag_columns].any(axis=1).sum()\n",
    "        print(f\"\\nTotal flags created: {flags_created}\")\n",
    "        print(f\"Responses with any flag: {total_flagged} ({total_flagged/len(df)*100:.1f}%)\")\n",
    "        print(f\"Clean responses (no flags): {len(df)-total_flagged} ({(len(df)-total_flagged)/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        total_flagged = 0\n",
    "        print(f\"\\nTotal flags created: {flags_created}\")\n",
    "\n",
    "    cleaning_log.append(f\"Created {flags_created} quality flag types affecting {total_flagged} responses\")\n",
    "\n",
    "    return flagged_data\n",
    "\n",
    "def create_response_pattern_flags(df, cleaning_log):\n",
    "    \"\"\"\n",
    "    Create flags for suspicious response patterns.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    pattern_flags = {}\n",
    "\n",
    "    # Find potential rating/scale columns (likely to show straight-lining)\n",
    "    question_cols = [col for col in df.columns if not any(meta in col for meta in\n",
    "                    ['Date', 'Status', 'IP', 'Progress', 'Duration', 'Finished', 'Recorded', 'Response', 'Recipient', 'External', 'Location', 'Distribution', 'Language'])]\n",
    "\n",
    "    # Look for columns that might be scales (numeric responses)\n",
    "    numeric_question_cols = []\n",
    "    for col in question_cols:\n",
    "        # Try to convert to numeric and see if it's reasonable scale data\n",
    "        numeric_vals = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        if len(numeric_vals) > 0:\n",
    "            unique_vals = numeric_vals.unique()\n",
    "            if len(unique_vals) <= 10 and numeric_vals.min() >= 0 and numeric_vals.max() <= 10:\n",
    "                numeric_question_cols.append(col)\n",
    "\n",
    "    # Flag potential straight-lining if we have scale columns\n",
    "    if len(numeric_question_cols) >= 3:\n",
    "        straightline_flags = []\n",
    "        for idx, row in df.iterrows():\n",
    "            scale_responses = []\n",
    "            for col in numeric_question_cols[:10]:  # Check up to 10 scale columns\n",
    "                val = pd.to_numeric(row[col], errors='coerce')\n",
    "                if not pd.isna(val):\n",
    "                    scale_responses.append(val)\n",
    "\n",
    "            # Flag if 80% or more of scale responses are identical (and we have at least 3 responses)\n",
    "            if len(scale_responses) >= 3:\n",
    "                most_common_val = max(set(scale_responses), key=scale_responses.count)\n",
    "                same_response_pct = scale_responses.count(most_common_val) / len(scale_responses)\n",
    "                straightline_flags.append(same_response_pct >= 0.8)\n",
    "            else:\n",
    "                straightline_flags.append(False)\n",
    "\n",
    "        pattern_flags['flag_potential_straightlining'] = pd.Series(straightline_flags, index=df.index)\n",
    "\n",
    "    return pattern_flags\n",
    "\n",
    "def apply_universal_cleaning(df, cleaning_log):\n",
    "    \"\"\"\n",
    "    Apply universal data cleaning rules that work across all surveys.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Applying Universal Cleaning Rules ---\")\n",
    "\n",
    "    cleaned_data = df.copy()\n",
    "    changes_made = 0\n",
    "\n",
    "    # Rule 1: Standardize NA representations\n",
    "    na_patterns = {\n",
    "        r'^\\s*n\\s*a\\s*$': np.nan,           # \"n a\", \"N A\", \" na \", etc.\n",
    "        r'^\\s*n/a\\s*$': np.nan,             # \"n/a\", \"N/A\", \" n/a \", etc.\n",
    "        r'^\\s*na\\s*$': np.nan,              # \"na\", \"NA\", \" na \", etc.\n",
    "        r'^\\s*none\\s*$': np.nan,            # \"none\", \"None\", \" none \", etc.\n",
    "        r'^\\s*null\\s*$': np.nan,            # \"null\", \"Null\", etc.\n",
    "        r'^\\s*$': np.nan                    # Empty strings and whitespace-only\n",
    "    }\n",
    "\n",
    "    initial_nulls = cleaned_data.isnull().sum().sum()\n",
    "\n",
    "    for pattern, replacement in na_patterns.items():\n",
    "        for col in cleaned_data.columns:\n",
    "            if cleaned_data[col].dtype == 'object':\n",
    "                mask = cleaned_data[col].astype(str).str.match(pattern, case=False, na=False)\n",
    "                if mask.any():\n",
    "                    cleaned_data.loc[mask, col] = replacement\n",
    "                    changes_made += mask.sum()\n",
    "\n",
    "    final_nulls = cleaned_data.isnull().sum().sum()\n",
    "    na_changes = final_nulls - initial_nulls\n",
    "\n",
    "    if na_changes > 0:\n",
    "        print(f\"Standardized NA representations: +{na_changes} null values created\")\n",
    "        cleaning_log.append(f\"Standardized {na_changes} NA representations to null values\")\n",
    "\n",
    "    # Rule 2: Standardize Yes/No responses\n",
    "    yes_no_patterns = {\n",
    "        r'^\\s*yes\\s*\\.?\\s*$': 'Yes',        # \"yes\", \"Yes.\", \" yes \", etc.\n",
    "        r'^\\s*no\\s*\\.?\\s*$': 'No',          # \"no\", \"No.\", \" no \", etc.\n",
    "        r'^\\s*y\\s*$': 'Yes',                # \"y\", \"Y\"\n",
    "        r'^\\s*n\\s*$': 'No'                  # \"n\", \"N\"\n",
    "    }\n",
    "\n",
    "    yes_no_changes = 0\n",
    "    for pattern, replacement in yes_no_patterns.items():\n",
    "        for col in cleaned_data.columns:\n",
    "            if cleaned_data[col].dtype == 'object':\n",
    "                mask = cleaned_data[col].astype(str).str.match(pattern, case=False, na=False)\n",
    "                if mask.any():\n",
    "                    cleaned_data.loc[mask, col] = replacement\n",
    "                    yes_no_changes += mask.sum()\n",
    "\n",
    "    if yes_no_changes > 0:\n",
    "        print(f\"Standardized Yes/No responses: {yes_no_changes} changes made\")\n",
    "        cleaning_log.append(f\"Standardized {yes_no_changes} Yes/No response formats\")\n",
    "\n",
    "    print(f\"Universal cleaning complete: {changes_made + yes_no_changes} total changes\")\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def standardize_column_names_with_mapping(df, metadata_cols, question_cols, cleaning_log, question_mapping, codebook):\n",
    "    \"\"\"\n",
    "    Standardize column names while preserving the question mapping.\n",
    "    FIXED: Preserves the # character for matrix questions like Q2#1_1_1\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Standardizing Column Names ---\")\n",
    "\n",
    "    final_data = df.copy()\n",
    "\n",
    "    # Get the original mapping\n",
    "    original_mapping = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    # Create column name mapping\n",
    "    column_mapping = {}\n",
    "    updated_question_mapping = {'column_to_question': {}}\n",
    "\n",
    "    for col in df.columns:\n",
    "        # FIXED: Special handling for # character - treat it like a letter\n",
    "        # Replace special characters EXCEPT # with underscore\n",
    "        clean_name = re.sub(r'[^\\w\\s#]', '_', col)  # Added # to the exclusion pattern\n",
    "        clean_name = re.sub(r'\\s+', '_', clean_name)  # Replace spaces with underscore\n",
    "        clean_name = re.sub(r'_+', '_', clean_name)  # Replace multiple underscores with single\n",
    "        clean_name = clean_name.strip('_').lower()  # Remove leading/trailing underscores and lowercase\n",
    "\n",
    "        # Ensure name is not empty\n",
    "        if not clean_name:\n",
    "            clean_name = f\"col_{df.columns.get_loc(col)}\"\n",
    "\n",
    "        column_mapping[col] = clean_name\n",
    "\n",
    "        # Update the question mapping with new column names\n",
    "        if col in original_mapping:\n",
    "            updated_question_mapping['column_to_question'][clean_name] = original_mapping[col]\n",
    "        else:\n",
    "            updated_question_mapping['column_to_question'][clean_name] = col\n",
    "\n",
    "    # Apply column name changes\n",
    "    final_data.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    # Update other parts of question_mapping\n",
    "    updated_question_mapping['has_question_text'] = question_mapping.get('has_question_text', False)\n",
    "    updated_question_mapping['question_text_source'] = question_mapping.get('question_text_source', 'unknown')\n",
    "    updated_question_mapping['mapping_quality'] = question_mapping.get('mapping_quality', 'unknown')\n",
    "\n",
    "    # Update question categories with new column names\n",
    "    if 'question_categories' in question_mapping:\n",
    "        updated_categories = {}\n",
    "        for category, columns in question_mapping['question_categories'].items():\n",
    "            updated_cols = [column_mapping.get(col, col) for col in columns if col in column_mapping]\n",
    "            updated_categories[category] = updated_cols\n",
    "        updated_question_mapping['question_categories'] = updated_categories\n",
    "\n",
    "    # Report changes\n",
    "    changes_made = sum(1 for old, new in column_mapping.items() if old != new)\n",
    "    print(f\"Standardized {changes_made} column names\")\n",
    "    print(f\"Question mapping updated for {len(updated_question_mapping['column_to_question'])} columns\")\n",
    "\n",
    "    # Show examples of changes\n",
    "    if changes_made > 0:\n",
    "        print(\"Sample column name changes:\")\n",
    "        examples_shown = 0\n",
    "        for old, new in column_mapping.items():\n",
    "            if old != new and examples_shown < 5:\n",
    "                print(f\"  '{old}' -> '{new}'\")\n",
    "                examples_shown += 1\n",
    "        if changes_made > 5:\n",
    "            print(f\"  ... and {changes_made - 5} more changes\")\n",
    "\n",
    "    cleaning_log.append(f\"Standardized {changes_made} column names while preserving question mapping\")\n",
    "\n",
    "    return final_data, updated_question_mapping\n",
    "\n",
    "def generate_cleaning_summary(initial_count, final_data, cleaning_log):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of cleaning operations.\n",
    "    Enhanced to include question mapping status.\n",
    "    \"\"\"\n",
    "\n",
    "    final_count = len(final_data)\n",
    "    flag_columns = [col for col in final_data.columns if col.startswith('flag_')]\n",
    "\n",
    "    summary = {\n",
    "        'initial_responses': initial_count,\n",
    "        'final_responses': final_count,\n",
    "        'responses_removed': initial_count - final_count,\n",
    "        'removal_rate_pct': ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0,\n",
    "        'quality_flags_created': len(flag_columns),\n",
    "        'flagged_responses': final_data[flag_columns].any(axis=1).sum() if flag_columns else 0,\n",
    "        'clean_responses': final_count - (final_data[flag_columns].any(axis=1).sum() if flag_columns else 0),\n",
    "        'cleaning_operations': len(cleaning_log),\n",
    "        'total_columns': len(final_data.columns),\n",
    "        'flag_columns': len(flag_columns)\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'structure_result' exists from Step 2a\n",
    "        cleaning_result = clean_and_filter_responses(structure_result)\n",
    "\n",
    "        cleaned_data = cleaning_result['cleaned_data']\n",
    "        summary = cleaning_result['summary_stats']\n",
    "        updated_mapping = cleaning_result.get('question_mapping', {})\n",
    "\n",
    "        print(f\"\\n=== Step 2b Summary ===\")\n",
    "        print(f\"Data cleaning completed:\")\n",
    "        print(f\"  Initial responses: {summary['initial_responses']:,}\")\n",
    "        print(f\"  Final responses: {summary['final_responses']:,}\")\n",
    "        print(f\"  Responses removed: {summary['responses_removed']} ({summary['removal_rate_pct']:.1f}%)\")\n",
    "        print(f\"  Quality flags created: {summary['quality_flags_created']}\")\n",
    "        print(f\"  Flagged responses: {summary['flagged_responses']} ({summary['flagged_responses']/summary['final_responses']*100:.1f}%)\")\n",
    "        print(f\"  Clean responses: {summary['clean_responses']} ({summary['clean_responses']/summary['final_responses']*100:.1f}%)\")\n",
    "        print(f\"  Final data shape: {cleaned_data.shape[0]:,} x {cleaned_data.shape[1]:,}\")\n",
    "\n",
    "        # NEW: Display question mapping status\n",
    "        if updated_mapping:\n",
    "            print(f\"\\nQuestion Mapping Status:\")\n",
    "            print(f\"  Mapping preserved: Yes\")\n",
    "            print(f\"  Mapped columns: {len(updated_mapping.get('column_to_question', {}))}\")\n",
    "            print(f\"  Mapping quality: {updated_mapping.get('mapping_quality', 'unknown')}\")\n",
    "\n",
    "        print(\"\\n[SUCCESS] Step 2b Complete: Data cleaned with quality flags and question mapping preserved\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"[ERROR] Please run Step 2a first to create the 'structure_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error in Step 2b: {str(e)}\")"
   ],
   "id": "f709bfb13cc45f4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2b: Cleaning and Filtering Response Data ===\n",
      "Starting with 192 total responses\n",
      "Question mapping preserved for 57 columns\n",
      "\n",
      "--- Filtering Test Responses ---\n",
      "Removed 5 test responses (2.6%)\n",
      "Retained 187 genuine responses (97.4%)\n",
      "\n",
      "--- Creating Data Quality Flags ---\n",
      "Created flag_incomplete: 121 responses (64.7%)\n",
      "Created flag_very_incomplete: 120 responses (64.2%)\n",
      "Created flag_duration_too_short: 56 responses (29.9%)\n",
      "Created flag_duration_too_long: 30 responses (16.0%)\n",
      "Created flag_potential_straightlining: 0 responses (0.0%)\n",
      "\n",
      "Total flags created: 5\n",
      "Responses with any flag: 133 (71.1%)\n",
      "Clean responses (no flags): 54 (28.9%)\n",
      "\n",
      "--- Applying Universal Cleaning Rules ---\n",
      "Standardized NA representations: +24 null values created\n",
      "Standardized Yes/No responses: 554 changes made\n",
      "Universal cleaning complete: 578 total changes\n",
      "\n",
      "--- Standardizing Column Names ---\n",
      "Standardized 57 column names\n",
      "Question mapping updated for 62 columns\n",
      "Sample column name changes:\n",
      "  'StartDate' -> 'startdate'\n",
      "  'EndDate' -> 'enddate'\n",
      "  'Status' -> 'status'\n",
      "  'IPAddress' -> 'ipaddress'\n",
      "  'Progress' -> 'progress'\n",
      "  ... and 52 more changes\n",
      "\n",
      "=== Step 2b Summary ===\n",
      "Data cleaning completed:\n",
      "  Initial responses: 192\n",
      "  Final responses: 187\n",
      "  Responses removed: 5 (2.6%)\n",
      "  Quality flags created: 5\n",
      "  Flagged responses: 133 (71.1%)\n",
      "  Clean responses: 54 (28.9%)\n",
      "  Final data shape: 187 x 62\n",
      "\n",
      "Question Mapping Status:\n",
      "  Mapping preserved: Yes\n",
      "  Mapped columns: 62\n",
      "  Mapping quality: high\n",
      "\n",
      "[SUCCESS] Step 2b Complete: Data cleaned with quality flags and question mapping preserved\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T13:59:12.434304Z",
     "start_time": "2025-08-29T13:59:12.429288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quick diagnostic after Step 2b\n",
    "q2_cols_cleaned = [col for col in cleaning_result['cleaned_data'].columns if 'q2' in col.lower()]\n",
    "print(f\"\\nQ2 columns after cleaning: {len(q2_cols_cleaned)}\")\n",
    "for col in q2_cols_cleaned[:5]:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Check the updated mapping\n",
    "if 'question_mapping' in cleaning_result:\n",
    "    q2_mapping = {k: v for k, v in cleaning_result['question_mapping']['column_to_question'].items() if 'q2' in k.lower()}\n",
    "    print(f\"\\nQ2 columns in updated mapping: {len(q2_mapping)}\")\n",
    "    for k, v in list(q2_mapping.items())[:3]:\n",
    "        print(f\"  {k} -> {v[:50]}...\")"
   ],
   "id": "f77bc6b590238f6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2 columns after cleaning: 20\n",
      "  q2#1_1_1\n",
      "  q2#1_2_1\n",
      "  q2#1_3_1\n",
      "  q2#1_4_1\n",
      "  q2#1_5_1\n",
      "\n",
      "Q2 columns in updated mapping: 20\n",
      "  q2#1_1_1 -> Identify the top 5prioritized research activities...\n",
      "  q2#1_2_1 -> Identify the top 5prioritized research activities...\n",
      "  q2#1_3_1 -> Identify the top 5prioritized research activities...\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Data Type Optimization and Validation",
   "id": "efe9bab2b7d75d9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Intelligent Data Type Detection",
   "id": "c848b5d5b7da9dcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T13:59:37.267405Z",
     "start_time": "2025-08-29T13:59:37.131943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3a: Intelligent Data Type Detection with Question Mapping\n",
    "# ENHANCED: Uses question text to inform data type decisions and preserves mapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_and_assign_data_types(cleaning_result):\n",
    "    \"\"\"\n",
    "    Intelligently detect and assign appropriate data types for all columns.\n",
    "    ENHANCED: Uses question mapping to inform type detection decisions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cleaning_result : dict\n",
    "        Result from Step 2b containing cleaned_data, question_mapping, and metadata\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains typed_data, type_analysis, conversion_log, validation_results, and preserved question_mapping\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 3a: Intelligent Data Type Detection ===\")\n",
    "\n",
    "    df = cleaning_result['cleaned_data'].copy()\n",
    "    question_mapping = cleaning_result.get('question_mapping', {})\n",
    "    codebook = cleaning_result.get('codebook', pd.DataFrame())\n",
    "    initial_dtypes = df.dtypes.to_dict()\n",
    "\n",
    "    # Get question text mapping for informed type detection\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    print(f\"Using question mapping for {len(column_to_question)} columns to inform type detection\")\n",
    "\n",
    "    # Initialize tracking structures\n",
    "    type_analysis = {\n",
    "        'columns_analyzed': len(df.columns),\n",
    "        'conversions_attempted': 0,\n",
    "        'conversions_successful': 0,\n",
    "        'columns_by_final_type': {},\n",
    "        'problematic_columns': [],\n",
    "        'mapping_informed_decisions': 0  # NEW: Track mapping-informed decisions\n",
    "    }\n",
    "\n",
    "    conversion_log = []\n",
    "\n",
    "    # Skip flag columns and ID columns from type conversion\n",
    "    skip_columns = get_columns_to_skip(df)\n",
    "\n",
    "    # Analyze each column for optimal data type\n",
    "    typed_data = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in skip_columns:\n",
    "            conversion_log.append(f\"{col}: Skipped (administrative/flag column)\")\n",
    "            continue\n",
    "\n",
    "        # Get question text for this column\n",
    "        question_text = column_to_question.get(col, col)\n",
    "\n",
    "        print(f\"\\nAnalyzing column: {col}\")\n",
    "        if question_text != col and len(question_text) > len(col):\n",
    "            print(f\"  Question text: {question_text[:60]}...\")\n",
    "\n",
    "        # Use question-aware type detection\n",
    "        type_result = analyze_column_for_type_with_context(\n",
    "            df[col],\n",
    "            col,\n",
    "            question_text\n",
    "        )\n",
    "\n",
    "        if type_result.get('mapping_informed'):\n",
    "            type_analysis['mapping_informed_decisions'] += 1\n",
    "\n",
    "        if type_result['recommended_type'] != 'object':\n",
    "            type_analysis['conversions_attempted'] += 1\n",
    "\n",
    "            # Attempt the conversion\n",
    "            conversion_success = apply_type_conversion(typed_data, col, type_result, conversion_log)\n",
    "\n",
    "            if conversion_success:\n",
    "                type_analysis['conversions_successful'] += 1\n",
    "            else:\n",
    "                type_analysis['problematic_columns'].append(col)\n",
    "\n",
    "    # Categorize final types\n",
    "    final_dtypes = typed_data.dtypes.to_dict()\n",
    "    type_analysis['columns_by_final_type'] = categorize_final_types(final_dtypes)\n",
    "\n",
    "    # Validate type assignments\n",
    "    validation_results = validate_type_assignments(typed_data, conversion_log)\n",
    "\n",
    "    print(f\"\\n{type_analysis['mapping_informed_decisions']} type decisions were informed by question text\")\n",
    "\n",
    "    return {\n",
    "        'typed_data': typed_data,\n",
    "        'type_analysis': type_analysis,\n",
    "        'conversion_log': conversion_log,\n",
    "        'validation_results': validation_results,\n",
    "        'initial_dtypes': initial_dtypes,\n",
    "        'final_dtypes': final_dtypes,\n",
    "        'question_mapping': question_mapping,  # NEW: Preserve mapping\n",
    "        'codebook': codebook  # NEW: Preserve codebook\n",
    "    }\n",
    "\n",
    "def analyze_column_for_type_with_context(series, col_name, question_text):\n",
    "    \"\"\"\n",
    "    Analyze a single column to determine the optimal data type using question context.\n",
    "    NEW FUNCTION: Enhanced version that uses question text for better type detection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Column data to analyze\n",
    "    col_name : str\n",
    "        Name of the column\n",
    "    question_text : str\n",
    "        Full question text from mapping\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Analysis results with recommended type and confidence\n",
    "    \"\"\"\n",
    "\n",
    "    # Get non-null values for analysis\n",
    "    non_null_data = series.dropna()\n",
    "\n",
    "    if len(non_null_data) == 0:\n",
    "        return {\n",
    "            'recommended_type': 'object',\n",
    "            'confidence': 'high',\n",
    "            'reason': 'all_null',\n",
    "            'sample_values': [],\n",
    "            'mapping_informed': False\n",
    "        }\n",
    "\n",
    "    unique_values = non_null_data.unique()\n",
    "    sample_values = list(unique_values[:5])\n",
    "\n",
    "    # Use question text to inform type detection\n",
    "    question_lower = question_text.lower()\n",
    "\n",
    "    # Check for date/time indicators in question text\n",
    "    datetime_keywords = ['date', 'time', 'when', 'year', 'month', 'day', 'timestamp']\n",
    "    if any(keyword in question_lower for keyword in datetime_keywords):\n",
    "        datetime_result = detect_datetime_type(non_null_data, col_name)\n",
    "        if datetime_result['is_datetime']:\n",
    "            datetime_result['mapping_informed'] = True\n",
    "            datetime_result['reason'] += '_question_context'\n",
    "            return {\n",
    "                'recommended_type': 'datetime',\n",
    "                'confidence': datetime_result['confidence'],\n",
    "                'reason': datetime_result['reason'],\n",
    "                'sample_values': sample_values,\n",
    "                'mapping_informed': True\n",
    "            }\n",
    "\n",
    "    # Check for year-specific questions\n",
    "    year_keywords = ['year', 'founded', 'established', 'est.', 'birth', 'graduation']\n",
    "    if any(keyword in question_lower for keyword in year_keywords):\n",
    "        numeric_result = detect_numeric_type(non_null_data, col_name)\n",
    "        if numeric_result['is_numeric']:\n",
    "            # Check if values are in reasonable year range\n",
    "            numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            valid_nums = numeric_series.dropna()\n",
    "            if len(valid_nums) > 0:\n",
    "                if valid_nums.min() > 1800 and valid_nums.max() < 2100:\n",
    "                    return {\n",
    "                        'recommended_type': 'float64',  # Use float for years with potential NaN\n",
    "                        'confidence': 'high',\n",
    "                        'reason': 'year_values_from_question_context',\n",
    "                        'sample_values': sample_values,\n",
    "                        'mapping_informed': True,\n",
    "                        'contamination_rate': numeric_result.get('contamination_rate', 0)\n",
    "                    }\n",
    "\n",
    "    # Check for numeric indicators in question text\n",
    "    numeric_keywords = ['number', 'count', 'amount', 'total', 'quantity', 'how many', 'percentage', '%', 'rate', 'score']\n",
    "    if any(keyword in question_lower for keyword in numeric_keywords):\n",
    "        numeric_result = detect_numeric_type(non_null_data, col_name)\n",
    "        if numeric_result['is_numeric']:\n",
    "            numeric_result['mapping_informed'] = True\n",
    "            numeric_result['confidence'] = 'high' if numeric_result['confidence'] == 'medium' else numeric_result['confidence']\n",
    "            return {\n",
    "                'recommended_type': numeric_result['numeric_subtype'],\n",
    "                'confidence': numeric_result['confidence'],\n",
    "                'reason': numeric_result['reason'] + '_question_context',\n",
    "                'sample_values': sample_values,\n",
    "                'mapping_informed': True,\n",
    "                'contamination_rate': numeric_result.get('contamination_rate', 0)\n",
    "            }\n",
    "\n",
    "    # Check for yes/no or binary questions\n",
    "    binary_keywords = ['yes or no', 'y/n', 'agree', 'disagree', 'true or false', 'do you', 'have you', 'are you', 'is there']\n",
    "    if any(keyword in question_lower for keyword in binary_keywords):\n",
    "        boolean_result = detect_boolean_type(non_null_data, col_name)\n",
    "        if boolean_result['is_boolean']:\n",
    "            boolean_result['mapping_informed'] = True\n",
    "            return {\n",
    "                'recommended_type': 'category',\n",
    "                'confidence': 'high',\n",
    "                'reason': boolean_result['reason'] + '_question_context',\n",
    "                'sample_values': sample_values,\n",
    "                'mapping_informed': True\n",
    "            }\n",
    "\n",
    "    # Check for scale/rating questions\n",
    "    scale_keywords = ['scale', 'rating', 'satisfaction', 'likelihood', 'extent', 'strongly', 'somewhat']\n",
    "    if any(keyword in question_lower for keyword in scale_keywords):\n",
    "        # These are often categorical even if numeric\n",
    "        categorical_result = detect_categorical_type(non_null_data, col_name)\n",
    "        if categorical_result['is_categorical']:\n",
    "            categorical_result['mapping_informed'] = True\n",
    "            return {\n",
    "                'recommended_type': 'category',\n",
    "                'confidence': categorical_result['confidence'],\n",
    "                'reason': categorical_result['reason'] + '_scale_question',\n",
    "                'sample_values': sample_values,\n",
    "                'mapping_informed': True\n",
    "            }\n",
    "\n",
    "    # Fall back to standard detection without context\n",
    "    return analyze_column_for_type_standard(series, col_name, non_null_data, unique_values, sample_values)\n",
    "\n",
    "def analyze_column_for_type_standard(series, col_name, non_null_data, unique_values, sample_values):\n",
    "    \"\"\"\n",
    "    Standard type analysis without question context.\n",
    "    (Extracted from original analyze_column_for_type function)\n",
    "    \"\"\"\n",
    "\n",
    "    # Date/Time Detection\n",
    "    datetime_result = detect_datetime_type(non_null_data, col_name)\n",
    "    if datetime_result['is_datetime']:\n",
    "        return {\n",
    "            'recommended_type': 'datetime',\n",
    "            'confidence': datetime_result['confidence'],\n",
    "            'reason': datetime_result['reason'],\n",
    "            'sample_values': sample_values,\n",
    "            'mapping_informed': False\n",
    "        }\n",
    "\n",
    "    # Numeric Detection\n",
    "    numeric_result = detect_numeric_type(non_null_data, col_name)\n",
    "    if numeric_result['is_numeric']:\n",
    "        return {\n",
    "            'recommended_type': numeric_result['numeric_subtype'],\n",
    "            'confidence': numeric_result['confidence'],\n",
    "            'reason': numeric_result['reason'],\n",
    "            'sample_values': sample_values,\n",
    "            'contamination_rate': numeric_result.get('contamination_rate', 0),\n",
    "            'mapping_informed': False\n",
    "        }\n",
    "\n",
    "    # Boolean/Binary Detection\n",
    "    boolean_result = detect_boolean_type(non_null_data, col_name)\n",
    "    if boolean_result['is_boolean']:\n",
    "        return {\n",
    "            'recommended_type': 'category',\n",
    "            'confidence': boolean_result['confidence'],\n",
    "            'reason': boolean_result['reason'],\n",
    "            'sample_values': sample_values,\n",
    "            'mapping_informed': False\n",
    "        }\n",
    "\n",
    "    # Categorical Detection\n",
    "    categorical_result = detect_categorical_type(non_null_data, col_name)\n",
    "    if categorical_result['is_categorical']:\n",
    "        return {\n",
    "            'recommended_type': 'category',\n",
    "            'confidence': categorical_result['confidence'],\n",
    "            'reason': categorical_result['reason'],\n",
    "            'sample_values': sample_values,\n",
    "            'mapping_informed': False\n",
    "        }\n",
    "\n",
    "    # Default to object (text)\n",
    "    return {\n",
    "        'recommended_type': 'object',\n",
    "        'confidence': 'high',\n",
    "        'reason': 'free_text_or_complex',\n",
    "        'sample_values': sample_values,\n",
    "        'mapping_informed': False\n",
    "    }\n",
    "\n",
    "def get_columns_to_skip(df):\n",
    "    \"\"\"\n",
    "    Identify columns that should not undergo type conversion.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    skip_patterns = [\n",
    "        r'^flag_',           # Quality flags\n",
    "        r'.*id$',            # ID columns\n",
    "        r'.*address$',       # IP addresses\n",
    "        r'^responseid$',     # Response IDs\n",
    "        r'^ipaddress$'       # IP addresses\n",
    "    ]\n",
    "\n",
    "    skip_columns = []\n",
    "    for col in df.columns:\n",
    "        for pattern in skip_patterns:\n",
    "            if re.match(pattern, col, re.IGNORECASE):\n",
    "                skip_columns.append(col)\n",
    "                break\n",
    "\n",
    "    return skip_columns\n",
    "\n",
    "def detect_datetime_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect if column contains date/time data.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check column name patterns first\n",
    "    datetime_name_patterns = [r'date', r'time', r'timestamp']\n",
    "    name_suggests_datetime = any(re.search(pattern, col_name, re.IGNORECASE)\n",
    "                                for pattern in datetime_name_patterns)\n",
    "\n",
    "    if not name_suggests_datetime:\n",
    "        return {'is_datetime': False, 'confidence': 'low', 'reason': 'name_pattern_mismatch'}\n",
    "\n",
    "    # Try to parse as datetime\n",
    "    try:\n",
    "        parsed = pd.to_datetime(series, errors='coerce')\n",
    "        success_rate = parsed.notna().sum() / len(series)\n",
    "\n",
    "        if success_rate >= 0.8:\n",
    "            return {\n",
    "                'is_datetime': True,\n",
    "                'confidence': 'high' if success_rate >= 0.95 else 'medium',\n",
    "                'reason': f'{success_rate:.0%}_successful_datetime_parsing'\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {'is_datetime': False, 'confidence': 'low', 'reason': 'datetime_parsing_failed'}\n",
    "\n",
    "def detect_numeric_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect if column contains numeric data, handling contamination.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    # Attempt numeric conversion\n",
    "    try:\n",
    "        numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "        numeric_count = numeric_series.notna().sum()\n",
    "        total_count = series.notna().sum()\n",
    "\n",
    "        if total_count == 0:\n",
    "            return {'is_numeric': False, 'confidence': 'high', 'reason': 'no_data'}\n",
    "\n",
    "        success_rate = numeric_count / total_count\n",
    "        contamination_rate = 1 - success_rate\n",
    "\n",
    "        # High success rate - clearly numeric\n",
    "        if success_rate >= 0.9:\n",
    "            # Determine if integer or float\n",
    "            if numeric_series.dropna().apply(lambda x: x == int(x)).all():\n",
    "                subtype = 'int64'\n",
    "            else:\n",
    "                subtype = 'float64'\n",
    "\n",
    "            return {\n",
    "                'is_numeric': True,\n",
    "                'numeric_subtype': subtype,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'{success_rate:.0%}_numeric_with_{contamination_rate:.0%}_contamination',\n",
    "                'contamination_rate': contamination_rate\n",
    "            }\n",
    "\n",
    "        # Medium success rate - might be numeric with contamination\n",
    "        elif success_rate >= 0.7:\n",
    "            return {\n",
    "                'is_numeric': True,\n",
    "                'numeric_subtype': 'float64',  # Use float to handle mixed cases\n",
    "                'confidence': 'medium',\n",
    "                'reason': f'{success_rate:.0%}_numeric_contaminated',\n",
    "                'contamination_rate': contamination_rate\n",
    "            }\n",
    "\n",
    "        # Low success rate - not primarily numeric\n",
    "        else:\n",
    "            return {\n",
    "                'is_numeric': False,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'only_{success_rate:.0%}_numeric'\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'is_numeric': False,\n",
    "            'confidence': 'high',\n",
    "            'reason': f'numeric_conversion_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "def detect_boolean_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect binary/boolean columns (Yes/No, True/False, etc.).\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    unique_values = set(str(val).lower().strip() for val in series.unique())\n",
    "\n",
    "    # Common boolean patterns\n",
    "    boolean_patterns = [\n",
    "        {'yes', 'no'},\n",
    "        {'true', 'false'},\n",
    "        {'1', '0'},\n",
    "        {'y', 'n'},\n",
    "        {'on', 'off'},\n",
    "        {'enabled', 'disabled'}\n",
    "    ]\n",
    "\n",
    "    for pattern in boolean_patterns:\n",
    "        if unique_values.issubset(pattern) and len(unique_values) >= 2:\n",
    "            return {\n",
    "                'is_boolean': True,\n",
    "                'confidence': 'high',\n",
    "                'reason': f'binary_values_{list(unique_values)}'\n",
    "            }\n",
    "\n",
    "    # Single value that could be boolean (all Yes, all No, etc.)\n",
    "    if len(unique_values) == 1 and list(unique_values)[0] in ['yes', 'no', 'true', 'false', '1', '0']:\n",
    "        return {\n",
    "            'is_boolean': True,\n",
    "            'confidence': 'medium',\n",
    "            'reason': f'single_boolean_value_{list(unique_values)[0]}'\n",
    "        }\n",
    "\n",
    "    return {'is_boolean': False, 'confidence': 'high', 'reason': 'not_binary_pattern'}\n",
    "\n",
    "def detect_categorical_type(series, col_name):\n",
    "    \"\"\"\n",
    "    Detect categorical columns based on repetition patterns.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    unique_count = len(series.unique())\n",
    "    total_count = len(series)\n",
    "\n",
    "    if total_count == 0:\n",
    "        return {'is_categorical': False, 'confidence': 'high', 'reason': 'no_data'}\n",
    "\n",
    "    # Calculate repetition ratio\n",
    "    repetition_ratio = total_count / unique_count if unique_count > 0 else 0\n",
    "\n",
    "    # Categorical if:\n",
    "    # 1. Few unique values with high repetition\n",
    "    # 2. Reasonable number of categories (not too many, not too few)\n",
    "\n",
    "    if unique_count <= 2:\n",
    "        # Very few categories - likely categorical\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'high',\n",
    "            'reason': f'{unique_count}_unique_values_high_repetition'\n",
    "        }\n",
    "    elif unique_count <= 10 and repetition_ratio >= 2:\n",
    "        # Moderate categories with good repetition\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'high' if repetition_ratio >= 5 else 'medium',\n",
    "            'reason': f'{unique_count}_categories_repetition_{repetition_ratio:.1f}x'\n",
    "        }\n",
    "    elif unique_count <= 20 and repetition_ratio >= 5:\n",
    "        # More categories but very high repetition\n",
    "        return {\n",
    "            'is_categorical': True,\n",
    "            'confidence': 'medium',\n",
    "            'reason': f'{unique_count}_categories_high_repetition_{repetition_ratio:.1f}x'\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'is_categorical': False,\n",
    "        'confidence': 'high',\n",
    "        'reason': f'too_many_unique_values_{unique_count}_or_low_repetition_{repetition_ratio:.1f}x'\n",
    "    }\n",
    "\n",
    "def apply_type_conversion(df, col_name, type_result, conversion_log):\n",
    "    \"\"\"\n",
    "    Apply the recommended type conversion to a column.\n",
    "    Enhanced with better reporting for mapping-informed decisions.\n",
    "    \"\"\"\n",
    "\n",
    "    recommended_type = type_result['recommended_type']\n",
    "    mapping_note = \" (informed by question text)\" if type_result.get('mapping_informed') else \"\"\n",
    "\n",
    "    try:\n",
    "        if recommended_type == 'datetime':\n",
    "            df[col_name] = pd.to_datetime(df[col_name], errors='coerce')\n",
    "            conversion_log.append(f\"{col_name}: Converted to datetime - {type_result['reason']}{mapping_note}\")\n",
    "            print(f\"  [SUCCESS] Converted to datetime{mapping_note}\")\n",
    "            return True\n",
    "\n",
    "        elif recommended_type in ['int64', 'float64']:\n",
    "            original_nulls = df[col_name].isnull().sum()\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "            new_nulls = df[col_name].isnull().sum()\n",
    "            contamination_nulls = new_nulls - original_nulls\n",
    "\n",
    "            if recommended_type == 'int64':\n",
    "                # Only convert to int if no fractional parts\n",
    "                if df[col_name].dropna().apply(lambda x: x == int(x) if pd.notna(x) else True).all():\n",
    "                    df[col_name] = df[col_name].astype('Int64')  # Nullable integer\n",
    "                else:\n",
    "                    recommended_type = 'float64'  # Fall back to float\n",
    "\n",
    "            conversion_log.append(f\"{col_name}: Converted to {recommended_type} - {type_result['reason']} - {contamination_nulls} values became null{mapping_note}\")\n",
    "            print(f\"  [SUCCESS] Converted to {recommended_type} ({contamination_nulls} contaminated values -> null){mapping_note}\")\n",
    "            return True\n",
    "\n",
    "        elif recommended_type == 'category':\n",
    "            df[col_name] = df[col_name].astype('category')\n",
    "            conversion_log.append(f\"{col_name}: Converted to category - {type_result['reason']}{mapping_note}\")\n",
    "            print(f\"  [SUCCESS] Converted to category{mapping_note}\")\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            conversion_log.append(f\"{col_name}: Kept as object - {type_result['reason']}{mapping_note}\")\n",
    "            print(f\"  -> Kept as object ({type_result['reason']}){mapping_note}\")\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        conversion_log.append(f\"{col_name}: Conversion to {recommended_type} FAILED - {str(e)}\")\n",
    "        print(f\"  [ERROR] Conversion to {recommended_type} failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def categorize_final_types(dtypes_dict):\n",
    "    \"\"\"\n",
    "    Categorize the final data types for summary reporting.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    type_categories = {\n",
    "        'datetime': [],\n",
    "        'numeric': [],\n",
    "        'categorical': [],\n",
    "        'text': [],\n",
    "        'administrative': []\n",
    "    }\n",
    "\n",
    "    for col, dtype in dtypes_dict.items():\n",
    "        dtype_str = str(dtype)\n",
    "\n",
    "        if col.startswith('flag_') or 'id' in col.lower():\n",
    "            type_categories['administrative'].append(col)\n",
    "        elif 'datetime' in dtype_str:\n",
    "            type_categories['datetime'].append(col)\n",
    "        elif dtype_str in ['int64', 'Int64', 'float64', 'Float64']:\n",
    "            type_categories['numeric'].append(col)\n",
    "        elif dtype_str == 'category':\n",
    "            type_categories['categorical'].append(col)\n",
    "        else:\n",
    "            type_categories['text'].append(col)\n",
    "\n",
    "    return type_categories\n",
    "\n",
    "def validate_type_assignments(df, conversion_log):\n",
    "    \"\"\"\n",
    "    Validate that type assignments were successful and reasonable.\n",
    "    (Unchanged from original)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_results = {\n",
    "        'datetime_columns': [],\n",
    "        'numeric_columns': [],\n",
    "        'categorical_columns': [],\n",
    "        'potential_issues': []\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            validation_results['datetime_columns'].append({\n",
    "                'column': col,\n",
    "                'null_percentage': null_pct\n",
    "            })\n",
    "\n",
    "            if null_pct > 50:\n",
    "                validation_results['potential_issues'].append(f\"{col}: High null rate ({null_pct:.1f}%) after datetime conversion\")\n",
    "\n",
    "        elif pd.api.types.is_numeric_dtype(dtype):\n",
    "            null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            validation_results['numeric_columns'].append({\n",
    "                'column': col,\n",
    "                'null_percentage': null_pct,\n",
    "                'min_value': df[col].min(),\n",
    "                'max_value': df[col].max()\n",
    "            })\n",
    "\n",
    "            if null_pct > 30:\n",
    "                validation_results['potential_issues'].append(f\"{col}: High contamination ({null_pct:.1f}% null) after numeric conversion\")\n",
    "\n",
    "        elif isinstance(dtype, pd.CategoricalDtype):\n",
    "            n_categories = len(df[col].cat.categories)\n",
    "            validation_results['categorical_columns'].append({\n",
    "                'column': col,\n",
    "                'n_categories': n_categories\n",
    "            })\n",
    "\n",
    "            if n_categories > 20:\n",
    "                validation_results['potential_issues'].append(f\"{col}: Many categories ({n_categories}) - might not be truly categorical\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'cleaning_result' exists from Step 2b\n",
    "        typing_result = detect_and_assign_data_types(cleaning_result)\n",
    "\n",
    "        typed_data = typing_result['typed_data']\n",
    "        analysis = typing_result['type_analysis']\n",
    "        validation = typing_result['validation_results']\n",
    "\n",
    "        print(f\"\\n=== Step 3a Summary ===\")\n",
    "        print(f\"Data type detection completed:\")\n",
    "        print(f\"  Columns analyzed: {analysis['columns_analyzed']}\")\n",
    "        print(f\"  Conversions attempted: {analysis['conversions_attempted']}\")\n",
    "        print(f\"  Conversions successful: {analysis['conversions_successful']}\")\n",
    "        print(f\"  Question-informed decisions: {analysis['mapping_informed_decisions']}\")\n",
    "        print(f\"  Problematic columns: {len(analysis['problematic_columns'])}\")\n",
    "\n",
    "        print(f\"\\nFinal type distribution:\")\n",
    "        for type_category, columns in analysis['columns_by_final_type'].items():\n",
    "            if columns:\n",
    "                print(f\"  {type_category.title()}: {len(columns)} columns\")\n",
    "\n",
    "        if validation['potential_issues']:\n",
    "            print(f\"\\nPotential issues detected:\")\n",
    "            for issue in validation['potential_issues'][:5]:\n",
    "                print(f\"  - {issue}\")\n",
    "            if len(validation['potential_issues']) > 5:\n",
    "                print(f\"  ... and {len(validation['potential_issues']) - 5} more issues\")\n",
    "\n",
    "        print(f\"\\nData shape: {typed_data.shape[0]:,} x {typed_data.shape[1]:,}\")\n",
    "\n",
    "        # Display mapping preservation status\n",
    "        if typing_result.get('question_mapping'):\n",
    "            print(f\"\\nQuestion Mapping Status:\")\n",
    "            print(f\"  Mapping preserved: Yes\")\n",
    "            print(f\"  Mapping quality: {typing_result['question_mapping'].get('mapping_quality', 'unknown')}\")\n",
    "\n",
    "        print(\"\\n[SUCCESS] Step 3a Complete: Data types detected and assigned with question context\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"[ERROR] Please run Step 2b first to create the 'cleaning_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error in Step 3a: {str(e)}\")"
   ],
   "id": "e9fce0c3cf959b39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 3a: Intelligent Data Type Detection ===\n",
      "Using question mapping for 62 columns to inform type detection\n",
      "\n",
      "Analyzing column: startdate\n",
      "  Question text: Start Date...\n",
      "  [SUCCESS] Converted to datetime (informed by question text)\n",
      "\n",
      "Analyzing column: enddate\n",
      "  Question text: End Date...\n",
      "  [SUCCESS] Converted to datetime (informed by question text)\n",
      "\n",
      "Analyzing column: status\n",
      "  Question text: Response Type...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: progress\n",
      "  [SUCCESS] Converted to int64 (0 contaminated values -> null)\n",
      "\n",
      "Analyzing column: duration_in_seconds\n",
      "  Question text: Duration (in seconds)...\n",
      "  [SUCCESS] Converted to int64 (0 contaminated values -> null)\n",
      "\n",
      "Analyzing column: finished\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: recordeddate\n",
      "  Question text: Recorded Date...\n",
      "  [SUCCESS] Converted to datetime (informed by question text)\n",
      "\n",
      "Analyzing column: recipientlastname\n",
      "  Question text: Recipient Last Name...\n",
      "\n",
      "Analyzing column: recipientfirstname\n",
      "  Question text: Recipient First Name...\n",
      "\n",
      "Analyzing column: recipientemail\n",
      "  Question text: Recipient Email...\n",
      "\n",
      "Analyzing column: externalreference\n",
      "  Question text: External Data Reference...\n",
      "\n",
      "Analyzing column: locationlatitude\n",
      "  Question text: Location Latitude...\n",
      "  [SUCCESS] Converted to float64 (0 contaminated values -> null)\n",
      "\n",
      "Analyzing column: locationlongitude\n",
      "  Question text: Location Longitude...\n",
      "  [SUCCESS] Converted to float64 (0 contaminated values -> null)\n",
      "\n",
      "Analyzing column: distributionchannel\n",
      "  Question text: Distribution Channel...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: userlanguage\n",
      "  Question text: User Language...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q0\n",
      "  Question text: Which of the following best describes your role? - Selected ...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q0_1_text\n",
      "  Question text: Which of the following best describes your role? - Core Faci...\n",
      "\n",
      "Analyzing column: q0_2_text\n",
      "  Question text: Which of the following best describes your role? - Individua...\n",
      "\n",
      "Analyzing column: q0_3_text\n",
      "  Question text: Which of the following best describes your role? - Title/Fac...\n",
      "\n",
      "Analyzing column: q1\n",
      "  Question text: Is your program or facility funded by NIH, other federal age...\n",
      "\n",
      "Analyzing column: q1_6_text\n",
      "  Question text: Is your program or facility funded by NIH, other federal age...\n",
      "\n",
      "Analyzing column: q2#1_1_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#1_2_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#1_3_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#1_4_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#1_5_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#2_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#2_2\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#2_3\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#2_4\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#2_5\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#3_1_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#3_2_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#3_3_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#3_4_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#3_5_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "\n",
      "Analyzing column: q2#4_1\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q2#4_2\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q2#4_3\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q2#4_4\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q2#4_5\n",
      "  Question text: Identify the top 5prioritized research activities and equip...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q3\n",
      "  Question text: Can your research group maintain prioritized research activi...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q3_1_text\n",
      "  Question text: Can your research group maintain prioritized research activi...\n",
      "\n",
      "Analyzing column: q4\n",
      "  Question text: Can your freezer-stored samples and/or reagents be consolida...\n",
      "\n",
      "Analyzing column: q5\n",
      "  Question text: Do you have alternative storage solutions for your data? - S...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q5_4_text\n",
      "  Question text: Do you have alternative storage solutions for your data? - O...\n",
      "\n",
      "Analyzing column: q6\n",
      "  Question text: Provide the year your lab operation was established at UM...\n",
      "  [SUCCESS] Converted to int64 (6 contaminated values -> null)\n",
      "\n",
      "Analyzing column: q7\n",
      "  Question text: Does your lab provide shared services or equipment? Can thes...\n",
      "\n",
      "Analyzing column: q8\n",
      "  Question text: Identify below any equipment in your lab that requires more ...\n",
      "\n",
      "Analyzing column: q9\n",
      "  Question text: Identify below any equipment in your lab that will require v...\n",
      "\n",
      "Analyzing column: q10\n",
      "  Question text: Identify any experiments with uninterruptable use of equipme...\n",
      "\n",
      "Analyzing column: q11\n",
      "  Question text: Do your research activities involve human subjects? Example:...\n",
      "  [SUCCESS] Converted to category (informed by question text)\n",
      "\n",
      "Analyzing column: q12\n",
      "  Question text: Does your laboratory have a detailed research continuity pla...\n",
      "  [SUCCESS] Converted to category\n",
      "\n",
      "Analyzing column: q12_1_text\n",
      "  Question text: Does your laboratory have a detailed research continuity pla...\n",
      "\n",
      "Analyzing column: q12_2_text\n",
      "  Question text: Does your laboratory have a detailed research continuity pla...\n",
      "\n",
      "4 type decisions were informed by question text\n",
      "\n",
      "=== Step 3a Summary ===\n",
      "Data type detection completed:\n",
      "  Columns analyzed: 62\n",
      "  Conversions attempted: 22\n",
      "  Conversions successful: 22\n",
      "  Question-informed decisions: 4\n",
      "  Problematic columns: 0\n",
      "\n",
      "Final type distribution:\n",
      "  Datetime: 3 columns\n",
      "  Numeric: 5 columns\n",
      "  Categorical: 14 columns\n",
      "  Text: 34 columns\n",
      "  Administrative: 6 columns\n",
      "\n",
      "Potential issues detected:\n",
      "  - q6: High contamination (67.4% null) after numeric conversion\n",
      "\n",
      "Data shape: 187 x 62\n",
      "\n",
      "Question Mapping Status:\n",
      "  Mapping preserved: Yes\n",
      "  Mapping quality: high\n",
      "\n",
      "[SUCCESS] Step 3a Complete: Data types detected and assigned with question context\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:00:08.048298Z",
     "start_time": "2025-08-29T14:00:08.041298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quick diagnostic after Step 3a\n",
    "q2_cols_typed = [col for col in typing_result['typed_data'].columns if 'q2#' in col.lower()]\n",
    "print(f\"\\nQ2 columns after type detection: {len(q2_cols_typed)}\")\n",
    "for col in q2_cols_typed[:5]:\n",
    "    print(f\"  {col}: {typing_result['typed_data'][col].dtype}\")\n",
    "\n",
    "# Check if question mapping is still preserved\n",
    "if 'question_mapping' in typing_result:\n",
    "    q2_in_mapping = [k for k in typing_result['question_mapping']['column_to_question'].keys() if 'q2#' in k.lower()]\n",
    "    print(f\"\\nQ2 columns still in mapping: {len(q2_in_mapping)}\")"
   ],
   "id": "8a6382e502613248",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2 columns after type detection: 20\n",
      "  q2#1_1_1: object\n",
      "  q2#1_2_1: object\n",
      "  q2#1_3_1: object\n",
      "  q2#1_4_1: object\n",
      "  q2#1_5_1: object\n",
      "\n",
      "Q2 columns still in mapping: 20\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Type Validation and Optimization",
   "id": "a4414a1bd4ee2b79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:45:19.960355Z",
     "start_time": "2025-08-29T18:45:19.828957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3b: Data Type Validation and Optimization - UNIVERSAL VERSION\n",
    "# FIXED: Now truly universal for ANY Qualtrics survey without hardcoding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def validate_and_optimize_data_types(typing_result):\n",
    "    \"\"\"\n",
    "    Validate type assignments, handle contaminated columns, and optimize the final data structure.\n",
    "    UNIVERSAL: Works with any Qualtrics survey without hardcoded assumptions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    typing_result : dict\n",
    "        Result from Step 3a containing typed_data, question_mapping, and analysis results\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains optimized_data, validation_report, contamination_handling, and preserved mapping\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Step 3b: Data Type Validation and Optimization ===\")\n",
    "\n",
    "    df = typing_result['typed_data'].copy()\n",
    "    type_analysis = typing_result['type_analysis']\n",
    "    validation_results = typing_result['validation_results']\n",
    "    question_mapping = typing_result.get('question_mapping', {})\n",
    "    codebook = typing_result.get('codebook', pd.DataFrame())\n",
    "\n",
    "    # Initialize optimization tracking\n",
    "    optimization_log = []\n",
    "    contamination_handling = {}\n",
    "\n",
    "    # Step 1: Handle highly contaminated numeric columns\n",
    "    contaminated_columns = handle_contaminated_numerics(df, validation_results, question_mapping,\n",
    "                                                       optimization_log, contamination_handling)\n",
    "\n",
    "    # Step 2: Optimize categorical columns\n",
    "    optimized_categoricals = optimize_categorical_columns(df, validation_results, optimization_log)\n",
    "\n",
    "    # Step 3: Create summary variables for multi-part questions (dynamically detected)\n",
    "    summary_variables = create_dynamic_summary_variables(df, question_mapping, optimization_log)\n",
    "\n",
    "    # Step 4: Create data quality summaries\n",
    "    quality_summaries = create_quality_summaries(df, optimization_log)\n",
    "\n",
    "    # Step 5: Final data validation\n",
    "    final_validation = perform_final_validation(df, optimization_log)\n",
    "\n",
    "    # Generate comprehensive validation report\n",
    "    validation_report = generate_validation_report(df, type_analysis, validation_results,\n",
    "                                                 contamination_handling, optimization_log,\n",
    "                                                 summary_variables, quality_summaries)\n",
    "\n",
    "    return {\n",
    "        'optimized_data': df,\n",
    "        'validation_report': validation_report,\n",
    "        'contamination_handling': contamination_handling,\n",
    "        'optimization_log': optimization_log,\n",
    "        'final_validation': final_validation,\n",
    "        'question_mapping': question_mapping,  # Preserve mapping\n",
    "        'codebook': codebook,  # Preserve codebook\n",
    "        'summary_variables': summary_variables,\n",
    "        'quality_summaries': quality_summaries\n",
    "    }\n",
    "\n",
    "def handle_contaminated_numerics(df, validation_results, question_mapping, optimization_log, contamination_handling):\n",
    "    \"\"\"\n",
    "    Handle numeric columns with high contamination rates by creating clean versions.\n",
    "    UNIVERSAL: Uses question text to identify year columns dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Handling Contaminated Numeric Columns ---\")\n",
    "\n",
    "    contaminated_columns = []\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    for col_info in validation_results.get('numeric_columns', []):\n",
    "        col_name = col_info['column']\n",
    "        null_pct = col_info['null_percentage']\n",
    "\n",
    "        # If contamination is high (>20% null after conversion), create a cleaned version\n",
    "        if null_pct > 20:\n",
    "            contaminated_columns.append(col_name)\n",
    "\n",
    "            print(f\"Processing contaminated column: {col_name} ({null_pct:.1f}% null)\")\n",
    "\n",
    "            # Get question text to understand the column\n",
    "            question_text = column_to_question.get(col_name, '').lower()\n",
    "\n",
    "            # Check if this appears to be a year column based on question text\n",
    "            year_indicators = ['year', 'founded', 'established', 'est.', 'birth', 'graduation', 'started', 'began']\n",
    "            is_year_column = any(indicator in question_text for indicator in year_indicators)\n",
    "\n",
    "            clean_col_name = f\"{col_name}_clean\"\n",
    "            df[clean_col_name] = df[col_name].copy()\n",
    "\n",
    "            if is_year_column:\n",
    "                # Apply year-specific validation\n",
    "                current_year = pd.Timestamp.now().year\n",
    "\n",
    "                # Determine reasonable range based on context\n",
    "                if any(word in question_text for word in ['birth', 'born', 'age']):\n",
    "                    min_reasonable_year = current_year - 120  # Human age context\n",
    "                    max_reasonable_year = current_year\n",
    "                elif any(word in question_text for word in ['founded', 'established', 'started']):\n",
    "                    min_reasonable_year = 1800  # Organization context\n",
    "                    max_reasonable_year = current_year\n",
    "                else:\n",
    "                    min_reasonable_year = 1900  # Generic year context\n",
    "                    max_reasonable_year = current_year + 5\n",
    "\n",
    "                # Apply range validation\n",
    "                out_of_range_mask = (df[clean_col_name] < min_reasonable_year) | (df[clean_col_name] > max_reasonable_year)\n",
    "                out_of_range_count = out_of_range_mask.sum()\n",
    "\n",
    "                if out_of_range_count > 0:\n",
    "                    print(f\"  Removing {out_of_range_count} out-of-range years (not between {min_reasonable_year}-{max_reasonable_year})\")\n",
    "                    df.loc[out_of_range_mask, clean_col_name] = np.nan\n",
    "\n",
    "                # Calculate age/duration if it's a founding year\n",
    "                if any(word in question_text for word in ['founded', 'established', 'started', 'began']):\n",
    "                    age_col_name = f\"{col_name}_age_years\"\n",
    "                    df[age_col_name] = np.where(\n",
    "                        df[clean_col_name].notna(),\n",
    "                        current_year - df[clean_col_name],\n",
    "                        np.nan\n",
    "                    )\n",
    "\n",
    "                    # Create age categories\n",
    "                    age_category_col = f\"{col_name}_age_category\"\n",
    "                    df[age_category_col] = pd.cut(\n",
    "                        df[age_col_name],\n",
    "                        bins=[0, 5, 10, 20, 50, 200],\n",
    "                        labels=['<5 years', '5-10 years', '10-20 years', '20-50 years', '50+ years']\n",
    "                    )\n",
    "\n",
    "                    contamination_handling[col_name] = {\n",
    "                        'original_valid': df[col_name].notna().sum(),\n",
    "                        'cleaned_valid': df[clean_col_name].notna().sum(),\n",
    "                        'clean_column_created': clean_col_name,\n",
    "                        'derived_columns': [age_col_name, age_category_col],\n",
    "                        'cleaning_method': 'year_validation_with_age_calculation'\n",
    "                    }\n",
    "\n",
    "                    print(f\"  Created age calculations: {age_col_name}, {age_category_col}\")\n",
    "                else:\n",
    "                    contamination_handling[col_name] = {\n",
    "                        'original_valid': df[col_name].notna().sum(),\n",
    "                        'cleaned_valid': df[clean_col_name].notna().sum(),\n",
    "                        'clean_column_created': clean_col_name,\n",
    "                        'cleaning_method': 'year_range_validation'\n",
    "                    }\n",
    "\n",
    "                optimization_log.append(f\"Cleaned year column {col_name} based on question context\")\n",
    "\n",
    "            else:\n",
    "                # Generic numeric cleaning for non-year columns\n",
    "                contamination_handling[col_name] = {\n",
    "                    'original_valid': df[col_name].notna().sum(),\n",
    "                    'cleaned_valid': df[col_name].notna().sum(),\n",
    "                    'clean_column_created': clean_col_name,\n",
    "                    'cleaning_method': 'preserved_as_is'\n",
    "                }\n",
    "\n",
    "                optimization_log.append(f\"Preserved contaminated numeric column {col_name} as {clean_col_name}\")\n",
    "\n",
    "    if not contaminated_columns:\n",
    "        print(\"No highly contaminated numeric columns found\")\n",
    "\n",
    "    return contaminated_columns\n",
    "\n",
    "def create_dynamic_summary_variables(df, question_mapping, optimization_log):\n",
    "    \"\"\"\n",
    "    Dynamically create summary variables for multi-part questions.\n",
    "    UNIVERSAL: Detects question series patterns without hardcoding.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Creating Summary Variables for Question Series ---\")\n",
    "\n",
    "    summary_variables = {}\n",
    "\n",
    "    # Detect question series by analyzing column name patterns\n",
    "    question_series = detect_question_series(df.columns)\n",
    "\n",
    "    for series_base, related_cols in question_series.items():\n",
    "        if len(related_cols) < 3:  # Skip if too few related columns\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing question series: {series_base} ({len(related_cols)} columns)\")\n",
    "\n",
    "        # Group columns by subseries pattern\n",
    "        subseries = detect_subseries(related_cols)\n",
    "\n",
    "        # Create count variables for each subseries\n",
    "        for subseries_name, cols in subseries.items():\n",
    "            if cols:\n",
    "                count_col = f\"{series_base}_{subseries_name}_count\"\n",
    "                df[count_col] = df[cols].notna().sum(axis=1)\n",
    "\n",
    "                # Calculate completion rate\n",
    "                rate_col = f\"{series_base}_{subseries_name}_completion_rate\"\n",
    "                df[rate_col] = (df[count_col] / len(cols) * 100).round(1)\n",
    "\n",
    "                print(f\"  Created {count_col}: Average {df[count_col].mean():.1f} responses\")\n",
    "\n",
    "        # Create overall series completion\n",
    "        all_series_cols = [col for cols in subseries.values() for col in cols]\n",
    "        if all_series_cols:\n",
    "            overall_count = f\"{series_base}_total_responses\"\n",
    "            df[overall_count] = df[all_series_cols].notna().sum(axis=1)\n",
    "\n",
    "            overall_rate = f\"{series_base}_overall_completion\"\n",
    "            df[overall_rate] = (df[overall_count] / len(all_series_cols) * 100).round(1)\n",
    "\n",
    "            summary_variables[series_base] = {\n",
    "                'related_columns': all_series_cols,\n",
    "                'subseries': subseries,\n",
    "                'summary_columns': [col for col in df.columns if col.startswith(f\"{series_base}_\") and\n",
    "                                   any(suffix in col for suffix in ['_count', '_completion', '_responses'])]\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"Created summary variables for {series_base} series ({len(all_series_cols)} columns)\")\n",
    "\n",
    "    if not summary_variables:\n",
    "        print(\"No multi-part question series detected\")\n",
    "\n",
    "    return summary_variables\n",
    "\n",
    "def detect_question_series(columns):\n",
    "    \"\"\"\n",
    "    Detect question series based on column naming patterns.\n",
    "    FIXED: Handles the # character in matrix questions like q2#1_1_1\n",
    "    \"\"\"\n",
    "\n",
    "    series_patterns = {}\n",
    "\n",
    "    for col in columns:\n",
    "        # Skip flag columns and derived columns\n",
    "        if col.startswith('flag_') or any(suffix in col for suffix in ['_clean', '_binary', '_age', '_category']):\n",
    "            continue\n",
    "\n",
    "        # FIXED: Look for patterns like q2#1_1, q2_1_1, etc.\n",
    "        # Handle both regular questions (q1, q2) and matrix questions (q2#)\n",
    "        if '#' in col:\n",
    "            # Matrix question pattern (e.g., q2#1_1_1)\n",
    "            match = re.match(r'^(q\\d+)#', col, re.IGNORECASE)\n",
    "            if match:\n",
    "                base = match.group(1)  # This gets 'q2' from 'q2#1_1_1'\n",
    "        else:\n",
    "            # Regular question pattern (e.g., q1_1_text)\n",
    "            match = re.match(r'^(q\\d+)', col, re.IGNORECASE)\n",
    "            if match:\n",
    "                base = match.group(1)\n",
    "\n",
    "        if match:\n",
    "            if base not in series_patterns:\n",
    "                series_patterns[base] = []\n",
    "            series_patterns[base].append(col)\n",
    "\n",
    "    # Filter to only keep series with multiple related columns\n",
    "    return {k: v for k, v in series_patterns.items() if len(v) > 1}\n",
    "\n",
    "def detect_subseries(columns):\n",
    "    \"\"\"\n",
    "    Group columns into subseries based on their naming patterns.\n",
    "    FIXED: Handles matrix questions with # character\n",
    "    \"\"\"\n",
    "\n",
    "    subseries = {}\n",
    "\n",
    "    for col in columns:\n",
    "        # Handle matrix questions differently\n",
    "        if '#' in col:\n",
    "            # For q2#1_2_1, the subseries is the part after #\n",
    "            parts = col.split('#')[1].split('_')\n",
    "            if len(parts) >= 1:\n",
    "                subseries_key = f\"matrix_row_{parts[0]}\"\n",
    "        else:\n",
    "            # Regular question handling\n",
    "            parts = col.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                if len(parts) > 2 and parts[1].isdigit():\n",
    "                    subseries_key = f\"part_{parts[1]}\"\n",
    "                else:\n",
    "                    subseries_key = \"main\"\n",
    "            else:\n",
    "                subseries_key = \"main\"\n",
    "\n",
    "        if subseries_key not in subseries:\n",
    "            subseries[subseries_key] = []\n",
    "        subseries[subseries_key].append(col)\n",
    "\n",
    "    return subseries\n",
    "\n",
    "def create_quality_summaries(df, optimization_log):\n",
    "    \"\"\"\n",
    "    Create data quality summary variables.\n",
    "    UNIVERSAL: Works with any flag columns created in Step 2b.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Creating Data Quality Summaries ---\")\n",
    "\n",
    "    quality_summaries = {}\n",
    "\n",
    "    # Work with any flag columns that exist\n",
    "    flag_cols = [col for col in df.columns if col.startswith('flag_')]\n",
    "\n",
    "    if flag_cols:\n",
    "        # Count total quality issues per response\n",
    "        df['quality_issue_count'] = df[flag_cols].sum(axis=1)\n",
    "\n",
    "        # Calculate quality score (inverse of issues)\n",
    "        df['quality_score'] = 100 - (df['quality_issue_count'] / len(flag_cols) * 100)\n",
    "\n",
    "        # Categorize quality level\n",
    "        df['quality_level'] = pd.cut(\n",
    "            df['quality_score'],\n",
    "            bins=[0, 50, 75, 100],\n",
    "            labels=['Poor', 'Fair', 'Good']\n",
    "        )\n",
    "\n",
    "        quality_summaries = {\n",
    "            'flag_columns': flag_cols,\n",
    "            'issue_count_column': 'quality_issue_count',\n",
    "            'quality_score_column': 'quality_score',\n",
    "            'quality_level_column': 'quality_level'\n",
    "        }\n",
    "\n",
    "        print(f\"Created quality summaries from {len(flag_cols)} flag columns\")\n",
    "        print(f\"  Average quality score: {df['quality_score'].mean():.1f}%\")\n",
    "        quality_dist = df['quality_level'].value_counts()\n",
    "        for level, count in quality_dist.items():\n",
    "            print(f\"  {level} quality: {count} responses ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "        optimization_log.append(f\"Created quality summaries from {len(flag_cols)} quality flags\")\n",
    "    else:\n",
    "        print(\"No quality flag columns found\")\n",
    "\n",
    "    # Detect and summarize yes/no questions dynamically\n",
    "    yes_no_summaries = create_yes_no_summaries(df, optimization_log)\n",
    "    if yes_no_summaries:\n",
    "        quality_summaries['yes_no_analysis'] = yes_no_summaries\n",
    "\n",
    "    return quality_summaries\n",
    "\n",
    "def create_yes_no_summaries(df, optimization_log):\n",
    "    \"\"\"\n",
    "    Create summaries for yes/no categorical columns.\n",
    "    UNIVERSAL: Automatically detects yes/no questions without hardcoding.\n",
    "    \"\"\"\n",
    "\n",
    "    yes_no_cols = []\n",
    "\n",
    "    # Find categorical columns that appear to be yes/no questions\n",
    "    for col in df.columns:\n",
    "        if str(df[col].dtype) == 'category':\n",
    "            categories = df[col].cat.categories.tolist()\n",
    "            # Check if categories look like yes/no\n",
    "            categories_lower = [str(cat).lower() for cat in categories]\n",
    "            if any(yes_variant in categories_lower for yes_variant in ['yes', 'y', 'true', '1']) and \\\n",
    "               any(no_variant in categories_lower for no_variant in ['no', 'n', 'false', '0']):\n",
    "                yes_no_cols.append(col)\n",
    "\n",
    "                # Create binary version\n",
    "                binary_col = f\"{col}_binary\"\n",
    "                df[binary_col] = df[col].astype(str).str.lower().str.contains('yes|y|true|1', na=False).astype(int)\n",
    "\n",
    "    if yes_no_cols:\n",
    "        # Create overall yes/no summary\n",
    "        binary_cols = [f\"{col}_binary\" for col in yes_no_cols]\n",
    "        df['yes_response_count'] = df[binary_cols].sum(axis=1)\n",
    "        df['yes_response_rate'] = (df['yes_response_count'] / len(binary_cols) * 100).round(1)\n",
    "\n",
    "        print(f\"\\nDetected {len(yes_no_cols)} yes/no questions\")\n",
    "        print(f\"  Average 'Yes' responses: {df['yes_response_count'].mean():.1f}/{len(yes_no_cols)}\")\n",
    "\n",
    "        optimization_log.append(f\"Created binary indicators for {len(yes_no_cols)} yes/no questions\")\n",
    "\n",
    "        return {\n",
    "            'yes_no_columns': yes_no_cols,\n",
    "            'binary_columns': binary_cols,\n",
    "            'summary_columns': ['yes_response_count', 'yes_response_rate']\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n",
    "def optimize_categorical_columns(df, validation_results, optimization_log):\n",
    "    \"\"\"\n",
    "    Optimize categorical columns by handling issues like too many categories.\n",
    "    (Unchanged from original - already generic)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Optimizing Categorical Columns ---\")\n",
    "\n",
    "    optimized_categoricals = {}\n",
    "\n",
    "    for col_info in validation_results.get('categorical_columns', []):\n",
    "        col_name = col_info['column']\n",
    "        n_categories = col_info['n_categories']\n",
    "\n",
    "        if n_categories > 20:\n",
    "            print(f\"Column {col_name} has {n_categories} categories - flagged for review\")\n",
    "\n",
    "            optimized_categoricals[col_name] = {\n",
    "                'original_categories': n_categories,\n",
    "                'action': 'flagged_for_review',\n",
    "                'recommendation': 'Consider if this should be categorical or text'\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"{col_name}: Flagged - {n_categories} categories may be too many\")\n",
    "\n",
    "        elif n_categories == 1:\n",
    "            print(f\"Column {col_name} has only 1 category - converting to constant\")\n",
    "\n",
    "            unique_val = df[col_name].cat.categories[0]\n",
    "            df[f\"{col_name}_constant\"] = unique_val\n",
    "\n",
    "            optimized_categoricals[col_name] = {\n",
    "                'original_categories': 1,\n",
    "                'action': 'converted_to_constant',\n",
    "                'constant_value': unique_val\n",
    "            }\n",
    "\n",
    "            optimization_log.append(f\"{col_name}: Converted to constant value '{unique_val}'\")\n",
    "\n",
    "    if not optimized_categoricals:\n",
    "        print(\"All categorical columns are appropriately sized\")\n",
    "\n",
    "    return optimized_categoricals\n",
    "\n",
    "def perform_final_validation(df, optimization_log):\n",
    "    \"\"\"\n",
    "    Perform final validation checks on the optimized data.\n",
    "    UNIVERSAL: Generic validation without survey-specific assumptions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Performing Final Validation ---\")\n",
    "\n",
    "    final_validation = {\n",
    "        'total_columns': len(df.columns),\n",
    "        'total_rows': len(df),\n",
    "        'data_types': df.dtypes.value_counts().to_dict(),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "        'null_percentages': {},\n",
    "        'validation_warnings': [],\n",
    "        'summary_columns_created': len([col for col in df.columns if any(suffix in col for suffix in\n",
    "                                       ['_count', '_rate', '_score', '_level', '_category', '_binary', '_clean',\n",
    "                                        '_completion', '_responses', '_age', '_years'])])\n",
    "    }\n",
    "\n",
    "    # Calculate null percentages for columns with high missing data\n",
    "    for col in df.columns:\n",
    "        null_pct = df[col].isnull().sum() / len(df) * 100\n",
    "        if null_pct > 50:\n",
    "            final_validation['null_percentages'][col] = null_pct\n",
    "\n",
    "    # Generic validation warnings\n",
    "    if len(final_validation['null_percentages']) > 10:\n",
    "        final_validation['validation_warnings'].append(\n",
    "            f\"Many columns ({len(final_validation['null_percentages'])}) have >50% missing data\"\n",
    "        )\n",
    "\n",
    "    if final_validation['memory_usage_mb'] > 100:\n",
    "        final_validation['validation_warnings'].append(\n",
    "            f\"High memory usage: {final_validation['memory_usage_mb']:.1f} MB\"\n",
    "        )\n",
    "\n",
    "    print(f\"Final validation complete:\")\n",
    "    print(f\"  Data shape: {df.shape[0]:,} x {df.shape[1]:,}\")\n",
    "    print(f\"  Memory usage: {final_validation['memory_usage_mb']:.1f} MB\")\n",
    "    print(f\"  Summary columns created: {final_validation['summary_columns_created']}\")\n",
    "\n",
    "    if final_validation['validation_warnings']:\n",
    "        print(f\"  Warnings: {len(final_validation['validation_warnings'])}\")\n",
    "        for warning in final_validation['validation_warnings']:\n",
    "            print(f\"    - {warning}\")\n",
    "\n",
    "    optimization_log.append(\n",
    "        f\"Final validation: {df.shape[0]} rows x {df.shape[1]} columns, \"\n",
    "        f\"{final_validation['memory_usage_mb']:.1f} MB, \"\n",
    "        f\"{final_validation['summary_columns_created']} summary columns\"\n",
    "    )\n",
    "\n",
    "    return final_validation\n",
    "\n",
    "def generate_validation_report(df, type_analysis, validation_results, contamination_handling,\n",
    "                              optimization_log, summary_variables, quality_summaries):\n",
    "    \"\"\"\n",
    "    Generate comprehensive validation report.\n",
    "    UNIVERSAL: Generic report structure for any survey.\n",
    "    \"\"\"\n",
    "\n",
    "    report = {\n",
    "        'data_overview': {\n",
    "            'shape': df.shape,\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "            'original_columns': type_analysis['columns_analyzed'],\n",
    "            'final_columns': len(df.columns)\n",
    "        },\n",
    "        'type_distribution': type_analysis['columns_by_final_type'],\n",
    "        'contamination_summary': contamination_handling,\n",
    "        'validation_issues': validation_results.get('potential_issues', []),\n",
    "        'optimization_steps': len(optimization_log),\n",
    "        'summary_variables': summary_variables,\n",
    "        'quality_summaries': quality_summaries,\n",
    "        'ready_for_analysis': len(validation_results.get('potential_issues', [])) < 5\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'typing_result' exists from Step 3a\n",
    "        optimization_result = validate_and_optimize_data_types(typing_result)\n",
    "\n",
    "        optimized_data = optimization_result['optimized_data']\n",
    "        validation_report = optimization_result['validation_report']\n",
    "        contamination_handling = optimization_result['contamination_handling']\n",
    "\n",
    "        print(f\"\\n=== Step 3b Summary ===\")\n",
    "        print(f\"Data type validation and optimization completed:\")\n",
    "        print(f\"  Final data shape: {optimized_data.shape[0]:,} x {optimized_data.shape[1]:,}\")\n",
    "        print(f\"  Memory usage: {validation_report['data_overview']['memory_usage_mb']:.1f} MB\")\n",
    "        print(f\"  Contaminated columns handled: {len(contamination_handling)}\")\n",
    "        print(f\"  Optimization steps performed: {validation_report['optimization_steps']}\")\n",
    "\n",
    "        if contamination_handling:\n",
    "            print(f\"\\nContamination handling results:\")\n",
    "            for col, info in contamination_handling.items():\n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    Original valid: {info['original_valid']} values\")\n",
    "                print(f\"    Method: {info['cleaning_method']}\")\n",
    "                if 'derived_columns' in info:\n",
    "                    print(f\"    Created: {', '.join(info['derived_columns'])}\")\n",
    "\n",
    "        # Display summary variables created\n",
    "        if validation_report.get('summary_variables'):\n",
    "            print(f\"\\nSummary variables created for question series:\")\n",
    "            for series, info in validation_report['summary_variables'].items():\n",
    "                print(f\"  {series}: {len(info.get('summary_columns', []))} summary columns\")\n",
    "\n",
    "        print(f\"\\nReady for analysis: {'Yes' if validation_report['ready_for_analysis'] else 'No'}\")\n",
    "\n",
    "        # Display mapping preservation\n",
    "        if optimization_result.get('question_mapping'):\n",
    "            print(f\"\\nQuestion Mapping Status:\")\n",
    "            print(f\"  Preserved through optimization: Yes\")\n",
    "            print(f\"  Mapping quality: {optimization_result['question_mapping'].get('mapping_quality', 'unknown')}\")\n",
    "\n",
    "        print(\"\\n[SUCCESS] Step 3b Complete: Data validated and optimized for analysis\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"[ERROR] Please run Step 3a first to create the 'typing_result' variable\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error in Step 3b: {str(e)}\")"
   ],
   "id": "9a26615851dd32c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 3b: Data Type Validation and Optimization ===\n",
      "\n",
      "--- Handling Contaminated Numeric Columns ---\n",
      "Processing contaminated column: q6 (67.4% null)\n",
      "  Removing 1 out-of-range years (not between 1800-2025)\n",
      "  Created age calculations: q6_age_years, q6_age_category\n",
      "\n",
      "--- Optimizing Categorical Columns ---\n",
      "Column status has only 1 category - converting to constant\n",
      "Column distributionchannel has only 1 category - converting to constant\n",
      "Column userlanguage has only 1 category - converting to constant\n",
      "\n",
      "--- Creating Summary Variables for Question Series ---\n",
      "\n",
      "Processing question series: q0 (4 columns)\n",
      "  Created q0_main_count: Average 0.4 responses\n",
      "  Created q0_part_1_count: Average 0.1 responses\n",
      "  Created q0_part_2_count: Average 0.3 responses\n",
      "  Created q0_part_3_count: Average 0.4 responses\n",
      "\n",
      "Processing question series: q2 (20 columns)\n",
      "  Created q2_matrix_row_1_count: Average 1.5 responses\n",
      "  Created q2_matrix_row_2_count: Average 1.8 responses\n",
      "  Created q2_matrix_row_3_count: Average 1.6 responses\n",
      "  Created q2_matrix_row_4_count: Average 1.8 responses\n",
      "\n",
      "Processing question series: q12 (3 columns)\n",
      "  Created q12_main_count: Average 0.4 responses\n",
      "  Created q12_part_1_count: Average 0.2 responses\n",
      "  Created q12_part_2_count: Average 0.1 responses\n",
      "\n",
      "--- Creating Data Quality Summaries ---\n",
      "Created quality summaries from 5 flag columns\n",
      "  Average quality score: 65.0%\n",
      "  Poor quality: 74 responses (39.6%)\n",
      "  Good quality: 67 responses (35.8%)\n",
      "  Fair quality: 46 responses (24.6%)\n",
      "\n",
      "Detected 8 yes/no questions\n",
      "  Average 'Yes' responses: 1.7/8\n",
      "\n",
      "--- Performing Final Validation ---\n",
      "Final validation complete:\n",
      "  Data shape: 187 x 109\n",
      "  Memory usage: 0.4 MB\n",
      "  Summary columns created: 44\n",
      "  Warnings: 1\n",
      "    - Many columns (47) have >50% missing data\n",
      "\n",
      "=== Step 3b Summary ===\n",
      "Data type validation and optimization completed:\n",
      "  Final data shape: 187 x 109\n",
      "  Memory usage: 0.4 MB\n",
      "  Contaminated columns handled: 1\n",
      "  Optimization steps performed: 10\n",
      "\n",
      "Contamination handling results:\n",
      "  q6:\n",
      "    Original valid: 61 values\n",
      "    Method: year_validation_with_age_calculation\n",
      "    Created: q6_age_years, q6_age_category\n",
      "\n",
      "Summary variables created for question series:\n",
      "  q0: 10 summary columns\n",
      "  q2: 10 summary columns\n",
      "  q12: 8 summary columns\n",
      "\n",
      "Ready for analysis: Yes\n",
      "\n",
      "Question Mapping Status:\n",
      "  Preserved through optimization: Yes\n",
      "  Mapping quality: high\n",
      "\n",
      "[SUCCESS] Step 3b Complete: Data validated and optimized for analysis\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4:",
   "id": "aeccaccf25d77c16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T19:05:43.713587Z",
     "start_time": "2025-08-29T19:05:40.087894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Generate Final Datasets and Documentation - ENHANCED VERSION\n",
    "# INCLUDES: JSON output for sentiment analysis and GUI folder selection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_top_values_summary(series):\n",
    "    \"\"\"\n",
    "    IMPROVED HELPER FUNCTION: More robustly summarizes a series by checking for\n",
    "    comma-separated values FIRST, before checking the data type.\n",
    "    \"\"\"\n",
    "    if series.notna().sum() == 0:\n",
    "        return \"No responses\"\n",
    "\n",
    "    clean_series = series.dropna().astype(str)\n",
    "\n",
    "    if clean_series.str.contains(',').sum() > len(clean_series) * 0.05:\n",
    "        top_vals = clean_series.str.split(',').explode().str.strip().value_counts().head(3)\n",
    "        summary_str = \", \".join([f\"'{k}' ({v})\" for k, v in top_vals.items()])\n",
    "        return f\"[Multi-Select] {summary_str}\"\n",
    "\n",
    "    if str(series.dtype) == 'category':\n",
    "        top_vals = series.value_counts().head(3)\n",
    "        return \", \".join([f\"'{k}' ({v})\" for k,v in top_vals.items()])\n",
    "\n",
    "    if str(series.dtype) == 'object':\n",
    "        unique_count = clean_series.nunique()\n",
    "        return f\"[{unique_count} unique text entries]\"\n",
    "\n",
    "    return f\"[{str(series.dtype)} data]\"\n",
    "\n",
    "def create_json_for_sentiment_analysis(df, question_mapping, generated_files, export_summary, output_dir):\n",
    "    \"\"\"\n",
    "    ENHANCED VERSION: Creates a JSON file optimized for sentiment analysis.\n",
    "    Includes concatenated text, question categorization, and flattened structure option.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating JSON for Sentiment Analysis ---\")\n",
    "\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    # Structure for sentiment analysis\n",
    "    sentiment_data = {\n",
    "        'metadata': {\n",
    "            'total_responses': len(df),\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'survey_completion_rate': float(df['progress'].mean()) if 'progress' in df.columns else None\n",
    "        },\n",
    "        'questions': {},\n",
    "        'responses': [],\n",
    "        'flattened_responses': []  # NEW: For tools that prefer row-based data\n",
    "    }\n",
    "\n",
    "    # Build enhanced questions dictionary with categorization\n",
    "    for col in df.columns:\n",
    "        if col.startswith('q') and not any(suffix in col for suffix in ['_clean', '_binary', '_count', '_rate', '_age', '_category', '_score', '_level', '_completion', '_responses']):\n",
    "            question_text = column_to_question.get(col, col)\n",
    "\n",
    "            # Determine question category\n",
    "            if df[col].dtype == 'object':\n",
    "                # Check if it's likely multiple choice based on unique values\n",
    "                unique_ratio = df[col].nunique() / df[col].notna().sum() if df[col].notna().sum() > 0 else 1\n",
    "                if unique_ratio < 0.3:  # Less than 30% unique responses suggests multiple choice\n",
    "                    question_category = 'multiple_choice'\n",
    "                else:\n",
    "                    question_category = 'open_text'\n",
    "            elif str(df[col].dtype) == 'category':\n",
    "                # Check for yes/no\n",
    "                categories = df[col].cat.categories.tolist()\n",
    "                if len(categories) <= 2 and any('yes' in str(cat).lower() or 'no' in str(cat).lower() for cat in categories):\n",
    "                    question_category = 'binary_yes_no'\n",
    "                else:\n",
    "                    question_category = 'categorical'\n",
    "            else:\n",
    "                question_category = 'numeric'\n",
    "\n",
    "            sentiment_data['questions'][col] = {\n",
    "                'question_id': col,\n",
    "                'question_text': question_text,\n",
    "                'response_type': 'text' if df[col].dtype == 'object' else 'categorical',\n",
    "                'question_category': question_category,  # NEW: More detailed categorization\n",
    "                'response_count': int(df[col].notna().sum()),\n",
    "                'unique_responses': int(df[col].nunique()) if df[col].dtype == 'object' else None,\n",
    "                'is_matrix_question': '#' in col  # NEW: Flag for matrix questions\n",
    "            }\n",
    "\n",
    "    # Build enhanced responses array\n",
    "    for idx, row in df.iterrows():\n",
    "        # Collect all text responses for concatenation\n",
    "        text_responses = []\n",
    "        open_text_responses = []\n",
    "\n",
    "        response_entry = {\n",
    "            'response_id': row.get('responseid', idx),\n",
    "            'completion_percentage': float(row.get('progress', 0)),\n",
    "            'duration_seconds': float(row.get('duration_in_seconds', 0)) if pd.notna(row.get('duration_in_seconds')) else None,\n",
    "            'quality_score': float(row.get('quality_score', 0)) if 'quality_score' in row.index else None,\n",
    "            'quality_level': row.get('quality_level', None) if 'quality_level' in row.index else None,\n",
    "            'answers': {},\n",
    "            'text_responses_concatenated': '',  # NEW: All text responses combined\n",
    "            'open_text_concatenated': ''  # NEW: Only open-ended text responses\n",
    "        }\n",
    "\n",
    "        # Add all question responses\n",
    "        for col in sentiment_data['questions'].keys():\n",
    "            if pd.notna(row[col]):\n",
    "                answer_text = str(row[col])\n",
    "                response_entry['answers'][col] = answer_text\n",
    "\n",
    "                # Collect text for concatenation\n",
    "                if sentiment_data['questions'][col]['question_category'] in ['open_text', 'multiple_choice']:\n",
    "                    text_responses.append(answer_text)\n",
    "                    if sentiment_data['questions'][col]['question_category'] == 'open_text':\n",
    "                        open_text_responses.append(answer_text)\n",
    "\n",
    "        # Create concatenated fields\n",
    "        response_entry['text_responses_concatenated'] = ' | '.join(text_responses)\n",
    "        response_entry['open_text_concatenated'] = ' | '.join(open_text_responses)\n",
    "\n",
    "        sentiment_data['responses'].append(response_entry)\n",
    "\n",
    "        # NEW: Create flattened structure for each response-question pair\n",
    "        for col, answer in response_entry['answers'].items():\n",
    "            if col in sentiment_data['questions']:\n",
    "                flattened_entry = {\n",
    "                    'response_id': response_entry['response_id'],\n",
    "                    'question_id': col,\n",
    "                    'question_text': sentiment_data['questions'][col]['question_text'],\n",
    "                    'question_category': sentiment_data['questions'][col]['question_category'],\n",
    "                    'answer': answer,\n",
    "                    'completion_percentage': response_entry['completion_percentage'],\n",
    "                    'quality_score': response_entry['quality_score'],\n",
    "                    'quality_level': response_entry['quality_level']\n",
    "                }\n",
    "                sentiment_data['flattened_responses'].append(flattened_entry)\n",
    "\n",
    "    # Add summary statistics\n",
    "    sentiment_data['metadata']['question_categories'] = {}\n",
    "    for category in ['open_text', 'multiple_choice', 'categorical', 'binary_yes_no', 'numeric']:\n",
    "        count = sum(1 for q in sentiment_data['questions'].values() if q['question_category'] == category)\n",
    "        sentiment_data['metadata']['question_categories'][category] = count\n",
    "\n",
    "    # Save JSON file\n",
    "    filepath = output_dir / 'sentiment_analysis_data.json'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentiment_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Also save flattened CSV for tools that prefer tabular data\n",
    "    if sentiment_data['flattened_responses']:\n",
    "        flattened_df = pd.DataFrame(sentiment_data['flattened_responses'])\n",
    "        csv_filepath = output_dir / 'sentiment_analysis_flattened.csv'\n",
    "        flattened_df.to_csv(csv_filepath, index=False)\n",
    "        print(f\"Created {csv_filepath.name}: Flattened CSV for row-based analysis tools\")\n",
    "\n",
    "    generated_files['sentiment_json'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "\n",
    "    print(f\"Created {filepath.name}: Enhanced JSON for sentiment analysis\")\n",
    "    print(f\"  - {len(sentiment_data['questions'])} questions included\")\n",
    "    print(f\"  - {len(sentiment_data['responses'])} responses included\")\n",
    "    print(f\"  - {len(sentiment_data['flattened_responses'])} flattened response-question pairs\")\n",
    "    print(f\"  - Question categories: {sentiment_data['metadata']['question_categories']}\")\n",
    "\n",
    "def create_html_summary_report(optimization_result, structure_result, export_summary, generated_files, output_dir):\n",
    "    \"\"\"\n",
    "    ENHANCED: Creates a comprehensive HTML report with proper handling of matrix questions (q2#).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Comprehensive HTML Summary Report ---\")\n",
    "\n",
    "    validation_report = optimization_result['validation_report']\n",
    "    df = optimization_result['optimized_data']\n",
    "    question_mapping = optimization_result.get('question_mapping', {})\n",
    "\n",
    "    html_style = \"\"\"\n",
    "    <style>\n",
    "        body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif; margin: 40px; color: #333; background-color: #f9f9f9;}\n",
    "        h1, h2, h3 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 40px;}\n",
    "        h1 { font-size: 2.5em; }\n",
    "        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; box-shadow: 0 2px 3px rgba(0,0,0,0.1); }\n",
    "        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }\n",
    "        th { background-color: #3498db; color: white; }\n",
    "        tr:nth-child(even) { background-color: #f2f2f2; }\n",
    "        .summary-box { background-color: #ffffff; border: 1px solid #ddd; border-left: 5px solid #3498db; padding: 20px; margin: 20px 0; box-shadow: 0 2px 3px rgba(0,0,0,0.1); }\n",
    "        .summary-box p { margin: 5px 0; font-size: 1.1em;}\n",
    "        .warning { color: #e74c3c; font-weight: bold; }\n",
    "        .bar { background-color: #3498db; height: 20px; display: inline-block; color: white; font-size: 12px; text-align: right; padding-right: 5px; box-sizing: border-box; border-radius: 3px;}\n",
    "        details { border: 1px solid #ddd; border-radius: 4px; margin-bottom: 10px; background-color: #fff;}\n",
    "        summary { cursor: pointer; padding: 12px; background-color: #ecf0f1; font-weight: bold; }\n",
    "        details > div { padding: 15px; }\n",
    "        code { background-color: #ecf0f1; padding: 2px 5px; border-radius: 3px; font-family: monospace; }\n",
    "        .q-label { font-size: 0.8em; color: #7f8c8d; margin-left: 5px; }\n",
    "        .matrix-question { background-color: #e8f4f8; border-left: 3px solid #3498db; padding: 10px; margin: 10px 0; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content = f\"<html><head><title>Qualtrics Processing Summary</title>{html_style}</head><body>\"\n",
    "    html_content += f\"<h1>Qualtrics Processing Summary Report</h1>\"\n",
    "    html_content += f\"<p><em>Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</em></p>\"\n",
    "\n",
    "    html_content += \"<div class='summary-box'><h2>Data Overview</h2>\"\n",
    "    html_content += f\"<p><strong>Total Genuine Responses:</strong> {export_summary['total_responses']}</p>\"\n",
    "    html_content += f\"<p><strong>Original Variables:</strong> {validation_report['data_overview']['original_columns']}</p>\"\n",
    "    html_content += f\"<p><strong>Final Variables (with derived):</strong> {export_summary['total_variables']}</p>\"\n",
    "    if 'progress' in df.columns:\n",
    "        html_content += f\"<p><strong>Average Survey Completion Rate:</strong> {pd.to_numeric(df['progress'], errors='coerce').mean():.1f}%</p>\"\n",
    "    html_content += \"</div>\"\n",
    "\n",
    "    html_content += \"<h2>Data Quality Assessment</h2>\"\n",
    "    if 'quality_level' in df.columns:\n",
    "        quality_dist = df['quality_level'].value_counts()\n",
    "        html_content += \"<table><tr><th>Quality Level</th><th>Count</th><th>Percentage</th></tr>\"\n",
    "        for level in ['Good', 'Fair', 'Poor']:\n",
    "            if level in quality_dist.index:\n",
    "                count = quality_dist[level]\n",
    "                pct = (count / len(df)) * 100\n",
    "                html_content += f\"<tr><td>{level}</td><td>{count}</td><td>{pct:.1f}%</td></tr>\"\n",
    "        html_content += \"</table>\"\n",
    "\n",
    "    if validation_report['validation_issues']:\n",
    "        html_content += \"<h3>Potential Issues Detected:</h3><ul>\"\n",
    "        for issue in validation_report['validation_issues']:\n",
    "            html_content += f\"<li class='warning'>{issue}</li>\"\n",
    "        html_content += \"</ul>\"\n",
    "\n",
    "    html_content += \"<h2>Detailed Reports</h2>\"\n",
    "\n",
    "    # Processing log section\n",
    "    trans_summary = optimization_result.get('validation_report', {}).get('summary_variables', {})\n",
    "    html_content += \"<details><summary> Processing & Transformation Log</summary><div><h4>Summary of Changes</h4><ul>\"\n",
    "    html_content += f\"<li>Test/Preview responses removed: {structure_result.get('structure_analysis', {}).get('test_responses', 0)}</li>\"\n",
    "    html_content += f\"<li>Quality flag columns created: {len([c for c in df.columns if c.startswith('flag_')])}</li>\"\n",
    "    html_content += f\"<li>Contaminated columns handled: {len(optimization_result.get('contamination_handling', {}))}</li>\"\n",
    "    if trans_summary:\n",
    "        html_content += f\"<li>Question series with summaries created: {len(trans_summary)}</li>\"\n",
    "    html_content += \"</ul></div></details>\"\n",
    "\n",
    "    # Question Response Summary with proper matrix question handling\n",
    "    html_content += \"<details open><summary> Question Response Summary</summary><div>\"\n",
    "\n",
    "    question_series = {}\n",
    "    excluded_suffixes = ['_clean', '_binary', '_count', '_rate', '_age', '_category',\n",
    "                         '_score', '_level', '_responses', '_completion', '_years',\n",
    "                         '_constant', '_issue']\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col.startswith('q') and not any(suffix in col for suffix in excluded_suffixes):\n",
    "            # Handle matrix questions (with #) separately\n",
    "            if '#' in col:\n",
    "                match = re.match(r'^(q\\d+)#', col, re.IGNORECASE)\n",
    "                if match:\n",
    "                    base = match.group(1)\n",
    "            else:\n",
    "                match = re.match(r'^(q\\d+)', col, re.IGNORECASE)\n",
    "                if match:\n",
    "                    base = match.group(1)\n",
    "\n",
    "            if match:\n",
    "                if base not in question_series:\n",
    "                    question_series[base] = []\n",
    "                question_series[base].append(col)\n",
    "\n",
    "    # Sort questions numerically\n",
    "    sorted_question_series = sorted(question_series.items(), key=lambda item: int(item[0][1:]))\n",
    "\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "\n",
    "    for base_q, related_cols in sorted_question_series:\n",
    "        # Check if this is a matrix question\n",
    "        is_matrix = any('#' in col for col in related_cols)\n",
    "\n",
    "        # Get main question text\n",
    "        main_question = column_to_question.get(base_q, '')\n",
    "        if not main_question and related_cols:\n",
    "            main_question = column_to_question.get(related_cols[0], base_q)\n",
    "\n",
    "        if is_matrix:\n",
    "            html_content += f\"<div class='matrix-question'>\"\n",
    "            html_content += f\"<h3>{base_q.upper()}: {main_question} [Matrix Question]</h3>\"\n",
    "        else:\n",
    "            html_content += f\"<h3>{base_q.upper()}: {main_question}</h3>\"\n",
    "\n",
    "        html_content += \"<table><tr><th>Sub-Question/Variable</th><th>Response Rate</th><th>Top Values Summary</th></tr>\"\n",
    "\n",
    "        # Sort columns for better display\n",
    "        sorted_cols = sorted(related_cols)\n",
    "\n",
    "        for col in sorted_cols:\n",
    "            rate = (df[col].notna().sum() / len(df)) * 100\n",
    "            bar_html = f\"<div class='bar' style='width:{rate:.1f}%;'>{rate:.1f}%</div>\"\n",
    "            value_info = get_top_values_summary(df[col])\n",
    "            col_type = 'Text' if str(df[col].dtype) == 'object' else 'Category'\n",
    "            col_label = f\"<code>{col}</code><span class='q-label'>[{col_type}]</span>\"\n",
    "            html_content += f\"<tr><td>{col_label}</td><td>{bar_html}</td><td>{value_info}</td></tr>\"\n",
    "\n",
    "        html_content += \"</table>\"\n",
    "\n",
    "        if is_matrix:\n",
    "            html_content += \"</div>\"\n",
    "\n",
    "    html_content += \"</div></details>\"\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    filepath = output_dir / 'comprehensive_summary_report.html'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    generated_files['html_summary_report'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "    print(f\"Created {filepath.name}: A comprehensive HTML summary with matrix question support\")\n",
    "\n",
    "def create_readme_file(export_summary, validation_report, output_dir):\n",
    "    print(\"\\n--- Creating README ---\")\n",
    "    readme_content = f\"\"\"# Qualtrics Data Processing Output\n",
    "Generated: {export_summary['timestamp']}\n",
    "\n",
    "##  Start Here\n",
    "For a complete overview of the processing results, data quality, and response summaries, please open the main deliverable:\n",
    "- **comprehensive_summary_report.html**\n",
    "\n",
    "This single HTML file contains all the summary information you need in a user-friendly format.\n",
    "\n",
    "## Other Generated Files\n",
    "\n",
    "### Data Files\n",
    "- **analysis_ready_data.csv**: The main, cleaned dataset for use in statistical software or programming.\n",
    "- **analysis_ready_data.xlsx**: An Excel version of the main dataset for easy viewing.\n",
    "- **sentiment_analysis_data.json**: JSON structured data for sentiment analysis and NLP processing.\n",
    "\n",
    "### Detailed Documentation\n",
    "- **comprehensive_codebook.csv / .xlsx**: A complete data dictionary listing every variable, its full question text, and summary statistics.\n",
    "- **variable_summaries.xlsx**: Detailed statistical summaries for numeric, categorical, and text variables.\n",
    "\n",
    "### Data Quality Notes\n",
    "- Matrix questions (like Q2) are preserved with the # symbol (e.g., q2#1_1_1)\n",
    "- Summary variables have been created for all question series\n",
    "- Quality flags are included to identify potentially problematic responses\n",
    "\"\"\"\n",
    "\n",
    "    filepath = output_dir / 'README.txt'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "\n",
    "def generate_final_datasets(optimization_result, structure_result, export_excel_duplicates=True):\n",
    "    \"\"\"\n",
    "    Main function to generate all final datasets with GUI folder selection.\n",
    "    \"\"\"\n",
    "    print(\"=== Step 4: Generating Final Datasets and Documentation ===\")\n",
    "\n",
    "    # GUI folder selection\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    output_dir_str = filedialog.askdirectory(title=\"Select a Folder to Save Output Files\")\n",
    "\n",
    "    if not output_dir_str:\n",
    "        output_dir = Path('qualtrics_output_' + datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "        print(f\"No folder selected. Creating default folder: {output_dir}\")\n",
    "    else:\n",
    "        output_dir = Path(output_dir_str)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Files will be saved to: '{output_dir}'\")\n",
    "\n",
    "    generated_files = {}\n",
    "    export_summary = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_responses': len(optimization_result['optimized_data']),\n",
    "        'total_variables': len(optimization_result['optimized_data'].columns),\n",
    "        'files_created': []\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCreating final datasets...\")\n",
    "\n",
    "    # Create all outputs\n",
    "    create_analysis_dataset(optimization_result['optimized_data'], generated_files, export_summary, output_dir, export_excel_duplicates)\n",
    "    create_comprehensive_codebook_with_mapping(optimization_result['optimized_data'], optimization_result.get('question_mapping', {}), generated_files, export_summary, output_dir, export_excel_duplicates)\n",
    "    create_variable_summaries(optimization_result['optimized_data'], generated_files, export_summary, output_dir)\n",
    "    create_json_for_sentiment_analysis(optimization_result['optimized_data'], optimization_result.get('question_mapping', {}), generated_files, export_summary, output_dir)\n",
    "    create_html_summary_report(optimization_result, structure_result, export_summary, generated_files, output_dir)\n",
    "    create_readme_file(export_summary, optimization_result['validation_report'], output_dir)\n",
    "\n",
    "    return {'generated_files': generated_files, 'export_summary': export_summary}\n",
    "\n",
    "def create_analysis_dataset(df, generated_files, export_summary, output_dir, export_excel):\n",
    "    \"\"\"\n",
    "    Create the main analysis dataset with proper column ordering.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Main Analysis Dataset ---\")\n",
    "    analysis_data = df.copy()\n",
    "\n",
    "    # Organize columns by category\n",
    "    metadata_cols = [col for col in df.columns if any(meta in col.lower() for meta in\n",
    "                     ['date', 'status', 'progress', 'duration', 'finished', 'recipient',\n",
    "                      'location', 'distribution', 'language', 'ipaddress', 'responseid'])]\n",
    "    flag_cols = [col for col in df.columns if col.startswith('flag_')]\n",
    "\n",
    "    # Original question columns (including matrix questions with #)\n",
    "    original_question_cols = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('q') and not col.startswith('quality_'):\n",
    "            if not any(suffix in col for suffix in ['_clean', '_constant', '_count', '_rate',\n",
    "                                                    '_completion', '_responses', '_age', '_category',\n",
    "                                                    '_years', '_binary', '_level', '_score', '_filled']):\n",
    "                original_question_cols.append(col)\n",
    "\n",
    "    derived_cols = [col for col in df.columns if any(suffix in col for suffix in\n",
    "                   ['_clean', '_constant', '_count', '_rate', '_completion', '_responses',\n",
    "                    '_age', '_category', '_years', '_binary', '_filled', '_any_response', '_total_items'])]\n",
    "    summary_cols = [col for col in df.columns if any(term in col for term in\n",
    "                   ['quality_score', 'quality_level', 'quality_issue', 'yes_response'])]\n",
    "    other_cols = [col for col in df.columns if col not in\n",
    "                 metadata_cols + flag_cols + original_question_cols + derived_cols + summary_cols]\n",
    "\n",
    "    ordered_columns = metadata_cols + flag_cols + original_question_cols + derived_cols + summary_cols + other_cols\n",
    "    analysis_data = analysis_data[ordered_columns]\n",
    "\n",
    "    filepath = output_dir / 'analysis_ready_data.csv'\n",
    "    analysis_data.to_csv(filepath, index=False)\n",
    "    generated_files['analysis_dataset'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "    print(f\"Created {filepath.name}: {analysis_data.shape[0]:,} rows x {analysis_data.shape[1]:,} cols\")\n",
    "\n",
    "    if export_excel:\n",
    "        excel_filepath = output_dir / 'analysis_ready_data.xlsx'\n",
    "        analysis_data.to_excel(excel_filepath, index=False, sheet_name='Survey Data')\n",
    "        generated_files['analysis_dataset_excel'] = str(excel_filepath)\n",
    "        export_summary['files_created'].append(str(excel_filepath))\n",
    "        print(f\"Created {excel_filepath.name}: Excel version\")\n",
    "\n",
    "def create_comprehensive_codebook_with_mapping(df, question_mapping, generated_files, export_summary, output_dir, export_excel):\n",
    "    \"\"\"\n",
    "    Create comprehensive codebook with proper handling of matrix questions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Comprehensive Codebook ---\")\n",
    "    column_to_question = question_mapping.get('column_to_question', {})\n",
    "    codebook_data = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        question_text = column_to_question.get(col, col)\n",
    "\n",
    "        # Handle derived columns\n",
    "        if not question_text or question_text == col:\n",
    "            base_col = col.split('_')[0] if '_' in col else col\n",
    "            if '#' in base_col:\n",
    "                base_col = base_col.split('#')[0]\n",
    "\n",
    "            if base_col in column_to_question:\n",
    "                question_text = column_to_question[base_col]\n",
    "                if '_clean' in col: question_text += \" [CLEANED VERSION]\"\n",
    "                elif '_binary' in col: question_text += \" [BINARY: 1=Yes, 0=No]\"\n",
    "                elif '_age_years' in col: question_text += \" [CALCULATED AGE IN YEARS]\"\n",
    "                elif '_age_category' in col: question_text += \" [AGE CATEGORIES]\"\n",
    "                elif '_count' in col: question_text += \" [RESPONSE COUNT]\"\n",
    "                elif '_rate' in col or '_completion' in col: question_text += \" [COMPLETION RATE %]\"\n",
    "                elif '_score' in col: question_text += \" [CALCULATED SCORE]\"\n",
    "                elif '_level' in col: question_text += \" [CATEGORY LEVEL]\"\n",
    "                elif '_filled' in col: question_text += \" [NUMBER OF FILLED ITEMS]\"\n",
    "                elif '_any_response' in col: question_text += \" [HAS ANY RESPONSE IN MATRIX]\"\n",
    "                elif '_total_items' in col: question_text += \" [TOTAL ITEMS ANSWERED]\"\n",
    "\n",
    "        # Mark matrix questions\n",
    "        if '#' in col:\n",
    "            question_text = f\"[MATRIX] {question_text}\"\n",
    "\n",
    "        codebook_entry = {\n",
    "            'variable_name': col,\n",
    "            'question_text': question_text,\n",
    "            'variable_type': str(df[col].dtype),\n",
    "            'valid_responses': df[col].notna().sum(),\n",
    "            'missing_responses': df[col].isnull().sum(),\n",
    "            'value_information': get_detailed_value_information(df[col], str(df[col].dtype))\n",
    "        }\n",
    "        codebook_data.append(codebook_entry)\n",
    "\n",
    "    codebook_df = pd.DataFrame(codebook_data)\n",
    "\n",
    "    filepath = output_dir / 'comprehensive_codebook.csv'\n",
    "    codebook_df.to_csv(filepath, index=False)\n",
    "    generated_files['codebook'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "    print(f\"Created {filepath.name} with {len(codebook_df)} variable definitions\")\n",
    "\n",
    "    if export_excel:\n",
    "        excel_filepath = output_dir / 'comprehensive_codebook.xlsx'\n",
    "        codebook_df.to_excel(excel_filepath, index=False, sheet_name='Codebook')\n",
    "        generated_files['codebook_excel'] = str(excel_filepath)\n",
    "        export_summary['files_created'].append(str(excel_filepath))\n",
    "        print(f\"Created {excel_filepath.name}: Excel version\")\n",
    "\n",
    "def create_variable_summaries(df, generated_files, export_summary, output_dir):\n",
    "    \"\"\"\n",
    "    Create detailed variable summary statistics.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Variable Summary Statistics ---\")\n",
    "    filepath = output_dir / 'variable_summaries.xlsx'\n",
    "\n",
    "    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "        # Numeric variables\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if not numeric_cols.empty:\n",
    "            df[numeric_cols].describe().to_excel(writer, sheet_name='Numeric Variables')\n",
    "\n",
    "        # Categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['category']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            cat_summary_list = []\n",
    "            for col in categorical_cols:\n",
    "                counts = df[col].value_counts()\n",
    "                cat_summary_list.append({\n",
    "                    'Variable': col,\n",
    "                    'Categories': len(counts),\n",
    "                    'Top Category': counts.index[0] if not counts.empty else 'N/A',\n",
    "                    'Top Count': counts.iloc[0] if not counts.empty else 0\n",
    "                })\n",
    "            pd.DataFrame(cat_summary_list).to_excel(writer, sheet_name='Categorical Variables', index=False)\n",
    "\n",
    "        # Text variables\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not text_cols.empty:\n",
    "            text_summary_list = []\n",
    "            for col in text_cols:\n",
    "                text_summary_list.append({\n",
    "                    'Variable': col,\n",
    "                    'Non-null Responses': df[col].notna().sum(),\n",
    "                    'Unique Values': df[col].nunique(),\n",
    "                    'Average Length': df[col].astype(str).str.len().mean()\n",
    "                })\n",
    "            pd.DataFrame(text_summary_list).to_excel(writer, sheet_name='Text Variables', index=False)\n",
    "\n",
    "    generated_files['variable_summaries'] = str(filepath)\n",
    "    export_summary['files_created'].append(str(filepath))\n",
    "    print(f\"Created {filepath.name}: Variable summaries by type\")\n",
    "\n",
    "def get_detailed_value_information(series, dtype):\n",
    "    \"\"\"\n",
    "    Get detailed information about values in a series.\n",
    "    \"\"\"\n",
    "    if series.notna().sum() == 0:\n",
    "        return \"All missing values\"\n",
    "\n",
    "    try:\n",
    "        if 'datetime' in dtype:\n",
    "            return f\"Date range: {series.min().strftime('%Y-%m-%d')} to {series.max().strftime('%Y-%m-%d')}\"\n",
    "        elif 'int' in dtype.lower() or 'float' in dtype.lower():\n",
    "            return f\"Range: {series.min():.2f} to {series.max():.2f}, Mean: {series.mean():.2f}\"\n",
    "        elif 'category' in dtype:\n",
    "            n_cats = len(series.cat.categories)\n",
    "            top_3 = series.value_counts().head(3).to_dict()\n",
    "            return f\"{n_cats} categories, Top 3: {top_3}\"\n",
    "        else:\n",
    "            return f\"{series.nunique()} unique text responses\"\n",
    "    except Exception:\n",
    "        return \"Unable to summarize values\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # This assumes 'optimization_result' and 'structure_result' exist from previous steps\n",
    "        final_result = generate_final_datasets(optimization_result, structure_result, export_excel_duplicates=True)\n",
    "\n",
    "        print(\"\\n\\n===  Processing Complete ===\")\n",
    "        print(f\"{len(final_result['export_summary']['files_created'])} files were generated.\")\n",
    "        print(\"\\n Start by opening 'comprehensive_summary_report.html' in a web browser.\")\n",
    "        print(\" For sentiment analysis, use 'sentiment_analysis_data.json'\")\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"\\n[ERROR] Please run the previous steps (1 through 3b) first. Missing variable: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred in the final step: {e}\")"
   ],
   "id": "b2c87d30c7136ce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 4: Generating Final Datasets and Documentation ===\n",
      "Files will be saved to: 'C:\\Users\\p.bachas-daunert\\OneDrive - University of Miami\\Data Analyst Work\\AVP Team Collab\\MSOM Data Analytics Projects\\E.D. of Research\\FY26\\Space_Survey\\outputs'\n",
      "\n",
      "Creating final datasets...\n",
      "\n",
      "--- Creating Main Analysis Dataset ---\n",
      "Created analysis_ready_data.csv: 187 rows x 118 cols\n",
      "Created analysis_ready_data.xlsx: Excel version\n",
      "\n",
      "--- Creating Comprehensive Codebook ---\n",
      "Created comprehensive_codebook.csv with 109 variable definitions\n",
      "Created comprehensive_codebook.xlsx: Excel version\n",
      "\n",
      "--- Creating Variable Summary Statistics ---\n",
      "Created variable_summaries.xlsx: Variable summaries by type\n",
      "\n",
      "--- Creating JSON for Sentiment Analysis ---\n",
      "Created sentiment_analysis_flattened.csv: Flattened CSV for row-based analysis tools\n",
      "Created sentiment_analysis_data.json: Enhanced JSON for sentiment analysis\n",
      "  - 40 questions included\n",
      "  - 187 responses included\n",
      "  - 2173 flattened response-question pairs\n",
      "  - Question categories: {'open_text': 23, 'multiple_choice': 6, 'categorical': 2, 'binary_yes_no': 8, 'numeric': 1}\n",
      "\n",
      "--- Creating Comprehensive HTML Summary Report ---\n",
      "Created comprehensive_summary_report.html: A comprehensive HTML summary with matrix question support\n",
      "\n",
      "--- Creating README ---\n",
      "\n",
      "\n",
      "===  Processing Complete ===\n",
      "7 files were generated.\n",
      "\n",
      " Start by opening 'comprehensive_summary_report.html' in a web browser.\n",
      " For sentiment analysis, use 'sentiment_analysis_data.json'\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
